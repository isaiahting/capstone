{
  "hash": "e2444fb284cc70feaaa31ac47d6463ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Preview\"\ndate: \"21st Feb, 2025\"\ndate-modified: \"last-modified\"\nformat:\n  docx:\n    code-fold: false\n    code-summary: \"Code Chunk\"\n    number-sections: true\nexecute: \n  eval: false #r will run through all codes\n  echo: true #r will display all code chunk\n  warning: false #for mark down\n  freeze: true #r will not render all existing  html files\n  message: false #avoid printing warning message\neditor: source\n---\n\n\n\n# Overview\n\nIn this section, the methodology will be explained thereafter Exploratory Data Analysis will be done.\n\n# Data Acquisition\n\n3 main spectrum of data will be required for this research, namely the Population Data, Master Plan 2019 Subzone boundary and Care Centres.\n\n![Figure x: Data Overview](/methodology/images/data_overview.png){fig-align=\"center\"}\n\n### WebScraping of Care Centres\n\nDue to the lack of a centralised data of all care centres, web scarping is warranted in obtaining the information of the care centres. The geographical locations of the Care Centres alongside the centre names such as Active Ageing Centre, Day Care, Community Rehabilitation Centre, Centre-based Nursing were extracted using a web scraping tool, Web Scraper, available in Chrome web store as Seen in Figure x. As there is no centralised file that consist of the centre names and their locations, the location of each centre has to be manually extracted from the [Care Services](https://www.aic.sg/care-services/) webpage of the Agency of Integrated Care as seen in Figure x.\n\n#### Step 0: Download Web Scraper from Chrome web store\n\nWeb Scraper is used as it is free, works reasonably well and available in both Chrome and Firefox web store. In the below steps, Chrome will be the default web browser used.\n\n![*Figure x: Web Scraper*](/methodology/images/step0.png)\n\n#### Step 1: Navigate to Developer Tools in Chrome Web Browser\n\nAfter downloading the extension from Chrome Web Store, press onto the menu bar at the right of the browser and locate Developer Tools while onto the website you would like to scrape information from.\n\n![*Figure x: Web Scraper*](/methodology/images/step1.png)\n\n#### Step 2: Interface for Webscraper\n\nAfter clicking onto Developer Tools, click onto the Web Scraper in the menu bar (in black). Following which the below interface will appear.\n\n[![](/methodology/images/step2.png){width=\"499\"}](Figure%20x:%20Locate%20Developer%20Tools)\n\n#### Step 3: Create New Sitemap\n\nClick onto \"create new sitemap\", thereafter \"Create Sitemap\". Sitemap Name will be the overarching term used for these information; in this instance, it will be AAC. The Start URL will be the HTML link that you would like the information to be scraped from.\n\n![Figure x:](/methodology/images/step3.png){fig-align=\"center\"}\n\n#### Step 4: Add New Selector\n\nAfter creating a new sitemap, the following interface will appear. Click onto the \"Add new selector\" to select the information to scrape.\n\n![Figure x:](/methodology/images/step4.png){fig-align=\"center\"}\n\n#### Step 5: Selecting Whole Box\n\nFirstly, the id will be the column name. For Type, select Element Attribute from the drop down selection. Thereafter, press on Select under Selector and select two boxes of each centre as seen in the figure below (the remaining boxes will be highlighted through its intelligent function) and press onto Done Selecting in the green box.\n\n![Figure x:](/methodology/images/step5.png){fig-align=\"center\"}\n\n#### Step 6: Sitemap Interface\n\nAfter adding a new selector, the sitemap page will appear the selector that you've inputted.\n\n![Figure x: Step 6 - Create New Sitemap](/methodology/images/step6.png){fig-align=\"center\"}\n\n#### Step 7: Selecting Name of Care Centre\n\nFirstly, the id will be name (with reference to the name of care centre), serving as the column name. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 names of the care centres (The remaining care centres will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple names and root parent selector will be root and press onto Save Selector.\n\n![Figure x: Step 7 - Selecting Name of Care Centre](/methodology/images/step7.png){fig-align=\"center\"}\n\n#### Step 8: Create New Sitemap\n\nA popup window will be prompted and Group selectors was selected.\n\n![Figure x: Step 8 - Create New Sitemap](/methodology/images/step8.png){fig-align=\"center\"}\n\n#### Step 9: Selecting Address of Care Centre\n\nSimilar to Step 7, the id will be address. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 addresses of the care centres (Remaining addresses will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple addresses and parent selector will be wrapper_for_main_name (as we grouped selectors in step 8) and press onto Save Selector.\n\n![Figure x: Step 9 - Selecting Address of Care Centre](/methodology/images/step9.png){fig-align=\"center\"}\n\n#### Step 10: Data Preview\n\nPrior to data scraping, the data is previewed in ensuring each name of the care centre is correctly tagged to the address using the main website to verify.\n\n![Figure x: Step 10 - Data Preview](/methodology/images/step10.png){fig-align=\"center\"}\n\n#### Step 11: Commence Scraping\n\nHead over to sitemap aac and click onto Scrape. A new browser will appear indicating that it is in process of scraping. It will be closed automatically once the process has ended.\n\n![Figure x: Step 11 - Commence Scraping](/methodology/images/step11.png){fig-align=\"center\"}\n\n#### Step 11: Export Data\n\nExport data is selected upon clicking sitemap aac. 2 file options are offered: csv and xlsx. The former was chosen as CSV files are simple and portable which doesn't complicate data processing. Thereafter the data will be downloaded.\n\n![Figure x:](/methodology/images/step12.png){fig-align=\"center\"}\n\n#### Step 11: View CSV File\n\nIn ensuring the web scraping successful and accurate, the csv. file is opened and viewed.\n\n![Figure x: Step 11 - View CSV File](/methodology/images/step13.png){fig-align=\"center\"}\n\nThe above steps were repeated for each care centre. All of the Care Centre data was extracted on 7th February 2025.\n\n## Packages\n\n::: panel-tabset\nThe following packages are required for this section:\n\n| Package | Description |\n|----|----|\n| [**sf**](https://r-spatial.github.io/sf/) | For importing, managing, and handling geospatial data |\n| [**sfdep**](https://sfdep.josiahparry.com/) | Used to compute spatial weights, global and local spatial autocorrelation statistics |\n| [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) | For thematic mapping |\n| [**tidyverse**](https://www.tidyverse.org/) | For non-spatial data wrangling that includes dplyr, tibble, ggplot2, readr, tidyr, stringr, forcats, lubridate and purr |\n| [**knitr**](https://cran.r-project.org/web/packages/knitr/index.html) | For dynamic report generation |\n| [**patchwork**](https://patchwork.data-imaginist.com/) | For plot manipulation |\n| [**leaflet**](https://rstudio.github.io/leaflet/) | For interactive maps |\n| [**scales**](https://scales.r-lib.org/) | For scaling graphs |\n\n## Code\n\nThe code chunk below, using `p_load` function of the [**pacman**](https://cran.r-project.org/web/packages/pacman/pacman.pdf) package, ensures that packages required are installed and loaded in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, sf, httr,\n               jsonlite, rvest, dplyr, units,\n               lubridate, tmap)\n\n# -   Creates a package list containing the necessary R packages\n# -   Checks if the R packages in the package list have been installed\n# -   If not installed, will install the missing packages & launch into R environment.\n```\n:::\n\n\n:::\n\n## Geospatial Data\n\n### Importing Singapore's Master Plan 2019 Subzone Boundary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz = st_read(dsn = \"data/subzone/\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Care Centre (EXTRACT TRANSFORM LOAD)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naac <- read_csv(\"data/carecentre/activeageingcentre.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncounselling <- read_csv(\"data/carecentre/counselling.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndaycare <- read_csv(\"data/carecentre/daycare.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndementia <- read_csv(\"data/carecentre/dementiadaycare.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhospice <- read_csv(\"data/carecentre/dayhospice.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmaintenance <- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnhrespite <- read_csv(\"data/carecentre/nhrespite.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnursing <- read_csv(\"data/carecentre/centrebasednursing.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrehab <- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Cursory View\n\nUsing the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns \"web-scraper-order\" and \"web-scraper-start-url\" are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(popdata20)\n```\n:::\n\n\n\n### Deleting Unwanted Codes\n\nThe following R code is used to remove the columns \"web-scraper-order\" and \"web-scraper-start-url\" from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naac <- aac %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ncounselling <- counselling %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndaycare <- daycare %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndementia <- dementia %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nhospice <- hospice %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nmaintenance <- maintenance %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnhrespite <- nhrespite %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnursing <- nursing %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nrehab <- rehab %>% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\n```\n:::\n\n\n\nAfter removing the two columns, each data set has two columns, namely name and address only.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(aac)\n```\n:::\n\n\n\n### Checking for Missing Values\n\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\n\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing <- aac %>% summarise(across(c(name, address), ~sum(is.na(.))))\ncounselling_missing <- counselling %>% summarise(across(c(name, address), ~sum(is.na(.))))\ndaycare_missing <- daycare %>% summarise(across(c(name, address), ~sum(is.na(.))))\ndementia_missing <- dementia %>% summarise(across(c(name, address), ~sum(is.na(.))))\nhospice_missing <- hospice %>% summarise(across(c(name, address), ~sum(is.na(.))))\nmaintenance_missing <- maintenance %>% summarise(across(c(name, address), ~sum(is.na(.))))\nnhrespite_missing <- nhrespite %>% summarise(across(c(name, address), ~sum(is.na(.))))\nnursing_missing <- nursing %>% summarise(across(c(name, address), ~sum(is.na(.))))\nrehab_missing <- rehab %>% summarise(across(c(name, address), ~sum(is.na(.))))\n```\n:::\n\n\n\n### Duplicate Check\n\nThe code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \\> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \\> 1) keeps only the rows that appear more than once (i.e., duplicates).\n\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\n\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for duplicates in 'aac'\naac_duplicate <- aac %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'counselling'\ncounselling_duplicate <- counselling %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'daycare'\ndaycare_duplicate <- daycare %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'dementia'\ndementia_duplicate <- dementia %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'hospice'\nhospice_duplicate <- hospice %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'maintenance'\nmaintenance_duplicate <- maintenance %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'nhrespite'\nnhrespite_duplicate <- nhrespite %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'nursing'\nnursing_duplicate <- nursing %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n\n# Check for duplicates in 'rehab'\nrehab_duplicate <- rehab %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\n```\n:::\n\n\n\n## Data Manipulation (CC)\n\n### Seperating postal code from address\n\nThe code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Active Ageing Centre\naac <-mutate(aac,\n    postal_code = str_extract(address, \"[0-9]{6}$\"),  # Extract postal code\n    address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")  # Remove postal code from address\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Counselling\ncounselling <- mutate(counselling,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daycare\ndaycare <- mutate(daycare,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Dementia\ndementia <- mutate(dementia,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Day Hospice\nhospice <- mutate(hospice,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Maintenance Daycare\nmaintenance <- mutate(maintenance,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# NH Respite\nnhrespite <- mutate(nhrespite,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Centre Based Nursing\nnursing <- mutate(nursing,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Community Rehab Centre\nrehab <- mutate(rehab,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing <- aac %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ncounselling_missing <- counselling %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndaycare_missing <- daycare %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndementia_missing <- dementia %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nhospice_missing <- hospice %>% summarise(across(c(name, address, postal_code),~sum(is.na(.))))\nmaintenance_missing <- maintenance %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnhrespite_missing <- nhrespite %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnursing_missing <- nursing %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nrehab_missing <- rehab %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\n```\n:::\n\n\n\n### Labelling each dataset appropriately\n\nThe below code chunk adds a column and naming it as \"label\" in relation to the name of the dataset. This is done so we are able to combine the dataset together and identify\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naac <- aac %>%\n  mutate(label = \"aac\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncounselling <- counselling %>%\n  mutate(label = \"counselling\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndaycare <- daycare %>%\n  mutate(label = \"daycare\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndementia <- dementia %>%\n  mutate(label = \"dementia\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhospice <- hospice %>%\n  mutate(label = \"hospice\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmaintenance <- maintenance %>%\n  mutate(label = \"maintenance\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnhrespite <- nhrespite %>%\n  mutate(label = \"nhrespite\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnursing <- nursing %>%\n  mutate(label = \"nursing\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrehab <- rehab %>%\n  mutate(label = \"rehab\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Append all Care Centres into one dataset\n\nThe code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named c_data using the bind_rows() function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting c_data will contain all the rows from the individual datasets, assuming they have the same column structure.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc_data <- bind_rows(\n  aac, \n  counselling,\n  daycare,\n  dementia,\n  hospice,\n  maintenance,\n  nhrespite,\n  nursing,\n  rehab,\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### USE THIS\n\nIf yes is 1, no returns 0.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npivoted_cc_data <- cc_data %>%\n  select(-address) %>%\n\n  mutate(present = 1) %>%  # Create a column to indicate presence (1)\n\n  pivot_wider(\n\n    names_from = label,    # Pivot based on the 'label' column\n\n    values_from = present,\n    values_fill = list(0)# Use the 'present' column for the values\n    \n  )\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nGrouping by postal_code: The group_by(postal_code) function groups the dataset by the postal_code column. This ensures that all rows with the same postal_code are treated as a single group for further operations.\n\nRetaining name and address: The summarise() function is used to retain the name and address columns. For each group (i.e., rows with the same postal_code), the first(name) and first(address) functions are used to keep the first occurrence of these columns. This ensures that the name and address values are preserved in the final output.\n\nSummarizing Label Columns:The across(aac:rehab, \\~ ifelse(any(. == \"yes\"), \"yes\", \"no\")) part iterates over each label column (from aac to rehab). For each group, it checks if any row within the group has a \"yes\" for that label. If at least one \"yes\" is found, the combined row will have \"yes\" in that column; otherwise, it will have \"no\".\n\nOutput:The result is a new dataset (cc_data_combined) where rows with the same postal_code are combined into a single row. The name and address columns are retained, and the label columns are summarized to reflect whether any row in the group had a \"yes\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc_data_t <- cc_data_t %>%\n  group_by(postal_code) %>%\n  summarise(\n    name = first(name),  # Retain the first occurrence of 'name'\n    address = first(address),  # Retain the first occurrence of 'address'\n    across(aac:rehab, ~ ifelse(any(. == \"yes\"), \"yes\", \"no\"))\n  )\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Adding coordinates to care centre\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_list <- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_coords <- function(add_list){\n\n  # Create a data frame to store all retrieved coordinates\n  postal_coords <- data.frame()\n    \n  for (i in add_list){\n    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data <- fromJSON(rawToChar(r$content))\n    found <- data$found\n    res <- data$results\n    \n    # Create a new data frame for each address\n    new_row <- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal <- res$POSTAL \n      lat <- res$LATITUDE\n      lng <- res$LONGITUDE\n      new_row <- data.frame(address = i, \n                           postal = postal, \n                           latitude_wgs84 = lat,  # renamed to clarify coordinate system\n                           longitude_wgs84 = lng) # renamed to clarify coordinate system\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found > 1){\n      # Remove those with NIL as postal\n      res_sub <- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row <- data.frame(address = i, \n                               postal = NA, \n                               latitude_wgs84 = NA, \n                               longitude_wgs84 = NA)\n      }\n      else{\n        top1 <- head(res_sub, n = 1)\n        postal <- top1$POSTAL \n        lat <- top1$LATITUDE\n        lng <- top1$LONGITUDE\n        new_row <- data.frame(address = i, \n                             postal = postal, \n                             latitude_wgs84 = lat, \n                             longitude_wgs84 = lng)\n      }\n    }\n    else {\n      new_row <- data.frame(address = i, \n                           postal = NA, \n                           latitude_wgs84 = NA, \n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords <- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords <- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) > 0) {\n    coords_sf <- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 <- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix <- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe with desired column names\n    valid_coords$longitude <- coords_matrix[, 1]  # SVY21 X coordinate as longitude\n    valid_coords$latitude <- coords_matrix[, 2]   # SVY21 Y coordinate as latitude\n    \n    # Merge back with rows that had NA coordinates\n    result <- merge(postal_coords, valid_coords[c(\"address\", \"longitude\", \"latitude\")], \n                   by = \"address\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result <- postal_coords\n    result$longitude <- NA  # SVY21 coordinates\n    result$latitude <- NA   # SVY21 coordinates\n  }\n  \n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- get_coords(add_list)\n```\n:::\n\n\n\nThe longtitude and latitude is then combined into geometry and the crs has been set to EPSG = 3414.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_sf <- coords %>%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE) %>%\n  select(address, postal, longitude, latitude, latitude_wgs84, longitude_wgs84)\n```\n:::\n\n\n\n### try this\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncc_data_final <- cc_data_t %>%\n  left_join(coords_sf, \n            join_by(postal_code = postal)\n)\n```\n:::\n\n\n\n## Population Data\n\n### Importing Data\n\n[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2024.ashx)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata24 <- read_csv(\"data/popdata/respopagesex2024.csv\")\n```\n:::\n\n\n\n[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2023](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2023.ashx)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata23 <- read_csv(\"data/popdata/respopagesex2023.csv\")\n```\n:::\n\n\n\n[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2022.ashx) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata22 <- read_csv(\"data/popdata/respopagesex2022.csv\")\n```\n:::\n\n\n\n::: callout-warning\n## PARSING ERROR\\*\n\n```         \nWarning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:\n  dat <- vroom(...)\n  problems(dat)Rows: 60424 Columns: 6── Column specification\n```\n:::\n\n[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2021](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2021.ashx)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata21 <- read_csv(\"data/popdata/respopagesex2021.csv\")\n```\n:::\n\n\n\n[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2011-2020](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2011to2020.ashx)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata20 <- read_csv(\"data/popdata/respopagesex2011to2020.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(popdata20)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata24 <- popdata24 %>%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata23 <- popdata23 %>%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata22 <- popdata22 %>%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata21 <- popdata21 %>%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata20 <- popdata20 %>%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n```\n:::\n\n\n\n### Changing Columns to lowercase\n\n**`names(popdata20)`**: This part of the code retrieves the current column names of the dataset `popdata20`. The `names()` function in R is used to get or set the names of an object, such as the column names of a data frame.\n\n**`tolower(names(popdata20))`**: The `tolower()` function is applied to the column names retrieved in the previous step. This function converts all characters in the names to lowercase. For example, if a column name is `\"PA\"`, it will become `\"pa\"`.\n\n**`names(popdata20) <- ...`**: This part assigns the new lowercase column names back to the dataset `popdata20`. The `<-` operator is used to update the column names of the dataset with the lowercase versions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming your dataset is already loaded as popdata20\nnames(popdata20) <- tolower(names(popdata20))\nnames(popdata21) <- tolower(names(popdata21))\nnames(popdata22) <- tolower(names(popdata22))\nnames(popdata23) <- tolower(names(popdata23))\nnames(popdata24) <- tolower(names(popdata24))\n```\n:::\n\n\n\n### Checking for Missing Values\n\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\n\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\nResults: We noticed that there are 30 missing values popdata22 specifically under the column pop.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata20_missing <- popdata20 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata20_missing)\n\npopdata21_missing <- popdata21 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata21_missing)\n\npopdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n\npopdata23_missing <- popdata23 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata23_missing)\n\npopdata24_missing <- popdata24 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata24_missing)\n```\n:::\n\n\n\n### Issue with POPDATA22\n\nUsing the below code, we are able to see clearly the rows that are affected and in the pop column, it appears as NA. The csv file (respopagesex2022.csv) was opened using excel and each row returned in the below output was then cross checked in excel. There were numbers with comma appeared in excel. This may be because read_csv() function expects a numeric value (double) in one of the columns, but instead, it found a string (the values in the column are likely formatted with commas, such as \"1,020\"). This is why the parser is raising an issue earlier on.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nna_rows <- popdata22[is.na(popdata22$pop), ]\nprint(na_rows)\n```\n:::\n\n\n\nUsing problems(), it shows details about any rows or columns that caused problems during the import. The results are a reaffirmation of the explanation above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproblems(popdata22)\n```\n:::\n\n\n\nReferencing from [Stackoverflow](https://stackoverflow.com/questions/1523126/how-to-read-data-when-some-numbers-contain-commas-as-thousand-separator), the first line of the code is necessary as it defines a new class called `\"num.with.commas\"`. This class is intended to handle numeric values that are stored as strings with commas (e.g., `\"1,000\"`). Thereafter, the second line of the code defines a method to convert a `character` type to the custom `\"num.with.commas\"` class.\n\n-   The `gsub(\",\", \"\", from)` function removes commas from the string (e.g., `\"1,000\"` becomes `\"1000\"`)\n\n-   The `as.numeric()` function then converts the cleaned string into a numeric value (e.g., `\"1000\"` becomes `1000`)\n\nThis ensures that numbers with commas are properly converted to numeric values during data import.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetClass(\"num.with.commas\")\nsetAs(\"character\", \"num.with.commas\", \n        function(from) as.numeric(gsub(\",\", \"\", from) ) )\n```\n:::\n\n\n\nThe file is then re-imported again and column types were specified.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the CSV file and specify column types\npopdata22 <- read_csv(\"data/popdata/respopagesex2022.csv\", \n                      col_types = cols(\n                        PA = col_character(),\n                        SZ = col_character(),\n                        Age = col_character(),\n                        Sex = col_character(),\n                        Pop = col_character(),\n                        Time = col_integer()  # Adjust if necessary\n                      ))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nproblems(popdata22)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(popdata22) <- tolower(names(popdata22))\npopdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n```\n:::\n\n\n\n### Duplicate Check\n\nThe code provided checks for duplicate rows in each dataset by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \\> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \\> 1) keeps only the rows that appear more than once (i.e., duplicates).\n\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\n\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for duplicates in 'aac'\npopdata20_duplicate <- popdata20 %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\nshow(popdata20_duplicate)\n\n\n# Check for duplicates in 'counselling'\npopdata21_duplicate <- popdata21 %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\nshow(popdata21_duplicate)\n\n# Check for duplicates in 'daycare'\npopdata22_duplicate <- popdata22 %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\nshow(popdata22_duplicate)\n\n# Check for duplicates in 'dementia'\npopdata23_duplicate <- popdata23 %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\nshow(popdata23_duplicate)\n\n# Check for duplicates in 'hospice'\npopdata24_duplicate <- popdata24 %>% \n  group_by_all() %>% \n  filter(n() > 1) %>% \n  ungroup()\nshow(popdata24_duplicate)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Changing 90_and_over to 90\n\n## Calculating Population for Each Age Level\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopdata24 <- popdata24 %>%\n  filter(age >= 65) %>% # Filter for age >= 65\n  group_by(age) %>%      # Group by age\n  summarise(total_pop = sum(pop)) # Summarize the total population for each age group\n\n# View the resulting dataset\nprint(popdata24)\n```\n:::\n\n\n\n## Combining Datasets\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine popdata23 and popdata24 row-wise\ncombined_data <- bind_rows(popdata23, popdata24)\n\n# View the combined dataset\nhead(combined_data)\n```\n:::\n\n\n\n# Exploratory Data Analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\ntm_shape(mpsz) +\n  tm_polygons() +\ntm_shape(CHAS) +\n  tm_dots()\n```\n:::\n\n\n\n# References\n\nTan, K. (2023). Take-home Exercise 1: Geospatial Analytics for Public Good. Retrieved from <https://isss624-kytjy.netlify.app/take-home_ex/take-home_ex1/the1#background>\n\nUrban Redevelopment Authority. (2023). Master Plan 2019 Planning Area Boundary (No Sea) (2024) \\[Dataset\\]. data.gov.sg. Retrieved February 23, 2025 from https://data.gov.sg/datasets/d_6c6d7361dd826d97b91bac914ca6b2ac/view\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}