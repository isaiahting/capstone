[
  {
    "objectID": "about.html#about-the-author.",
    "href": "about.html#about-the-author.",
    "title": "about",
    "section": "",
    "text": "Hello there! Welcome to my portfolio! :)\nI’m  Joshua TING\nA Master of IT in Business (Analytics) student with Singapore Management University\nBefore pursuing my Master’s degree, I worked as a healthcare professional in a restructured hospital. Prior to that, I obtained a Bachelor of Science in Nursing from the Singapore Institute of Technology through their university Scholars’ Programme.\nConnect with Me:\n  \nBack to Home:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accessibility of Eldercare Service In Singapore: A Geospatial Analysis",
    "section": "",
    "text": "Hello! Welcome to my Page!\nThis website is crafted in Partial Fulfillment of the Requirements for the Capstone Project Module in the Master in IT in Business (MITB) Programme at the Singapore Management University under the esteemed guidance of A/Prof KAM Tin Seong.\nMotivation in Undertaking Capstone\nInspired by the pioneering work of John Snow in epidemiology, whose spatial mapping of cholera outbreaks in the 1850s revolutionized disease tracking, I aim to apply geospatial principles to contemporary challenges, using spatial data to analyze patterns, uncover insights, and contribute to more informed decision-making in fields like public health and beyond, especially with the rise of infectious diseases.\nIn this capstone project, I aimed at ascertaining the accessibility of elderly care services in Singapore.\nLooking forward to your opinions too via Linkedin.\nBest Regards,\nJoshua TING\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about author.",
    "section": "",
    "text": "Hello there! Welcome to my portfolio! :)\nI’m  Joshua TING\nA Master of IT in Business (Analytics) student with Singapore Management University\nBefore pursuing my Master’s degree, I worked as a healthcare professional in a restructured hospital. Prior to that, I obtained a Bachelor of Science in Nursing from the Singapore Institute of Technology through their university Scholars’ Programme.\nConnect with Me:"
  },
  {
    "objectID": "methodology/data_preview.html",
    "href": "methodology/data_preview.html",
    "title": "Data Preview",
    "section": "",
    "text": "In this section, the methodology will be explained thereafter Exploratory Data Analysis will be done."
  },
  {
    "objectID": "methodology/data_preview.html#data-acquisition",
    "href": "methodology/data_preview.html#data-acquisition",
    "title": "Data Preview",
    "section": "3.1 Data Acquisition",
    "text": "3.1 Data Acquisition\n3 main spectrum of data will be required for this research, namely the Population Data, Master Plan 2019 Subzone boundary and Care Centres.\n\n\n\nFigure x: Data Overview\n\n\n\n3.1.1 WebScraping of Care Centres\nDue to the lack of a centralised data of all care centres, web scarping is warranted in obtaining the information of the care centres. The geographical locations of the Care Centres alongside the centre names such as Active Ageing Centre, Day Care, Community Rehabilitation Centre, Centre-based Nursing were extracted using a web scraping tool, Web Scraper, available in Chrome web store as Seen in Figure x. As there is no centralised file that consist of the centre names and their locations, the location of each centre has to be manually extracted from the Care Services webpage of the Agency of Integrated Care as seen in Figure x.\n\n3.1.1.1 Step 0: Download Web Scraper from Chrome web store\nWeb Scraper is used as it is free, works reasonably well and available in both Chrome and Firefox web store. In the below steps, Chrome will be the default web browser used.\n\n\n\nFigure x: Web Scraper\n\n\n\n\n3.1.1.2 Step 1: Navigate to Developer Tools in Chrome Web Browser\nAfter downloading the extension from Chrome Web Store, press onto the menu bar at the right of the browser and locate Developer Tools while onto the website you would like to scrape information from.\n\n\n\nFigure x: Web Scraper\n\n\n\n\n3.1.1.3 Step 2: Interface for Webscraper\nAfter clicking onto Developer Tools, click onto the Web Scraper in the menu bar (in black). Following which the below interface will appear.\n\n\n\n3.1.1.4 Step 3: Create New Sitemap\nClick onto “create new sitemap”, thereafter “Create Sitemap”. Sitemap Name will be the overarching term used for these information; in this instance, it will be AAC. The Start URL will be the HTML link that you would like the information to be scraped from.\n\n\n\nFigure x:\n\n\n\n\n3.1.1.5 Step 4: Add New Selector\nAfter creating a new sitemap, the following interface will appear. Click onto the “Add new selector” to select the information to scrape.\n\n\n\nFigure x:\n\n\n\n\n3.1.1.6 Step 5: Selecting Whole Box\nFirstly, the id will be the column name. For Type, select Element Attribute from the drop down selection. Thereafter, press on Select under Selector and select two boxes of each centre as seen in the figure below (the remaining boxes will be highlighted through its intelligent function) and press onto Done Selecting in the green box.\n\n\n\nFigure x:\n\n\n\n\n3.1.1.7 Step 6: Sitemap Interface\nAfter adding a new selector, the sitemap page will appear the selector that you’ve inputted.\n\n\n\nFigure x: Step 6 - Create New Sitemap\n\n\n\n\n3.1.1.8 Step 7: Selecting Name of Care Centre\nFirstly, the id will be name (with reference to the name of care centre), serving as the column name. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 names of the care centres (The remaining care centres will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple names and root parent selector will be root and press onto Save Selector.\n\n\n\nFigure x: Step 7 - Selecting Name of Care Centre\n\n\n\n\n3.1.1.9 Step 8: Create New Sitemap\nA popup window will be prompted and Group selectors was selected.\n\n\n\nFigure x: Step 8 - Create New Sitemap\n\n\n\n\n3.1.1.10 Step 9: Selecting Address of Care Centre\nSimilar to Step 7, the id will be address. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 addresses of the care centres (Remaining addresses will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple addresses and parent selector will be wrapper_for_main_name (as we grouped selectors in step 8) and press onto Save Selector.\n\n\n\nFigure x: Step 9 - Selecting Address of Care Centre\n\n\n\n\n3.1.1.11 Step 10: Data Preview\nPrior to data scraping, the data is previewed in ensuring each name of the care centre is correctly tagged to the address using the main website to verify.\n\n\n\nFigure x: Step 10 - Data Preview\n\n\n\n\n3.1.1.12 Step 11: Commence Scraping\nHead over to sitemap aac and click onto Scrape. A new browser will appear indicating that it is in process of scraping. It will be closed automatically once the process has ended.\n\n\n\nFigure x: Step 11 - Commence Scraping\n\n\n\n\n3.1.1.13 Step 11: Export Data\nExport data is selected upon clicking sitemap aac. 2 file options are offered: csv and xlsx. The former was chosen as CSV files are simple and portable which doesn’t complicate data processing. Thereafter the data will be downloaded.\n\n\n\nFigure x:\n\n\n\n\n3.1.1.14 Step 11: View CSV File\nIn ensuring the web scraping successful and accurate, the csv. file is opened and viewed.\n\n\n\nFigure x: Step 11 - View CSV File\n\n\nThe above steps were repeated for each care centre."
  },
  {
    "objectID": "methodology/data_preview.html#importing-data",
    "href": "methodology/data_preview.html#importing-data",
    "title": "Data Preview",
    "section": "3.2 Importing Data",
    "text": "3.2 Importing Data\n\n3.2.1 Importing Geospatial Data\n\n3.2.1.1 Singapore’s Master Plan 2019 Subzone Boundary\n\n\nCode Chunk\nmpsz = st_read(dsn = \"data/subzone/masterplansubzone/\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\n\n\n\nCode Chunk\nmppa &lt;- st_read(\"data/planningarea/MasterPlan2019PlanningAreaBoundaryNoSea.kml\")\n\n\n\n\n3.2.1.2 Care Centres\n\n\nCode Chunk\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\") %&gt;%\n\n\n\n\nCode Chunk\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\n\n\n\n\nCode Chunk\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n\n\n\n\nCode Chunk\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\n\n\n\n\nCode Chunk\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\n\n\n\n\nCode Chunk\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\n\n\n\n\nCode Chunk\ndementia &lt;- read_csv(\"data/carecentre/dementia.csv\")\n\n\n\n\nCode Chunk\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n\n\n\n\nCode Chunk\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\n\n\n\n\nCode Chunk\nnursinghome &lt;- read_csv(\"data/carecentre/nursinghome.csv\")\n\n\n\n\n\n3.2.2 Importing Aspatial Data\n\n\nCode Chunk\npopdata2020 &lt;- read_csv(\"data/popdata/respopagesex2024.csv\")\n\n\n\n\nCode Chunk\npopdata2020 &lt;- read_csv(\"data/popdata/respopagesex2023.csv\")\n\n\n\n\nCode Chunk\npopdata2020 &lt;- read_csv(\"data/popdata/respopagesex2022.csv\")\n\n\n\n\nCode Chunk\npopdata2020 &lt;- read_csv(\"data/popdata/respopagesex2021.csv\")\n\n\n\n\nCode Chunk\npopdata2020 &lt;- read_csv(\"data/popdata/respopagesex2011to2020.csv\")\n\n\n\n\n3.2.3 Cursory View\n\n\nCode Chunk\nglimpse(\"\")\n\n\n\n\n3.2.4 Checking for Missing Values\n\n\n3.2.5 Duplicate Check\nThe code chunk below identifies all rows in the dataframe that have an exact duplicate (i.e., another row with the same values in all columns) using group_by_all().\n\n\nCode Chunk\nduplicate &lt;- cbn %&gt;% \n  group_by_all() %&gt;% \n  filter(n()&gt;1) %&gt;% \n  ungroup()\n  \nduplicate\n\n\nThe results returned 0 duplicated records."
  },
  {
    "objectID": "methodology/data_preview.html#data-manipulation",
    "href": "methodology/data_preview.html#data-manipulation",
    "title": "Data Preview",
    "section": "3.3 Data Manipulation",
    "text": "3.3 Data Manipulation\n\n3.3.1 Seperating postal code from address\n\n\nCode Chunk\nlibrary(tidyverse)\nlibrary(stringr)\n\n# Active Ageing Centre\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\")\naac &lt;- mutate(aac, \n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Centre Based Nursing\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\nnursing &lt;- mutate(nursing,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Community Rehab Centre\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\nrehab &lt;- mutate(rehab,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Counselling\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\ncounselling &lt;- mutate(counselling,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Daycare\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\ndaycare &lt;- mutate(daycare,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Day Hospice\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\nhospice &lt;- mutate(hospice,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Dementia\ndementia &lt;- read_csv(\"data/carecentre/dementia.csv\")\ndementia &lt;- mutate(dementia,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Maintenance Daycare\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\nmaintenance &lt;- mutate(maintenance,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# NH Respite\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\nnhrespite &lt;- mutate(nhrespite,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n# Nursing Home\nnursinghome &lt;- read_csv(\"data/carecentre/nursinghome.csv\")\nnursinghome &lt;- mutate(nursinghome,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n\n\n3.3.2 Labelling each dataset appropriately\n\n\nCode Chunk\naac &lt;- aac %&gt;%\n  mutate(label = \"aac\")\n\n\n\n\n3.3.3 Apend all Care Centres into one dataset\n\n\nCode Chunk\ncc_data &lt;- bind_rows(\n  aac, \n  nursing, \n  rehab, \n  counselling, \n  daycare, \n  hospice, \n  dementia, \n  maintenance, \n  nhrespite, \n  nursinghome\n)\n\n# Optional: view the combined dataset structure\nstr(all_data)\n\n\neach data has set adddress column with data like this 10 Tampines Street 62, 528519. i want to extra the last 6 digits from the right and make it to another column named “postal_code”. do it for every dateset and row. give me the r code"
  },
  {
    "objectID": "methodology/data_preview.html#packages",
    "href": "methodology/data_preview.html#packages",
    "title": "Data Preview",
    "section": "2.1 Packages",
    "text": "2.1 Packages\n\nCode\n\n\nThe code chunk below, using p_load function of the pacman package, ensures that packages required are installed and loaded in R.\n\npacman::p_load(tidyverse, sf, httr,\n               jsonlite, rvest, dplyr, units,\n               lubridate, tmap)\n\n# -   Creates a package list containing the necessary R packages\n# -   Checks if the R packages in the package list have been installed\n# -   If not installed, will install the missing packages & launch into R environment."
  },
  {
    "objectID": "methodology/data_preview.html#geospatial-data",
    "href": "methodology/data_preview.html#geospatial-data",
    "title": "Data Preview",
    "section": "2.2 Geospatial Data",
    "text": "2.2 Geospatial Data\n\n2.2.1 Importing Singapore’s Master Plan 2019 Subzone Boundary\n\nmpsz = st_read(dsn = \"data/subzone/\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")"
  },
  {
    "objectID": "methodology/data_preview.html#care-centre",
    "href": "methodology/data_preview.html#care-centre",
    "title": "Data Preview",
    "section": "2.3 Care Centre",
    "text": "2.3 Care Centre\n\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\")\n\n\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\n\n\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\n\n\ndementia &lt;- read_csv(\"data/carecentre/dementiadaycare.csv\")\n\n\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\n\n\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n\n\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\n\n\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\n\n\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n\n\n2.3.1 Cursory View\nUsing the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns “web-scraper-order” and “web-scraper-start-url” are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.\n\nglimpse(popdata20)\n\n\n\n2.3.2 Deleting Unwanted Codes\nThe following R code is used to remove the columns “web-scraper-order” and “web-scraper-start-url” from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.\n\naac &lt;- aac %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ncounselling &lt;- counselling %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndaycare &lt;- daycare %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndementia &lt;- dementia %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nhospice &lt;- hospice %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nmaintenance &lt;- maintenance %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnhrespite &lt;- nhrespite %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnursing &lt;- nursing %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nrehab &lt;- rehab %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\n\nAfter removing the two columns, each data set has two columns, namely name and address only.\n\nglimpse(aac)\n\n\n\n2.3.3 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\n\n\n\n2.3.4 Duplicate Check\nThe code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\naac_duplicate &lt;- aac %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'counselling'\ncounselling_duplicate &lt;- counselling %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'daycare'\ndaycare_duplicate &lt;- daycare %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'dementia'\ndementia_duplicate &lt;- dementia %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'hospice'\nhospice_duplicate &lt;- hospice %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'maintenance'\nmaintenance_duplicate &lt;- maintenance %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nhrespite'\nnhrespite_duplicate &lt;- nhrespite %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nursing'\nnursing_duplicate &lt;- nursing %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'rehab'\nrehab_duplicate &lt;- rehab %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "methodology/data_preview.html#data-manipulation-cc",
    "href": "methodology/data_preview.html#data-manipulation-cc",
    "title": "Data Preview",
    "section": "2.4 Data Manipulation (CC)",
    "text": "2.4 Data Manipulation (CC)\n\n2.4.1 Seperating postal code from address\nThe code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.\n\n# Active Ageing Centre\naac &lt;-mutate(aac,\n    postal_code = str_extract(address, \"[0-9]{6}$\"),  # Extract postal code\n    address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")  # Remove postal code from address\n  )\n\n\n# Counselling\ncounselling &lt;- mutate(counselling,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Daycare\ndaycare &lt;- mutate(daycare,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Dementia\ndementia &lt;- mutate(dementia,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Day Hospice\nhospice &lt;- mutate(hospice,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Maintenance Daycare\nmaintenance &lt;- mutate(maintenance,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# NH Respite\nnhrespite &lt;- mutate(nhrespite,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Centre Based Nursing\nnursing &lt;- mutate(nursing,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Community Rehab Centre\nrehab &lt;- mutate(rehab,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address, postal_code),~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\n\n\n\n2.4.2 Labelling each dataset appropriately\nThe below code chunk adds a column and naming it as “label” in relation to the name of the dataset. This is done so we are able to combine the dataset together and identify\n\naac &lt;- aac %&gt;%\n  mutate(label = \"aac\")\n\n\ncounselling &lt;- counselling %&gt;%\n  mutate(label = \"counselling\")\n\n\ndaycare &lt;- daycare %&gt;%\n  mutate(label = \"daycare\")\n\n\ndementia &lt;- dementia %&gt;%\n  mutate(label = \"dementia\")\n\n\nhospice &lt;- hospice %&gt;%\n  mutate(label = \"hospice\")\n\n\nmaintenance &lt;- maintenance %&gt;%\n  mutate(label = \"maintenance\")\n\n\nnhrespite &lt;- nhrespite %&gt;%\n  mutate(label = \"nhrespite\")\n\n\nnursing &lt;- nursing %&gt;%\n  mutate(label = \"nursing\")\n\n\nrehab &lt;- rehab %&gt;%\n  mutate(label = \"rehab\")\n\n\n\n2.4.3 Append all Care Centres into one dataset\nThe code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named c_data using the bind_rows() function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting c_data will contain all the rows from the individual datasets, assuming they have the same column structure.\n\ncc_data &lt;- bind_rows(\n  aac, \n  counselling,\n  daycare,\n  dementia,\n  hospice,\n  maintenance,\n  nhrespite,\n  nursing,\n  rehab,\n)\n\n\n\n2.4.4 USE THIS\nIf yes is 1, no returns 0.\n\npivoted_cc_data &lt;- cc_data %&gt;%\n  select(-address) %&gt;%\n\n  mutate(present = 1) %&gt;%  # Create a column to indicate presence (1)\n\n  pivot_wider(\n\n    names_from = label,    # Pivot based on the 'label' column\n\n    values_from = present,\n    values_fill = list(0)# Use the 'present' column for the values\n    \n  )\n\nGrouping by postal_code: The group_by(postal_code) function groups the dataset by the postal_code column. This ensures that all rows with the same postal_code are treated as a single group for further operations.\nRetaining name and address: The summarise() function is used to retain the name and address columns. For each group (i.e., rows with the same postal_code), the first(name) and first(address) functions are used to keep the first occurrence of these columns. This ensures that the name and address values are preserved in the final output.\nSummarizing Label Columns:The across(aac:rehab, ~ ifelse(any(. == “yes”), “yes”, “no”)) part iterates over each label column (from aac to rehab). For each group, it checks if any row within the group has a “yes” for that label. If at least one “yes” is found, the combined row will have “yes” in that column; otherwise, it will have “no”.\nOutput:The result is a new dataset (cc_data_combined) where rows with the same postal_code are combined into a single row. The name and address columns are retained, and the label columns are summarized to reflect whether any row in the group had a “yes”.\n\ncc_data_t &lt;- cc_data_t %&gt;%\n  group_by(postal_code) %&gt;%\n  summarise(\n    name = first(name),  # Retain the first occurrence of 'name'\n    address = first(address),  # Retain the first occurrence of 'address'\n    across(aac:rehab, ~ ifelse(any(. == \"yes\"), \"yes\", \"no\"))\n  )\n\n\n\n2.4.5 Adding coordinates to care centre\n\nadd_list &lt;- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\n\nget_coords &lt;- function(add_list){\n\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (i in add_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each address\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(address = i, \n                           postal = postal, \n                           latitude_wgs84 = lat,  # renamed to clarify coordinate system\n                           longitude_wgs84 = lng) # renamed to clarify coordinate system\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found &gt; 1){\n      # Remove those with NIL as postal\n      res_sub &lt;- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row &lt;- data.frame(address = i, \n                               postal = NA, \n                               latitude_wgs84 = NA, \n                               longitude_wgs84 = NA)\n      }\n      else{\n        top1 &lt;- head(res_sub, n = 1)\n        postal &lt;- top1$POSTAL \n        lat &lt;- top1$LATITUDE\n        lng &lt;- top1$LONGITUDE\n        new_row &lt;- data.frame(address = i, \n                             postal = postal, \n                             latitude_wgs84 = lat, \n                             longitude_wgs84 = lng)\n      }\n    }\n    else {\n      new_row &lt;- data.frame(address = i, \n                           postal = NA, \n                           latitude_wgs84 = NA, \n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe with desired column names\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate as longitude\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate as latitude\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, valid_coords[c(\"address\", \"longitude\", \"latitude\")], \n                   by = \"address\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA  # SVY21 coordinates\n    result$latitude &lt;- NA   # SVY21 coordinates\n  }\n  \n  return(result)\n}\n\n\ncoords &lt;- get_coords(add_list)\n\nThe longtitude and latitude is then combined into geometry and the crs has been set to EPSG = 3414.\n\ncoords_sf &lt;- coords %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 3414, remove = FALSE) %&gt;%\n  select(address, postal, longitude, latitude, latitude_wgs84, longitude_wgs84)\n\n\n\n2.4.6 try this\n\ncc_data_final &lt;- cc_data_t %&gt;%\n  left_join(coords_sf, \n            join_by(postal_code = postal)\n)"
  },
  {
    "objectID": "methodology/data_preview.html#population-data",
    "href": "methodology/data_preview.html#population-data",
    "title": "Data Preview",
    "section": "2.5 Population Data",
    "text": "2.5 Population Data\n\n2.5.1 Importing Data\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024\n\npopdata24 &lt;- read_csv(\"data/popdata/respopagesex2024.csv\")\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2023\n\npopdata23 &lt;- read_csv(\"data/popdata/respopagesex2023.csv\")\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022 \n\npopdata22 &lt;- read_csv(\"data/popdata/respopagesex2022.csv\")\n\n\n\n\n\n\n\nPARSING ERROR*\n\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:\n  dat &lt;- vroom(...)\n  problems(dat)Rows: 60424 Columns: 6── Column specification\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2021\n\npopdata21 &lt;- read_csv(\"data/popdata/respopagesex2021.csv\")\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2011-2020\n\npopdata20 &lt;- read_csv(\"data/popdata/respopagesex2011to2020.csv\")\n\n\nglimpse(popdata20)\n\n\npopdata24 &lt;- popdata24 %&gt;%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n\n\npopdata23 &lt;- popdata23 %&gt;%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n\n\npopdata22 &lt;- popdata22 %&gt;%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n\n\npopdata21 &lt;- popdata21 %&gt;%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n\n\npopdata20 &lt;- popdata20 %&gt;%\n  mutate(Age = ifelse(Age==\"90_and_over\", \"90\", Age), \n         Age = parse_number(Age))\n\n\n\n2.5.2 Changing Columns to lowercase\nnames(popdata20): This part of the code retrieves the current column names of the dataset popdata20. The names() function in R is used to get or set the names of an object, such as the column names of a data frame.\ntolower(names(popdata20)): The tolower() function is applied to the column names retrieved in the previous step. This function converts all characters in the names to lowercase. For example, if a column name is \"PA\", it will become \"pa\".\nnames(popdata20) &lt;- ...: This part assigns the new lowercase column names back to the dataset popdata20. The &lt;- operator is used to update the column names of the dataset with the lowercase versions.\n\n# Assuming your dataset is already loaded as popdata20\nnames(popdata20) &lt;- tolower(names(popdata20))\nnames(popdata21) &lt;- tolower(names(popdata21))\nnames(popdata22) &lt;- tolower(names(popdata22))\nnames(popdata23) &lt;- tolower(names(popdata23))\nnames(popdata24) &lt;- tolower(names(popdata24))\n\n\n\n2.5.3 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\nResults: We noticed that there are 30 missing values popdata22 specifically under the column pop.\n\npopdata20_missing &lt;- popdata20 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata20_missing)\n\npopdata21_missing &lt;- popdata21 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata21_missing)\n\npopdata22_missing &lt;- popdata22 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n\npopdata23_missing &lt;- popdata23 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata23_missing)\n\npopdata24_missing &lt;- popdata24 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata24_missing)\n\n\n\n2.5.4 Issue with POPDATA22\nUsing the below code, we are able to see clearly the rows that are affected and in the pop column, it appears as NA. The csv file (respopagesex2022.csv) was opened using excel and each row returned in the below output was then cross checked in excel. There were numbers with comma appeared in excel. This may be because read_csv() function expects a numeric value (double) in one of the columns, but instead, it found a string (the values in the column are likely formatted with commas, such as “1,020”). This is why the parser is raising an issue earlier on.\n\nna_rows &lt;- popdata22[is.na(popdata22$pop), ]\nprint(na_rows)\n\nUsing problems(), it shows details about any rows or columns that caused problems during the import. The results are a reaffirmation of the explanation above.\n\nproblems(popdata22)\n\nReferencing from Stackoverflow, the first line of the code is necessary as it defines a new class called \"num.with.commas\". This class is intended to handle numeric values that are stored as strings with commas (e.g., \"1,000\"). Thereafter, the second line of the code defines a method to convert a character type to the custom \"num.with.commas\" class.\n\nThe gsub(\",\", \"\", from) function removes commas from the string (e.g., \"1,000\" becomes \"1000\")\nThe as.numeric() function then converts the cleaned string into a numeric value (e.g., \"1000\" becomes 1000)\n\nThis ensures that numbers with commas are properly converted to numeric values during data import.\n\nsetClass(\"num.with.commas\")\nsetAs(\"character\", \"num.with.commas\", \n        function(from) as.numeric(gsub(\",\", \"\", from) ) )\n\nThe file is then re-imported again and column types were specified.\n\n# Read the CSV file and specify column types\npopdata22 &lt;- read_csv(\"data/popdata/respopagesex2022.csv\", \n                      col_types = cols(\n                        PA = col_character(),\n                        SZ = col_character(),\n                        Age = col_character(),\n                        Sex = col_character(),\n                        Pop = col_character(),\n                        Time = col_integer()  # Adjust if necessary\n                      ))\n\n\nproblems(popdata22)\n\n\nnames(popdata22) &lt;- tolower(names(popdata22))\npopdata22_missing &lt;- popdata22 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n\n\n\n2.5.5 Duplicate Check\nThe code provided checks for duplicate rows in each dataset by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\npopdata20_duplicate &lt;- popdata20 %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata20_duplicate)\n\n\n# Check for duplicates in 'counselling'\npopdata21_duplicate &lt;- popdata21 %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata21_duplicate)\n\n# Check for duplicates in 'daycare'\npopdata22_duplicate &lt;- popdata22 %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata22_duplicate)\n\n# Check for duplicates in 'dementia'\npopdata23_duplicate &lt;- popdata23 %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata23_duplicate)\n\n# Check for duplicates in 'hospice'\npopdata24_duplicate &lt;- popdata24 %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata24_duplicate)"
  },
  {
    "objectID": "methodology/data_preview.html#changing-to-integer",
    "href": "methodology/data_preview.html#changing-to-integer",
    "title": "Data Preview",
    "section": "2.7 Changing to Integer",
    "text": "2.7 Changing to Integer\n\npopdata24$age &lt;- as.integer(popdata24$age)\nstr(popdata24$age)\n\npopdata23$age &lt;- as.integer(popdata23$age)\nstr(popdata23$age)\n\npopdata22$age &lt;- as.integer(popdata22$age)\nstr(popdata22$age)\n\npopdata21$age &lt;- as.integer(popdata21$age)\nstr(popdata21$age)\n\npopdata20$age &lt;- as.integer(popdata20$age)\nstr(popdata20$age)\n\n\ntable(popdata21$age, useNA = \"ifany\")"
  },
  {
    "objectID": "methodology/data_preview.html#calculating-population-for-each-age-level",
    "href": "methodology/data_preview.html#calculating-population-for-each-age-level",
    "title": "Data Preview",
    "section": "2.7 Calculating Population for Each Age Level",
    "text": "2.7 Calculating Population for Each Age Level\n\npopdata24 &lt;- popdata24 %&gt;%\n  filter(age &gt;= 65) %&gt;% # Filter for age &gt;= 65\n  group_by(age) %&gt;%      # Group by age\n  summarise(total_pop = sum(pop)) # Summarize the total population for each age group\n\n# View the resulting dataset\nprint(popdata24)"
  },
  {
    "objectID": "methodology/data_preview.html#changing-90_and_over-to-90",
    "href": "methodology/data_preview.html#changing-90_and_over-to-90",
    "title": "Data Preview",
    "section": "2.6 Changing 90_and_over to 90",
    "text": "2.6 Changing 90_and_over to 90"
  },
  {
    "objectID": "methodology/data_preview.html#combining-datasets",
    "href": "methodology/data_preview.html#combining-datasets",
    "title": "Data Preview",
    "section": "2.8 Combining Datasets",
    "text": "2.8 Combining Datasets\n\n# Combine popdata23 and popdata24 row-wise\ncombined_data &lt;- bind_rows(popdata23, popdata24)\n\n# View the combined dataset\nhead(combined_data)"
  },
  {
    "objectID": "methodology/data_preview.html#care-centre-extract-transform-load",
    "href": "methodology/data_preview.html#care-centre-extract-transform-load",
    "title": "Data Preview",
    "section": "2.3 Care Centre (EXTRACT TRANSFORM LOAD)",
    "text": "2.3 Care Centre (EXTRACT TRANSFORM LOAD)\n\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\")\n\n\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\n\n\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\n\n\ndementia &lt;- read_csv(\"data/carecentre/dementiadaycare.csv\")\n\n\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\n\n\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n\n\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\n\n\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\n\n\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n\n\n2.3.1 Cursory View\nUsing the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns “web-scraper-order” and “web-scraper-start-url” are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.\n\nglimpse(popdata20)\n\n\n\n2.3.2 Deleting Unwanted Codes\nThe following R code is used to remove the columns “web-scraper-order” and “web-scraper-start-url” from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.\n\naac &lt;- aac %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ncounselling &lt;- counselling %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndaycare &lt;- daycare %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndementia &lt;- dementia %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nhospice &lt;- hospice %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nmaintenance &lt;- maintenance %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnhrespite &lt;- nhrespite %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnursing &lt;- nursing %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nrehab &lt;- rehab %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\n\nAfter removing the two columns, each data set has two columns, namely name and address only.\n\nglimpse(aac)\n\n\n\n2.3.3 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\n\n\n\n2.3.4 Duplicate Check\nThe code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\naac_duplicate &lt;- aac %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'counselling'\ncounselling_duplicate &lt;- counselling %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'daycare'\ndaycare_duplicate &lt;- daycare %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'dementia'\ndementia_duplicate &lt;- dementia %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'hospice'\nhospice_duplicate &lt;- hospice %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'maintenance'\nmaintenance_duplicate &lt;- maintenance %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nhrespite'\nnhrespite_duplicate &lt;- nhrespite %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nursing'\nnursing_duplicate &lt;- nursing %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'rehab'\nrehab_duplicate &lt;- rehab %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "Data Preview",
    "section": "",
    "text": "In this section, we will acquire the data sets from various government open-sources data repository and websites. Thereafter, we will install the necessary R packages and import the data sets. In each data set, we will delve into the necessary checks, issues faced and steps into resolving the issue. Thereafter, exploratory data analysis is done."
  },
  {
    "objectID": "methodology.html#installing-packages",
    "href": "methodology.html#installing-packages",
    "title": "Data Preview",
    "section": "3.1 Installing Packages",
    "text": "3.1 Installing Packages"
  },
  {
    "objectID": "methodology.html#geospatial-data",
    "href": "methodology.html#geospatial-data",
    "title": "Data Preview",
    "section": "2.2 Geospatial Data",
    "text": "2.2 Geospatial Data\n\n2.2.1 Importing Singapore’s Master Plan 2019 Subzone Boundary\n\nmpsz = st_read(dsn = \"data/subzone/\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\n\nplot(mpsz)"
  },
  {
    "objectID": "methodology.html#care-centre-extract-transform-load",
    "href": "methodology.html#care-centre-extract-transform-load",
    "title": "Data Preview",
    "section": "2.3 Care Centre (EXTRACT TRANSFORM LOAD)",
    "text": "2.3 Care Centre (EXTRACT TRANSFORM LOAD)\n\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\")\n\n\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\n\n\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\n\n\ndementia &lt;- read_csv(\"data/carecentre/dementiadaycare.csv\")\n\n\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\n\n\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n\n\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\n\n\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\n\n\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n\n\n2.3.1 Cursory View\nUsing the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns “web-scraper-order” and “web-scraper-start-url” are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.\n\nglimpse(aac)\n\n\n\n2.3.2 Deleting Unwanted Codes\nThe following R code is used to remove the columns “web-scraper-order” and “web-scraper-start-url” from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.\n\naac &lt;- aac %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ncounselling &lt;- counselling %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndaycare &lt;- daycare %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndementia &lt;- dementia %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nhospice &lt;- hospice %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nmaintenance &lt;- maintenance %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnhrespite &lt;- nhrespite %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnursing &lt;- nursing %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nrehab &lt;- rehab %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\n\nAfter removing the two columns, each data set has two columns, namely name and address only.\n\nglimpse(aac)\n\n\n\n2.3.3 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\n\n\n\n2.3.4 Duplicate Check\nThe code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\naac_duplicate &lt;- aac %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'counselling'\ncounselling_duplicate &lt;- counselling %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'daycare'\ndaycare_duplicate &lt;- daycare %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'dementia'\ndementia_duplicate &lt;- dementia %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'hospice'\nhospice_duplicate &lt;- hospice %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'maintenance'\nmaintenance_duplicate &lt;- maintenance %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nhrespite'\nnhrespite_duplicate &lt;- nhrespite %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nursing'\nnursing_duplicate &lt;- nursing %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'rehab'\nrehab_duplicate &lt;- rehab %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "methodology.html#data-manipulation-cc",
    "href": "methodology.html#data-manipulation-cc",
    "title": "Data Preview",
    "section": "2.5 Data Manipulation (CC)",
    "text": "2.5 Data Manipulation (CC)\n\n2.5.1 Separating postal code from address\nThe code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.\n\n# Active Ageing Centre\naac &lt;-mutate(aac,\n    postal_code = str_extract(address, \"[0-9]{6}$\"),  # Extract postal code\n    address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")  # Remove postal code from address\n  )\n\n\n# Counselling\ncounselling &lt;- mutate(counselling,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Daycare\ndaycare &lt;- mutate(daycare,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Dementia\ndementia &lt;- mutate(dementia,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Day Hospice\nhospice &lt;- mutate(hospice,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Maintenance Daycare\nmaintenance &lt;- mutate(maintenance,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# NH Respite\nnhrespite &lt;- mutate(nhrespite,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Centre Based Nursing\nnursing &lt;- mutate(nursing,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Community Rehab Centre\nrehab &lt;- mutate(rehab,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address, postal_code),~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\n\n\n\n2.5.2 Labelling each dataset appropriately\nThe below code chunk adds a column and naming it as “label” in relation to the name of the dataset hence we are able to identify the type of services provided by the care centres.\n\naac &lt;- aac %&gt;%\n  mutate(label = \"aac\")\n\n\ncounselling &lt;- counselling %&gt;%\n  mutate(label = \"counselling\")\n\n\ndaycare &lt;- daycare %&gt;%\n  mutate(label = \"daycare\")\n\n\ndementia &lt;- dementia %&gt;%\n  mutate(label = \"dementia\")\n\n\nhospice &lt;- hospice %&gt;%\n  mutate(label = \"hospice\")\n\n\nmaintenance &lt;- maintenance %&gt;%\n  mutate(label = \"maintenance\")\n\n\nnhrespite &lt;- nhrespite %&gt;%\n  mutate(label = \"nhrespite\")\n\n\nnursing &lt;- nursing %&gt;%\n  mutate(label = \"nursing\")\n\n\nrehab &lt;- rehab %&gt;%\n  mutate(label = \"rehab\")\n\n\n\n2.5.3 Append all Care Centres into one dataset\nThe code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named c_data using the bind_rows() function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting c_data will contain all the rows from the individual datasets, assuming they have the same column structure.\n\ncc_data &lt;- bind_rows(\n  aac, \n  counselling,\n  daycare,\n  dementia,\n  hospice,\n  maintenance,\n  nhrespite,\n  nursing,\n  rehab,\n)\n\n\n\n2.5.4 Transforming Categorical Data to Binary Indicator\nThis code transforms the dataset cc_data from a long format to a wide format by pivoting on the categorical values in the label column, effectively converting them into binary indicator columns. The process begins by removing the address column using select(-address) to exclude it from the transformation. Next, a new column called present is created using mutate(present = 1), where every row is assigned a value of 1 to indicate the presence of a label. The key reshaping operation is performed using pivot_wider(), which spreads the unique values from the label column into separate columns. The names_from = label argument specifies that the new column names should be derived from the distinct categories in label, while values_from = present fills these new columns with the corresponding 1s from the present column. Any missing combinations (where a particular label does not appear for a given record) are automatically filled with 0s due to the values_fill = list(0) argument. The final output, stored in pivoted_cc_data, is a wider dataframe where each original row now has binary flags (1 or 0) indicating the presence or absence of each label category, making it suitable for analyses that require a one-hot encoded or dummy variable representation of categorical data.\ncorrect code\n\npivoted_cc_data &lt;- cc_data %&gt;%\n  select(-address) %&gt;%\n\n  mutate(present = 1) %&gt;%  # Create a column to indicate presence (1)\n\n  pivot_wider(\n\n    names_from = label,    # Pivot based on the 'label' column\n\n    values_from = present,\n    values_fill = list(0)# Use the 'present' column for the values\n    \n  )\n\nusing arrange(0, we are able to see that that the centres are arranged in alphabetically order and they are similar ones.\n\narrange(pivoted_cc_data)\n\n\npivoted_cc_data$name[duplicated(pivoted_cc_data$name)]\n\n\nduplicate_rows &lt;- pivoted_cc_data %&gt;%\n  # Count occurrences of each name\n  add_count(name) %&gt;%\n  # Filter for names that appear more than once\n  filter(n &gt; 1) %&gt;%\n  # Remove the count column\n  select(-n) %&gt;%\n  # Arrange by name for better readability\n  arrange(name)\n\nif (nrow(duplicate_rows) &gt; 0) {\n  print(duplicate_rows)\n} else {\n  message(\"No duplicates found in the 'name' column.\")\n}\n\n\n\n2.5.5 Adding coordinates to care centre\n\nadd_list &lt;- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\ntry this\n\nget_coords &lt;- function(postal_list){\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (postal in postal_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=postal,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each postal code\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal_code &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(postal_code = postal, \n                           postal_found = postal_code, \n                           latitude_wgs84 = lat,\n                           longitude_wgs84 = lng)\n    }\n    \n    # If multiple results, use the exact postal code match\n    else if (found &gt; 1){\n      # Find exact match for postal code\n      res_match &lt;- res[res$POSTAL == postal, ]\n      \n      # If exact match found, use it\n      if (nrow(res_match) &gt; 0) {\n        postal_code &lt;- res_match$POSTAL[1]\n        lat &lt;- res_match$LATITUDE[1]\n        lng &lt;- res_match$LONGITUDE[1]\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = postal_code,\n                             latitude_wgs84 = lat,\n                             longitude_wgs84 = lng)\n      }\n      # If no exact match, set as NA\n      else {\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = NA,\n                             latitude_wgs84 = NA,\n                             longitude_wgs84 = NA)\n      }\n    }\n    # If no results found\n    else {\n      new_row &lt;- data.frame(postal_code = postal,\n                           postal_found = NA,\n                           latitude_wgs84 = NA,\n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate\n    \n    # Add geometry column\n    valid_coords$geometry &lt;- st_geometry(coords_svy21)\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, \n                   valid_coords[c(\"postal_code\", \"longitude\", \"latitude\", \"geometry\")], \n                   by = \"postal_code\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA\n    result$latitude &lt;- NA\n    result$geometry &lt;- NA\n  }\n  \n  return(result)\n}\n\n\ncoords &lt;- get_coords(add_list)\n\nLeft join is used to where both data sets share the same properties of postal code.\n\ncc_data_final &lt;- pivoted_cc_data %&gt;%\n  left_join(coords, \n            join_by(postal_code == postal_code)\n)\n\n?st join\n\ncc_mpsz &lt;- cc_data_final %&gt;%\n  left_join(mpsz,\n            join_by(geometry == geometry)\n)\n\ndoesnt work\n\ncc_mpsz &lt;- st_as_sf(cc_data_final, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "methodology.html#population-data-1",
    "href": "methodology.html#population-data-1",
    "title": "Data Preview",
    "section": "3.2 Population Data",
    "text": "3.2 Population Data\n\n3.2.1 Overview\nIn this section, a total of 4 population datasets from year 2020 to year 2024 will be used in the analysis. The datasets will be cleaned and transformed. Following which, survival analysis will be done in estimating the population for age 60 and above for the period 2025 to 2029.\n\n\n3.2.2 Importing Data\nIn importing data, read_csv() and rename_with() of tidyverse package are used to perform column name standardisation by converting all variable names in the respective datasets to lowercase.\n\nPopulation Data 2024Population Data 2023Population Data 2022Population Data 2021Population Data 2020\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024\n\npopdata24 &lt;- read_csv(\"data/popdata/respopagesex2024.csv\") %&gt;%\n  rename_with(tolower)\n\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2023\n\npopdata23 &lt;- read_csv(\"data/popdata/respopagesex2023.csv\") %&gt;%\n  rename_with(tolower)\n\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022 \n\npopdata22 &lt;- read_csv(\"data/popdata/respopagesex2022.csv\") %&gt;%\n  rename_with(tolower)\n\n\n\n\n\n\n\nWarning\n\n\n\nPARSING ERROR*\nWarning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:\n  dat &lt;- vroom(...)\n  problems(dat)Rows: 60424 Columns: 6── Column specification\n\n\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2021\n\npopdata21 &lt;- read_csv(\"data/popdata/respopagesex2021.csv\") %&gt;%\n  rename_with(tolower)\n\n\n\nSingapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2011-2020\n\npopdata20 &lt;- read_csv(\"data/popdata/respopagesex2011to2020.csv\") %&gt;%\n  rename_with(tolower) %&gt;%\n  filter(time == 2020)\n\n\n\n\n\nglimpse(popdata23)\n\n\n\n3.2.3 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\nResults: We noticed that there are 30 missing values popdata22 specifically under the column pop.\n\npopdata20_missing &lt;- popdata20 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata20_missing)\n\npopdata21_missing &lt;- popdata21 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata21_missing)\n\npopdata22_missing &lt;- popdata22 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n\npopdata23_missing &lt;- popdata23 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata23_missing)\n\npopdata24_missing &lt;- popdata24 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata24_missing)\n\n\n\n3.2.4 Issue with POPDATA22\nUsing the below code, we are able to see clearly the rows that are affected and in the pop column, it appears as NA. The csv file (respopagesex2022.csv) was opened using excel and each row returned in the below output was then cross checked in excel. Whole numbers with comma appeared in excel. This may be because read_csv() function expects a numeric value (double) in one of the columns, but instead, it found a string (the values in the column are likely formatted with commas, such as “1,020”). This is why the parser is raising an issue earlier on.\n\nna_rows &lt;- popdata22[is.na(popdata22$pop), ]\nprint(na_rows)\n\nReferencing from Stackoverflow, the first line of the code is necessary as it defines a new class called \"num.with.commas\". This class is intended to handle numeric values that are stored as strings with commas (e.g., \"1,000\"). Thereafter, the second line of the code defines a method to convert a character type to the custom \"num.with.commas\" class.\n\nThe gsub(\",\", \"\", from) function removes commas from the string (e.g., \"1,000\" becomes \"1000\")\nThe as.numeric() function then converts the cleaned string into a numeric value (e.g., \"1000\" becomes 1000)\n\nThis ensures that numbers with commas are properly converted to numeric values during data import.\n\nsetClass(\"num.with.commas\")\nsetAs(\"character\", \"num.with.commas\", \n        function(from) as.numeric(gsub(\",\", \"\", from) ) )\n\nThe file is then re-imported again and specifically, the column ‘pop’ is parsed as a character field in facilitating the next step in removing commas within the population itself.\n\npopdata22 &lt;- read_csv(\"data/popdata/respopagesex2022.csv\", \n                      col_types = cols(\n                        PA = col_character(),\n                        SZ = col_character(),\n                        Age = col_character(),\n                        Sex = col_character(),\n                        Pop = col_character(),\n                        Time = col_number()  # Adjust if necessary\n                      )) %&gt;%\n  rename_with(tolower)\n\nAs previously stated, commas are present in the ‘pop’ column, hence, mutate()\n\npopdata22 &lt;- popdata22 %&gt;%\n  mutate(pop = as.numeric(str_replace_all(pop, \",\", \"\")))\n\nIn the below codechunk, it was verified that there is no missing values and the above steps taken were successful.\n\nnames(popdata22) &lt;- tolower(names(popdata22))\npopdata22_missing &lt;- popdata22 %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))\nprint(popdata22_missing)\n\nIn the below code chunk a & b, we noticed that it returned two different outputs: 90_and_over and 90_and_Over. This may explain why the error NAs introduced by coercion was returned.\n\n#code chunk a\npopdata22 %&gt;% \n  summarise(max_age = max(age, na.rm = TRUE))\n\n\n#code chunk b\npopdata24 %&gt;% \n  summarise(max_age = max(age, na.rm = TRUE))\n\nHence, in addressing the above issue, the below code chunk was developed. First, The code defines a function called convert_age that takes a dataframe (df) as input. Inside the function, it modifies the age column using mutate(). It checks each value in the age column to see if it contains either “_and_over” or “_and_Over” (case-insensitive match). When a match is found, it extracts just the numeric part (e.g., “90” from “90_and_over”) using str_extract(). If no match is found, it keeps the original value. The second mutate() converts the cleaned age column to numeric values, ensuring all ages are stored as numbers. The function returns the modified dataframe with standardised age values.\n\nconvert_age &lt;- function(df) {\n  df %&gt;%\n    mutate(age = if_else(\n      str_detect(age, regex(\"_and_Over|_and_over\", ignore_case = TRUE)),\n      str_extract(age, \"\\\\d+\"),  # Extract just the numeric part\n      age\n    )) %&gt;%\n    mutate(age = as.numeric(age))\n}\n\n\npopdata20_c &lt;- convert_age(popdata20)\npopdata21_c &lt;- convert_age(popdata21)\npopdata22_c &lt;- convert_age(popdata22)\npopdata23_c &lt;- convert_age(popdata23)\npopdata24_c &lt;- convert_age(popdata24)\n\nAnother layer of confirmation of missing values was executed in ensuring no missing values were returned during the abovementioned process and it returns 0 for each dataset.\n\npopdata20_missing &lt;- popdata20_c %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) \nprint(popdata20_missing)\npopdata21_missing &lt;- popdata21_c %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) \nprint(popdata21_missing)\npopdata22_missing &lt;- popdata22_c %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) \nprint(popdata22_missing)\npopdata23_missing &lt;- popdata23_c %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) \nprint(popdata23_missing)\npopdata24_missing &lt;- popdata24_c %&gt;% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) \nprint(popdata24_missing)\n\n\n\n3.2.5 Duplicate Check\nThe code provided checks for duplicate rows in each dataset by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\npopdata20_duplicate &lt;- popdata20_c %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata20_duplicate)\n\n\n# Check for duplicates in 'counselling'\npopdata21_duplicate &lt;- popdata21_c %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata21_duplicate)\n\n# Check for duplicates in 'daycare'\npopdata22_duplicate &lt;- popdata22_c %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata22_duplicate)\n\n# Check for duplicates in 'dementia'\npopdata23_duplicate &lt;- popdata23_c %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata23_duplicate)\n\n# Check for duplicates in 'hospice'\npopdata24_duplicate &lt;- popdata24_c %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\nshow(popdata24_duplicate)\n\n\n\n3.2.6 Percentage of Missing Values\nIn pop(year)_geri_zero, the dataframe consists of older adults aged 60 & above where the population is zero meanwhile pop20_geri consists of the full dataframe of the older adults aged 60 & above. Lastly, the code chunk returns the percentage of missing values in df. From 2020 to 2024, there are 43 - 41% of missing values.\n\npop20_geri_zero &lt;- popdata20_c %&gt;%\n  filter(pop == 0, age&gt;=60)\npop20_geri &lt;- popdata20_c %&gt;%\n  filter(age&gt;=60)\n(nrow(pop20_geri_zero) / nrow(pop20_geri)) * 100\n\npop21_geri_zero &lt;- popdata21_c %&gt;%\n  filter(pop == 0, age&gt;=60)\npop21_geri &lt;- popdata21_c %&gt;%\n  filter(age&gt;=60)\n(nrow(pop21_geri_zero) / nrow(pop21_geri)) * 100\n\npop22_geri_zero &lt;- popdata22_c %&gt;%\n  filter(pop == 0, age&gt;=60)\npop22_geri &lt;- popdata22_c %&gt;%\n  filter(age&gt;=60)\n(nrow(pop22_geri_zero) / nrow(pop22_geri)) * 100\n\npop23_geri_zero &lt;- popdata23_c %&gt;%\n  filter(pop == 0, age&gt;=60)\npop23_geri &lt;- popdata23_c %&gt;%\n  filter(age&gt;=60)\n(nrow(pop23_geri_zero) / nrow(pop23_geri)) * 100\n\npop24_geri_zero &lt;- popdata24_c %&gt;%\n  filter(pop == 0, age&gt;=60)\npop24_geri &lt;- popdata24_c %&gt;%\n  filter(age&gt;=60)\n(nrow(pop24_geri_zero) / nrow(pop24_geri)) * 100\n\n\n\n3.2.7 Survival Analysis\n\n3.2.7.1 Introduction\nIn this section, the population data spanning from 2020 to 2024 will undergo cleaning and transformation processes to prepare it for advanced analytical modeling. Following this data preparation phase, survival analysis methodologies will be employed to predict the older adult population demographics from 2025 to 2029. The primary objective of this analytical approach is to calculate survival rates, where death serves as the event of interest in our survival framework. Given the nature of the population data being utilised, we adopt a non-parametric analytical stance, implementing the Kaplan-Meier estimator as our core statistical method for survival probability estimation. A fundamental assumption underlying this analysis is that residents maintain residential stability within their respective subzones throughout the entire study period. Under this assumption, any observed decrease in population within a given subzone can be reasonably attributed to mortality events, as migration and relocation factors are held constant. This methodological framework allows for a more accurate assessment of demographic changes driven primarily by natural population dynamics rather than residential mobility patterns.\n\n\n3.2.7.2 Step 0: Defining clean_age_columnFunction\nThis code defines a function called clean_age_column that takes a dataframe as input and processes its age column to ensure consistent numeric values. First, it trims any leading or trailing whitespace from the age column using str_trim. Next, it replaces the label \"90_and_over\" with \"90\" to standardise the representation of ages 90 and above. Then, it converts the age column to numeric values using as.numeric, while suppressing any warnings that might arise from non-numeric entries (e.g., empty strings or invalid values). Finally, the function filters out any rows where the age could not be converted to a numeric value (resulting in NA), ensuring only valid numeric ages remain in the dataframe. The cleaned dataframe is then returned as the output. This function ensures uniformity and removing invalid entries.\n\nclean_age_column &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      age = str_trim(age),  # Trim whitespace\n      age = if_else(age == \"90_and_over\", \"90\", age),  # Replace label\n      age = suppressWarnings(as.numeric(age))  # Convert safely\n    ) %&gt;%\n    filter(!is.na(age))  # Remove rows that still couldn't be converted\n}\n\n\n\n3.2.7.3 Step 1: Load data\nFirst, we will read the population data from year 2020 to 2024 of the rds file. Thereafter, we will apply clean_age_column() function that was defined in step 0 to the loaded dataframe.\n\npop20 &lt;- read_rds(\"data/rds/popdata/refined/popdata20_c.rds\") %&gt;% \n  clean_age_column()\npop21 &lt;- read_rds(\"data/rds/popdata/refined/popdata21_c.rds\") %&gt;% \n  clean_age_column()\npop22 &lt;- read_rds(\"data/rds/popdata/refined/popdata22_c.rds\") %&gt;% \n  clean_age_column()\npop23 &lt;- read_rds(\"data/rds/popdata/refined/popdata23_c.rds\") %&gt;% \n  clean_age_column()\npop24 &lt;- read_rds(\"data/rds/popdata/refined/popdata24_c.rds\") %&gt;% \n  clean_age_column()\n\n\n\n3.2.7.4 Step 2: Compute survival rates for each year-to-year transition\nThis code defines a function called compute_survival_rate that calculates survival rates between two consecutive time periods (e.g., years) for a population dataset. The function takes two dataframes (df1 and df2) as inputs, representing population data from two different time points (e.g., 2020 and 2021).\nFirst, the function filters df1 to include only individuals aged 59 to 90, as survival analysis is often focused on older populations. It then increments the age by 1 (using mutate(age = age + 1)) to align the ages with the next time period (df2). Next, it performs an inner join between the modified df1 and df2 using matching columns (age, sex, pa [possibly a region code], and sz. The join suffixes (_prev and _next) distinguish between the population counts from the two time periods.\nAfter joining, the function computes the survival rate (rate) by dividing the population in the later period (pop_next) by the population in the earlier period (pop_prev). Finally, it selects and returns only the relevant columns (pa, sz, age, sex, rate, pop_prev, pop_next) for further analysis.\nIn summary, this function helps estimate how many people from an initial cohort (in df1) survived into the next period (in df2) by age, sex, and other groupings, providing key insights for demographic or actuarial studies.\n\ncompute_survival_rate &lt;- function(df1, df2) {\n  df1 %&gt;%\n    filter(age &gt;= 55 & age &lt; 91) %&gt;%\n    mutate(age = age + 1) %&gt;%\n    inner_join(df2, \n               by = c(\"age\", \"sex\", \"pa\", \"sz\"),\n               suffix = c(\"_prev\", \"_next\")) %&gt;%\n    mutate(rate = pop_next / pop_prev) %&gt;%\n    select(pa, sz, age, sex, rate, pop_prev, pop_next)\n}\n\n\nrates_2020_2021 &lt;- compute_survival_rate(pop20, pop21)\nrates_2021_2022 &lt;- compute_survival_rate(pop21, pop22)\nrates_2022_2023 &lt;- compute_survival_rate(pop22, pop23)\nrates_2023_2024 &lt;- compute_survival_rate(pop23, pop24)\n\nHowever, upon closer inspection of the df rates_2020_2021, we noticed the column rate consists of Inf rate. Upon closer examination of the columns pop_past and `pop_next`, there is an increase of population which is not logical. This predicament is consistent throughout the remaining 3 dataframes too. Therefore, step 2 needs to be refined in addressing this issue.\n\nrates_2020_2021[is.infinite(rates_2020_2021$rate), ]\n\n\n\n3.2.7.5 Step 2: Refined Computing Survival Rates\nThe function processes survival rate data through four sequential steps, now correctly using dplyr’s grouping mechanism. First, it groups the data by both age and sex, which is crucial because survival rates likely vary significantly across these demographics. Within these groups, it performs three key operations: (1) capping any rates above 1.0 at 1.0 (assuming these represent survival probabilities that shouldn’t exceed 100%), (2) replacing infinite values (which occur when dividing by zero) with the maximum finite rate found in the data, and (3) imputing missing values (NaN) with the median rate for that specific age-sex group. After these grouped operations, it ungroups the data and performs a final safety check, replacing any remaining missing values with 1.0 (a neutral value indicating no change).\n\nclean_age_column &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      age = str_trim(age),  # Trim whitespace\n      age = if_else(age == \"90_and_over\", \"90\", age),  # Replace label\n      age = suppressWarnings(as.numeric(age))  # Convert safely\n    ) %&gt;%\n    filter(!is.na(age))  # Remove rows that still couldn't be converted\n}\npop20 &lt;- read_rds(\"data/rds/popdata/refined/popdata20_c.rds\") %&gt;% \n  clean_age_column()\npop21 &lt;- read_rds(\"data/rds/popdata/refined/popdata21_c.rds\") %&gt;% \n  clean_age_column()\npop22 &lt;- read_rds(\"data/rds/popdata/refined/popdata22_c.rds\") %&gt;% \n  clean_age_column()\npop23 &lt;- read_rds(\"data/rds/popdata/refined/popdata23_c.rds\") %&gt;% \n  clean_age_column()\npop24 &lt;- read_rds(\"data/rds/popdata/refined/popdata24_c.rds\") %&gt;% \n  clean_age_column()\n\n\ncompute_survival_rate &lt;- function(df1, df2) {\n  df1 %&gt;%\n    filter(age &gt;= 55 & age &lt; 91) %&gt;%\n    mutate(age = age + 1) %&gt;%\n    inner_join(df2, \n               by = c(\"age\", \"sex\", \"pa\", \"sz\"),\n               suffix = c(\"_prev\", \"_next\")) %&gt;%\n    mutate(rate = pop_next / pop_prev) %&gt;%\n    select(pa, sz, age, sex, rate, pop_prev, pop_next)\n}\n\nThe clean_rates function serves as a natural post-processing step for the output of compute_survival_rate, addressing several statistical and data quality considerations inherent to survival rate calculations. The compute_survival_rate function generates initial survival rate estimates by computing population ratios between two time periods (pop_next/pop_prev) for specific demographic groups (defined by age, sex, pa, and sz). This calculation can produce three types of problematic outputs that clean_rates systematically addresses: (1) undefined values (infinities) resulting from zero denominators, (2) rates outside the valid [0,1] range for probabilities, and (3) missing values from the inner join operation or zero populations.\nThe cleaning process employs statistically appropriate methods: replacement of infinite values with NAs followed by median imputation within age-sex strata maintains the grouped structure used in the original calculation. The bounding of values preserves the probabilistic interpretation of survival rates. The two functions together implement a complete analytical workflow where compute_survival_rate performs the demographic-specific calculation and clean_rates ensures the results meet necessary statistical assumptions for subsequent analysis. This separation of concerns between calculation and validation follows established data processing paradigms, with each function handling a distinct phase of the data transformation pipeline.\n\nclean_rates &lt;- function(df, rate_type = \"survival\") {\n  df %&gt;%\n    group_by(age, sex) %&gt;%\n    mutate(\n      # Handle infinites first - replace with NA for proper imputation\n      rate = ifelse(is.infinite(rate), NA, rate),\n      \n      # Detect illogical population scenario: pop_next &gt; pop_prev\n      # This suggests population growth rather than mortality/migration\n      pop_growth_detected = ifelse(exists(\"pop_next\") && exists(\"pop_prev\"), \n                                  pop_next &gt; pop_prev, FALSE),\n      \n      # If population growth detected, impute rate based on rate_type\n      rate = case_when(\n        pop_growth_detected & rate_type == \"survival\" ~ 1.0,  # Perfect survival\n        pop_growth_detected & rate_type == \"probability\" ~ 0.0,  # Zero probability of death\n        pop_growth_detected & rate_type == \"hazard\" ~ 0.0,  # Zero hazard rate\n        pop_growth_detected ~ NA_real_,  # Set to NA for other rate types\n        TRUE ~ rate  # Keep original rate if no population growth issue\n      ),\n      \n      # Cap rates only if they represent probabilities\n      rate = if(rate_type == \"survival\" || rate_type == \"probability\") {\n        pmin(pmax(rate, 0), 1)  # Bound between 0 and 1\n      } else {\n        pmax(rate, 0)  # Only ensure non-negative for hazard rates\n      },\n      \n      # Impute missing values with group median\n      rate = ifelse(is.na(rate), median(rate, na.rm = TRUE), rate)\n    ) %&gt;%\n    ungroup() %&gt;%\n    # Final check - if entire groups had no valid data\n    mutate(\n      rate = ifelse(is.na(rate), \n                   if(rate_type == \"survival\") 1.0 else 0.0, \n                   rate)\n    ) %&gt;%\n    # Clean up temporary column\n    select(-pop_growth_detected)\n}\n\n\nrates_2020_2021 &lt;- compute_survival_rate(pop20, pop21) %&gt;% clean_rates()\n\n\nrates_2021_2022 &lt;- compute_survival_rate(pop21, pop22) %&gt;% clean_rates()\n\n\nrates_2022_2023 &lt;- compute_survival_rate(pop22, pop23) %&gt;% clean_rates()\n\n\nrates_2023_2024 &lt;- compute_survival_rate(pop23, pop24) %&gt;% clean_rates()\n\n\n\n3.2.7.6 Step 3: Average the survival rates\nThis code calculates average survival rates across multiple years by combining data from four different time periods and then computing the mean rates for each demographic and geographic group. bind_rows() function stacks the four datasets (rates_2020_2021, rates_2021_2022, rates_2022_2023, and rates_2023_2024) vertically into one combined dataset, essentially appending all the rows together. Next, the group_by() function groups the combined data by four variables: age, sex, planning area (pa), and subzone (sz), creating distinct groups for each unique combination of these characteristics. The summarise() function then calculates the average survival rate for each group by taking the mean of the ‘rate’ column, while the na.rm = TRUE parameter ensures that any missing values are excluded from the calculation, and .groups = \"drop\" ungroups the data after summarizing. Finally, the distinct() function removes any potential duplicate rows by keeping only unique combinations of age, sex, pa, and sz, while .keep_all = TRUE preserves all other columns in the dataset.\n\navg_survival_rates &lt;- bind_rows(\n  rates_2020_2021,\n  rates_2021_2022,\n  rates_2022_2023,\n  rates_2023_2024\n) %&gt;%\n  group_by(age, sex, pa, sz) %&gt;%\n  summarise(avg_rate = mean(rate, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  distinct(age, sex, pa, sz, .keep_all = TRUE) #remove potential duplicates \n\n\n\n3.2.7.7 Step 4: Forecast each year 2025 to 2029\nThe forecast_year function defines a vector of required column names (age, sex, pa, sz, pop) that must be present in the input dateset. The function uses a conditional check with all() and %in% operators to verify that every required column exists in the base population dataset. If any columns are missing, the function immediately terminates execution using stop() and provides a detailed error message that specifically lists which columns are absent, calculated using setdiff() to find the difference between required and available columns.\nNext, the function creates a cleaned version of the base population data by first converting the population column to numeric format using as.numeric(). It then removes any rows where the population value is NA (missing), as these would create errors in subsequent calculations. The select(all_of(required_cols)) ensures that only the necessary columns are retained, reducing memory usage and eliminating potential conflicts from extraneous variables.\nIn step 3, the aggregation step addresses the possibility of duplicate records or multiple entries for the same demographic-geographic combination. By grouping the data by age, sex, planning area, and subzone, then summing the population values, the function ensures that each unique combination has a single, consolidated population count. This is particularly important in demographic data where the same group might appear multiple times due to data collection methods or administrative boundaries. The .groups = \"drop\" parameter automatically ungroups the data after summarization, preventing issues in subsequent operations.\nIn step 4 of the code chunk, for individuals aged 55-88, the function implements a cohort-component method where each age group advances by one year (age + 1), simulating the passage of time. The function then performs a left join with the survival rates dataset to match each demographic-geographic group with their appropriate survival rate. This join operation is critical because survival rates typically vary by age, sex, and location due to factors like healthcare access, socioeconomic conditions, and environmental factors. When survival rates are missing for specific groups, the function implements a robust fallback mechanism using coalesce() combined with the median survival rate from the entire rates dataset. This approach prevents data loss while providing a reasonable estimate based on the overall population’s survival characteristics. The population projection is then calculated by multiplying the current population by the survival rate, effectively reducing each cohort based on expected mortality.\nIn step 5, The treatment of individuals aged 89 and above reflects demographic modeling conventions where very elderly populations are often grouped together due to small sample sizes and similar demographic characteristics. The function consolidates all individuals aged 89 and above into a single age category (90), recognizing that survival patterns become more uniform at advanced ages. This approach uses survival rates specific to age 90, and when these rates are unavailable, it applies the median survival rate specifically calculated from age-90 data rather than the entire dataset, providing a more age-appropriate estimate.\nIn step 6, the function combines the forecasted populations from both the main aging process (ages 60-89) and the elderly category (90+) using bind_rows(), creating a comprehensive forecast dataset. It adds the forecast year as a new column, enabling tracking of temporal changes across multiple projection periods. The final aggregation step groups by all relevant variables including the year and sums any duplicate combinations that might have emerged from the separate processing of different age ranges, ensuring data consistency and preventing double-counting.\nIn the second code chunk, the forecasting loop implements a cohort-component projection method where each year’s forecast becomes the foundation for the next year’s projection. This approach captures the cumulative effects of demographic change over time, as population structure evolves with each iteration. The loop begins with 2024 population data (pop24) as the baseline, representing the most recent observed population distribution. The tryCatch() mechanism provides comprehensive error handling that prevents the entire forecasting process from failing due to issues with individual years. When errors occur, the system captures detailed debugging information including the data structure of the current population using str(), which reveals variable types, dimensions, and sample values. Each successful forecast year produces a new dataset that replaces the current population for the next iteration, creating a sequential chain of demographic projections.\n\n#1st code chunk\nforecast_year &lt;- function(base_pop, rates, year) {\n  # 1. Validate and clean input data\n  required_cols &lt;- c(\"age\", \"sex\", \"pa\", \"sz\", \"pop\")\n  if(!all(required_cols %in% names(base_pop))) {\n    stop(paste(\"Missing required columns:\", \n              setdiff(required_cols, names(base_pop))))\n  }\n  \n  # 2. Convert pop to numeric safely\n  base_pop_clean &lt;- base_pop %&gt;%\n    mutate(pop = as.numeric(pop)) %&gt;%\n    filter(!is.na(pop)) %&gt;%  # Remove NA cases\n    select(all_of(required_cols))\n  \n  # 3. Summarize with careful NA handling\n  base_pop_agg &lt;- base_pop_clean %&gt;%\n    group_by(age, sex, pa, sz) %&gt;%\n    summarise(pop = sum(pop, na.rm = TRUE), .groups = \"drop\")\n  \n  # 4. Forecast for ages 59-88\n  next_pop &lt;- base_pop_agg %&gt;%\n    filter(between(age, 55, 88)) %&gt;%\n    mutate(age = age + 1) %&gt;%\n    left_join(\n      rates %&gt;% select(age, sex, pa, sz, avg_rate), \n      by = c(\"age\", \"sex\", \"pa\", \"sz\")\n    ) %&gt;%\n    mutate(\n      avg_rate = coalesce(avg_rate, median(rates$avg_rate, na.rm = TRUE)),\n      pop = pop * avg_rate\n    ) %&gt;%\n    select(all_of(required_cols))\n  \n  # 5. Handle age 89+\n  pop_90 &lt;- base_pop_agg %&gt;%\n    filter(age &gt;= 89) %&gt;%\n    mutate(age = 90) %&gt;%\n    left_join(\n      rates %&gt;% filter(age == 90) %&gt;% select(sex, pa, sz, avg_rate),\n      by = c(\"sex\", \"pa\", \"sz\")\n    ) %&gt;%\n    mutate(\n      avg_rate = coalesce(avg_rate, median(rates$avg_rate[rates$age == 90], na.rm = TRUE)),\n      pop = pop * avg_rate\n    ) %&gt;%\n    select(all_of(required_cols))\n  \n  # 6. Combine results\n  bind_rows(next_pop, pop_90) %&gt;%\n    mutate(year = year) %&gt;%\n    group_by(year, age, sex, pa, sz) %&gt;%\n    summarise(pop = sum(pop, na.rm = TRUE), .groups = \"drop\")\n}\n\n#2nd code chunk\n# Run forecasting with error handling\nforecast_list &lt;- list()\ncurrent_pop &lt;- pop24 %&gt;% \n  select(age, sex, pa, sz, pop) %&gt;%\n  mutate(pop = as.numeric(pop))\n\nfor (y in 2025:2029) {\n  tryCatch({\n    forecast &lt;- forecast_year(current_pop, avg_survival_rates, y)\n    forecast_list[[as.character(y)]] &lt;- forecast\n    current_pop &lt;- forecast %&gt;% select(age, sex, pa, sz, pop)\n    message(\"Successfully forecasted year \", y)\n  }, error = function(e) {\n    message(\"Error forecasting year \", y, \": \", e$message)\n    # Debugging info:\n    message(\"Current population data structure:\")\n    print(str(current_pop))\n    message(\"Sample of problematic data:\")\n    print(head(current_pop[current_pop$pop != as.numeric(current_pop$pop), ]))\n  })\n}\n\n\n\n3.2.7.8 Step 5: Loop over the years\nThis code creates an iterative population forecasting system that projects demographic changes from 2025 to 2029. An empty list called forecast_list is initialized to store the forecast results for each year. The variable base_pop is set to pop24, which contains the 2024 population data used as the starting point. The for loop iterates through years 2025 to 2029, calling the forecast_year() function for each year with the current base population, survival rates, and target year as inputs. Each forecast result is stored in forecast_list using the year as the key. After each forecast, base_pop is updated with the projected population data from the current year, selecting only the essential columns (pa, sz, age, sex, pop). This creates a sequential chain where each year’s forecast becomes the input for the next year. This iterative approach captures cumulative demographic effects, as mortality and aging impacts from each year carry forward to subsequent projections. The final result is five separate population forecasts stored in forecast_list, representing projected demographics for each year from 2025-2029.\n\n# List to store forecasts\nforecast_list &lt;- list()\nbase_pop &lt;- pop24\n\nfor (y in 2025:2029) {\n  next_forecast &lt;- forecast_year(base_pop, avg_survival_rates, y)\n  forecast_list[[as.character(y)]] &lt;- next_forecast\n  base_pop &lt;- next_forecast %&gt;% select(pa, sz, age, sex,  pop)\n}\n\n\n\n\n3.2.8 Final Output of Population Forecast\nThis code combines all the individual yearly forecasts into a single comprehensive dataset and ensures population values are whole numbers. The bind_rows(forecast_list) function takes all the separate forecast datasets stored in the list (one for each year from 2025-2029) and stacks them vertically into one dataframe, preserving all columns and adding all rows together. The mutate(pop = ceiling(pop)) applies the ceiling function to round all population values up to the nearest integer, since population counts must be whole numbers and any fractional people resulting from the survival rate calculations need to be converted to realistic integer values. The ceiling function always rounds upward, so a value like 1547.3 becomes 1548, ensuring no population is lost due to rounding and maintaining conservative population estimates. The result is a single dataset containing all forecasted population data across all years, with properly formatted integer population counts ready for analysis or visualisation.\n\nall_forecasts &lt;- bind_rows(forecast_list) %&gt;%\n  mutate(pop = ceiling(pop))  # Round UP to nearest integer\n\n\n# Convert from long to wide format\npivoted_data &lt;- all_forecasts %&gt;%\n  select(year, age, sex, pa, sz, pop) %&gt;%  # Ensure only necessary columns\n  pivot_wider(\n    names_from = year,\n    values_from = pop,\n    names_prefix = \"pop_\",  # Adds \"pop_\" before each year\n    values_fill = NA        # Fill missing combinations with NA\n  ) %&gt;%\n  arrange(pa, sz, sex, age)  # Sort logically\n\nThe data aggregation process began by filtering the pivoted dataset using filter(age &gt;= 60) to include only individuals aged 60 years and above. The filtered data was then grouped by subzone using group_by(sz) to enable population calculations at the geographic level. For each subzone, the total elderly population was calculated using the summarise() function, which created new variables (aged_2025, aged_2026, aged_2027, aged_2028, aged_2029) by summing the population counts for both males and females across the five-year projection period from 2025 to 2029. The sum() function with the parameter na.rm = TRUE was used to handle missing values by excluding them from the calculations to ensure accurate totals. The .groups = 'drop' parameter was included to remove the grouping structure after summarization. This resulted in a summary dataset where each row represents a unique subzone, with columns containing the projected elderly population counts for each year from 2025 through 2029. The original age and sex variables were removed during this aggregation process as they were no longer needed, creating a streamlined dataset focused on subzone-level elderly population projections.\n\n# Aggregate population for ages 60 and above by subzone (combining males and females)\ngeri_forecast &lt;- pivoted_data %&gt;%\n  filter(age &gt;= 60) %&gt;%\n  group_by(sz) %&gt;%\n  summarise(\n    aged_2025 = sum(pop_2025, na.rm = TRUE),\n    aged_2026 = sum(pop_2026, na.rm = TRUE),\n    aged_2027 = sum(pop_2027, na.rm = TRUE),\n    aged_2028 = sum(pop_2028, na.rm = TRUE),\n    aged_2029 = sum(pop_2029, na.rm = TRUE),\n    .groups = 'drop'\n  )"
  },
  {
    "objectID": "methodology.html#prof",
    "href": "methodology.html#prof",
    "title": "Data Preview",
    "section": "3.6 PROF",
    "text": "3.6 PROF\n###Population Data\n\nclean_age_column &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      age = str_trim(age),  # Trim whitespace\n      age = if_else(age == \"90_and_over\", \"90\", age),  # Replace label\n      age = suppressWarnings(as.numeric(age))  # Convert safely\n    ) %&gt;%\n    filter(!is.na(age))  # Remove rows that still couldn't be converted\n}"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Background Singapore is an aged society where 1 in 4 of the population are above 65 years of age."
  },
  {
    "objectID": "methodology.html#singapores-master-plan-2019-subzone-boundary-mpsz",
    "href": "methodology.html#singapores-master-plan-2019-subzone-boundary-mpsz",
    "title": "Data Preview",
    "section": "3.4 Singapore’s Master Plan 2019 Subzone Boundary (MPSZ)",
    "text": "3.4 Singapore’s Master Plan 2019 Subzone Boundary (MPSZ)\nUsing st_read, the ESRI shapefile was imported and it contains 323 data entries and 15 fields. Each of the data entry consists of a multi-polygon shape, with geospatial coordinates with a geographic coordinate system (GCS) of WGS84. The features of GCS include using a 3D spherical model of earth with coordinates of longitude, latitude and altitude whereas PCS uses a 2D plane model with linear measurements (i.e. metres).\n\nmpsz = st_read(dsn = \"data/subzone/\",\n               layer = \"MP14_SUBZONE_NO_SEA_PL\")\nhead(mpsz, n=5)\n\n\nplot(mpsz[\"SUBZONE_N\"])\n\n\n3.4.1 Transforming CRS\nAs the research’s focus is exploring the accessibility of care centres in Singapore, PCS would be appropriate in this context as it measures the distance between the elderly’ residence and the care centre. Thus, in ensuring accurate measurement, the function `st_transform` with a crs of 3414 was used (Kam, 2022) in the code chunk below.\n\nmpsz &lt;- mpsz %&gt;%\n  st_transform(crs = 3414)\n\n\n\n3.4.2 Shared Boundaries\nThe below codechunk uses st_is_valid in assessing if there are shared boundaries in the mpsz data. This is important as shared boundaries can lead to inconsistent data impacting the research findings. 9 polygons with self-intersection (‘Ring Self-Intersection’) issues were returned.\n\nst_is_valid(mpsz, reason = TRUE)\n\nThis code chunk visualises the boundaries that are affected such as the smaller islands outside of Singapore Main Island: P. Ubin, P. Tekong, Sentosa Island\n\ninvalid_polygons &lt;- mpsz[!st_is_valid(mpsz),]\nplot(invalid_polygons)\n\nIn addressing the above point, we will use st_buffer() of sf package to compute a 5-metres buffers around the data.\n\nmpsz &lt;- st_buffer(mpsz, dist = 5)\n\n\n\n3.4.3 Sampling Grid\nIn measuring spatial accessibility, analytical grids of the sampling fields is used to standardise the means of measurement (Kam, 2022). These analytical grids can be comprised of equilateral triangles, squares or hexagon due to its ability to tessellate (ESRI, 2025). From the code output below, we noticed that the sampling fields in the subzones of mpsz are not equal for measurement. Therefore, it is imperative to select an appropriate analytical grid in measuring spatial accessibility. Hexagons grids is chosen over triangles and squares due to several factors. Firstly, the shape of a hexagon has a low-perimeter-to-area ratio, hence the edge effect of the grid shape reduces sampling bias. Secondly, when comparing equal area, any point inside a hexagon is closer to the centriod than any given point in an equal-area square or triangle due to the more acute angles of square and triangle versus the hexagon (Burdziej, 2018; ESRI, 2025). This is particular important in this research as we are using the centroid as a proxy in comparing accessibility to the various care centres. The centre of each hexagon is an Origin, additionally, acting as a starting point in ascertaining the shortest distance to the care centres.\n\nplot(mpsz[\"SUBZONE_N\"])\n\nIn the HealthierSG White Paper 2022, the Ministry of Health has indicated its intention to “expand the network [”care centres”] to 220 by 2025”. Furthermore, through the Ministry’s estimation, 8 in 10 seniors will have a care centre in the vicinity of their homes (Ministry of Health, 2022). Hence, we will assume that the maximum distance to the care centres are 100 metres. Hence, in the code chunk below, st_make_grid from sf package constructed hexagonal grids encompassing the Singapore Master Plan 2019 Planning Subzone Boundary using cellsize that defines the radius of 100 metres and square to be false to generate a hexagonal grid.\n\nhex_grid &lt;- st_make_grid(mpsz,\n                         cellsize = 100,\n                         what = \"polygon\",\n                         square = FALSE) %&gt;%\n  st_sf()\n\n\nhex_grid$hex_id &lt;- sprintf(\"H%04d\", seq_len(nrow(hex_grid))) %&gt;% as.factor()\nhead(hex_grid)\n\n\nhex_centroids &lt;- st_centroid(hex_grid)\n\n\n\n3.4.4 MPSZ Land Use\n\nmpsz_land &lt;- st_read(\"data/planningarea/AmendmenttoMasterPlan2019LandUselayer.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\nst_crs(mpsz_land)\n\n\nst_is_valid(mpsz_land, reason = TRUE)\n\n\nplot(mpsz_land[\"Name\"])\n\nunable to cross check\n\nmpsz_forecast &lt;- mpsz %&gt;%\n  left_join(all_forecasts, by = c(\"SUBZONE_N\" = \"sz\")\n\nCross-checking subzones in MPSZ & Forecast data.\n\ncombined_original_case &lt;- mpsz %&gt;%\n  distinct(SUBZONE_N) %&gt;%\n  mutate(join_key = toupper(SUBZONE_N)) %&gt;%\n  full_join(\n    all_forecasts %&gt;%\n      distinct(sz) %&gt;%\n      mutate(join_key = toupper(sz)),\n    by = \"join_key\"\n  ) %&gt;%\n  select(SUBZONE_N, sz) %&gt;%\n  arrange(SUBZONE_N, sz)"
  },
  {
    "objectID": "methodology.html#care-centre",
    "href": "methodology.html#care-centre",
    "title": "Data Preview",
    "section": "4.1 Care Centre",
    "text": "4.1 Care Centre\n\n4.1.1 Overview\n\naac &lt;- read_csv(\"data/carecentre/activeageingcentre.csv\")\n\n\ncounselling &lt;- read_csv(\"data/carecentre/counselling.csv\")\n\n\ndaycare &lt;- read_csv(\"data/carecentre/daycare.csv\")\n\n\ndementia &lt;- read_csv(\"data/carecentre/dementiadaycare.csv\")\n\n\nhospice &lt;- read_csv(\"data/carecentre/dayhospice.csv\")\n\n\nmaintenance &lt;- read_csv(\"data/carecentre/maintenancedaycare.csv\")\n\n\nnhrespite &lt;- read_csv(\"data/carecentre/nhrespite.csv\")\n\n\nnursing &lt;- read_csv(\"data/carecentre/centrebasednursing.csv\")\n\n\nrehab &lt;- read_csv(\"data/carecentre/communityrehabcentre.csv\")\n\n\n\n4.1.2 Cursory View\nUsing the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns “web-scraper-order” and “web-scraper-start-url” are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.\n\nglimpse(aac)\n\n\n\n4.1.3 Deleting Unwanted Codes\nThe following R code is used to remove the columns “web-scraper-order” and “web-scraper-start-url” from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.\n\naac &lt;- aac %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ncounselling &lt;- counselling %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndaycare &lt;- daycare %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\ndementia &lt;- dementia %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nhospice &lt;- hospice %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nmaintenance &lt;- maintenance %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnhrespite &lt;- nhrespite %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nnursing &lt;- nursing %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\nrehab &lt;- rehab %&gt;% select(-\"web-scraper-order\", -\"web-scraper-start-url\")\n\nAfter removing the two columns, each data set has two columns, namely name and address only.\n\nglimpse(aac)\n\n\n\n4.1.4 Checking for Missing Values\nTo check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.\nThe is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address), ~sum(is.na(.))))\n\n\n\n4.1.5 Duplicate Check\nThe code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() &gt; 1). The n() function counts the number of occurrences for each combination of values, and filter(n() &gt; 1) keeps only the rows that appear more than once (i.e., duplicates).\nFor each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.\nThus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.\n\n# Check for duplicates in 'aac'\naac_duplicate &lt;- aac %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'counselling'\ncounselling_duplicate &lt;- counselling %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'daycare'\ndaycare_duplicate &lt;- daycare %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'dementia'\ndementia_duplicate &lt;- dementia %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'hospice'\nhospice_duplicate &lt;- hospice %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'maintenance'\nmaintenance_duplicate &lt;- maintenance %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nhrespite'\nnhrespite_duplicate &lt;- nhrespite %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'nursing'\nnursing_duplicate &lt;- nursing %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# Check for duplicates in 'rehab'\nrehab_duplicate &lt;- rehab %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n\n\n4.1.6 Separating postal code from address\nThe code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.\n\n# Active Ageing Centre\naac &lt;-mutate(aac,\n    postal_code = str_extract(address, \"[0-9]{6}$\"),  # Extract postal code\n    address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")  # Remove postal code from address\n  )\n\n\n# Counselling\ncounselling &lt;- mutate(counselling,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Daycare\ndaycare &lt;- mutate(daycare,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Dementia\ndementia &lt;- mutate(dementia,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Day Hospice\nhospice &lt;- mutate(hospice,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Maintenance Daycare\nmaintenance &lt;- mutate(maintenance,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# NH Respite\nnhrespite &lt;- mutate(nhrespite,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Centre Based Nursing\nnursing &lt;- mutate(nursing,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Community Rehab Centre\nrehab &lt;- mutate(rehab,\n  postal_code = str_extract(address, \"[0-9]{6}$\"),\n  address = str_remove(address, \"[,]?\\\\s*[0-9]{6}$\")\n)\n\n\n# Checking for missing or null values in 'name' and 'address' columns\naac_missing &lt;- aac %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ncounselling_missing &lt;- counselling %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndaycare_missing &lt;- daycare %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\ndementia_missing &lt;- dementia %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nhospice_missing &lt;- hospice %&gt;% summarise(across(c(name, address, postal_code),~sum(is.na(.))))\nmaintenance_missing &lt;- maintenance %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnhrespite_missing &lt;- nhrespite %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nnursing_missing &lt;- nursing %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\nrehab_missing &lt;- rehab %&gt;% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))\n\n\n\n4.1.7 Labelling Dataset\nThe below code chunk adds a column and naming it as “label” in relation to the name of the dataset hence we are able to identify the type of services provided by the care centres.\n\naac &lt;- aac %&gt;%\n  mutate(label = \"aac\")\n\n\ncounselling &lt;- counselling %&gt;%\n  mutate(label = \"counselling\")\n\n\ndaycare &lt;- daycare %&gt;%\n  mutate(label = \"daycare\")\n\n\ndementia &lt;- dementia %&gt;%\n  mutate(label = \"dementia\")\n\n\nhospice &lt;- hospice %&gt;%\n  mutate(label = \"hospice\")\n\n\nmaintenance &lt;- maintenance %&gt;%\n  mutate(label = \"maintenance\")\n\n\nnhrespite &lt;- nhrespite %&gt;%\n  mutate(label = \"nhrespite\")\n\n\nnursing &lt;- nursing %&gt;%\n  mutate(label = \"nursing\")\n\n\nrehab &lt;- rehab %&gt;%\n  mutate(label = \"rehab\")\n\n\n\n4.1.8 Append all Care Centres into one dataset\nThe code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named c_data using the bind_rows() function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting c_data will contain all the rows from the individual datasets, assuming they have the same column structure.\n\ncc_data &lt;- bind_rows(\n  aac, \n  counselling,\n  daycare,\n  dementia,\n  hospice,\n  maintenance,\n  nhrespite,\n  nursing,\n  rehab,\n)\n\n\n\n4.1.9 Transforming Categorical Data to Binary Indicator\nThis code transforms the dataset cc_data from a long format to a wide format by pivoting on the categorical values in the label column, effectively converting them into binary indicator columns. The process begins by removing the address column using select(-address) to exclude it from the transformation. Next, a new column called present is created using mutate(present = 1), where every row is assigned a value of 1 to indicate the presence of a label. The key reshaping operation is performed using pivot_wider(), which spreads the unique values from the label column into separate columns. The names_from = label argument specifies that the new column names should be derived from the distinct categories in label, while values_from = present fills these new columns with the corresponding 1s from the present column. Any missing combinations (where a particular label does not appear for a given record) are automatically filled with 0s due to the values_fill = list(0) argument. The final output, stored in pivoted_cc_data, is a wider dataframe where each original row now has binary flags (1 or 0) indicating the presence or absence of each label category, making it suitable for analyses that require a one-hot encoded or dummy variable representation of categorical data.\n\npivoted_cc_data &lt;- cc_data %&gt;%\n  select(-address) %&gt;%\n\n  mutate(present = 1) %&gt;%  # Create a column to indicate presence (1)\n\n  pivot_wider(\n\n    names_from = label,    # Pivot based on the 'label' column\n\n    values_from = present,\n    values_fill = list(0)# Use the 'present' column for the values\n    \n  )\n\nusing arrange(0, we are able to see that that the centres are arranged in alphabetically order and they are similar ones.\n\narrange(pivoted_cc_data)\n\n\npivoted_cc_data$name[duplicated(pivoted_cc_data$name)]\n\n\nduplicate_rows &lt;- pivoted_cc_data %&gt;%\n  # Count occurrences of each name\n  add_count(name) %&gt;%\n  # Filter for names that appear more than once\n  filter(n &gt; 1) %&gt;%\n  # Remove the count column\n  select(-n) %&gt;%\n  # Arrange by name for better readability\n  arrange(name)\n\nif (nrow(duplicate_rows) &gt; 0) {\n  print(duplicate_rows)\n} else {\n  message(\"No duplicates found in the 'name' column.\")\n}\n\n\n\n4.1.10 Adding coordinates to care centre\nThis code prepares a list of unique postal codes from a dataset called pivoted_cc_data to be used for geocoding via an API. The line first extracts the postal_code column from the dataframe, then applies unique() to eliminate duplicate postal codes - this optimization reduces the number of API calls needed since the same postal code will return the same coordinates. The sort() function then arranges these unique postal codes in ascending order, which serves two purposes: it makes the list more organized for human review (easier to locate specific codes during debugging or verification), and it may help with processing efficiency when matching the geocoded results back to the original dataset. The resulting sorted unique list is stored in add_list, which can then be passed to a geocoding API (like the OneMap API in Singapore) that typically accepts individual addresses or postal codes rather than entire dataframes. This preprocessing step is crucial because APIs often have rate limits or usage constraints, so minimizing duplicate requests helps maximize efficiency and reduce potential errors or bottlenecks in the geocoding process. The comment “parse a list as API cannot read df” explicitly notes that this conversion from dataframe column to simple list is necessary because the target API expects individual values rather than dataframe structures as input.\n\nadd_list &lt;- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df\n#unique reduces records to pass to portal\n#sort is used to easier to find geo codes\n\nThe below codechunk defines a function called get_coords that takes a list of Singapore postal codes as input and retrieves their geographic coordinates from the OneMap API (a Singapore government mapping service). The function first initializes an empty data frame to store the results. For each postal code in the input list, it makes an HTTP GET request to the OneMap API, which returns the location data in JSON format. The function then processes the response differently depending on how many matches are found: if there’s exactly one match, it extracts those coordinates; if there are multiple matches, it looks for an exact postal code match; and if no matches are found, it records NA values. The valid coordinates (in WGS84 latitude/longitude format) are converted into an sf (simple features) spatial object, which is then transformed to Singapore’s SVY21 projected coordinate system (EPSG:3414). The function extracts these SVY21 coordinates and merges them back into the original data frame, preserving any rows that had invalid or missing coordinates. The final output is a data frame containing the original postal codes, any matching postal codes found, WGS84 coordinates, SVY21 coordinates, and the geometric data as an sf geometry column. This function is particularly useful for geocoding Singapore addresses and preparing spatial data for analysis with Singapore-specific geographic information systems.\n\nget_coords &lt;- function(postal_list){\n  # Create a data frame to store all retrieved coordinates\n  postal_coords &lt;- data.frame()\n    \n  for (postal in postal_list){\n    r &lt;- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=postal,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data &lt;- fromJSON(rawToChar(r$content))\n    found &lt;- data$found\n    res &lt;- data$results\n    \n    # Create a new data frame for each postal code\n    new_row &lt;- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal_code &lt;- res$POSTAL \n      lat &lt;- res$LATITUDE\n      lng &lt;- res$LONGITUDE\n      new_row &lt;- data.frame(postal_code = postal, \n                           postal_found = postal_code, \n                           latitude_wgs84 = lat,\n                           longitude_wgs84 = lng)\n    }\n    \n    # If multiple results, use the exact postal code match\n    else if (found &gt; 1){\n      # Find exact match for postal code\n      res_match &lt;- res[res$POSTAL == postal, ]\n      \n      # If exact match found, use it\n      if (nrow(res_match) &gt; 0) {\n        postal_code &lt;- res_match$POSTAL[1]\n        lat &lt;- res_match$LATITUDE[1]\n        lng &lt;- res_match$LONGITUDE[1]\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = postal_code,\n                             latitude_wgs84 = lat,\n                             longitude_wgs84 = lng)\n      }\n      # If no exact match, set as NA\n      else {\n        new_row &lt;- data.frame(postal_code = postal,\n                             postal_found = NA,\n                             latitude_wgs84 = NA,\n                             longitude_wgs84 = NA)\n      }\n    }\n    # If no results found\n    else {\n      new_row &lt;- data.frame(postal_code = postal,\n                           postal_found = NA,\n                           latitude_wgs84 = NA,\n                           longitude_wgs84 = NA)\n    }\n    \n    # Add the row\n    postal_coords &lt;- rbind(postal_coords, new_row)\n  }\n  \n  # Convert to sf object with WGS84 coordinates (EPSG:4326)\n  # Filter out rows with NA coordinates first\n  valid_coords &lt;- postal_coords[!is.na(postal_coords$latitude_wgs84) & \n                              !is.na(postal_coords$longitude_wgs84), ]\n  \n  if(nrow(valid_coords) &gt; 0) {\n    coords_sf &lt;- st_as_sf(valid_coords, \n                         coords = c(\"longitude_wgs84\", \"latitude_wgs84\"),\n                         crs = 4326)\n    \n    # Transform to SVY21 (EPSG:3414)\n    coords_svy21 &lt;- st_transform(coords_sf, 3414)\n    \n    # Extract coordinates\n    coords_matrix &lt;- st_coordinates(coords_svy21)\n    \n    # Add SVY21 coordinates back to the original dataframe\n    valid_coords$longitude &lt;- coords_matrix[, 1]  # SVY21 X coordinate\n    valid_coords$latitude &lt;- coords_matrix[, 2]   # SVY21 Y coordinate\n\n    \n    # Merge back with rows that had NA coordinates\n    result &lt;- merge(postal_coords, \n                   valid_coords[c(\"postal_code\", \"longitude\", \"latitude\")], \n                   by = \"postal_code\", all.x = TRUE)\n  } else {\n    # If no valid coordinates, add empty SVY21 columns\n    result &lt;- postal_coords\n    result$longitude &lt;- NA\n    result$latitude &lt;- NA\n  }\n  \n  return(result)\n}\n\nThe code coords &lt;- get_coords(add_list) calls the previously defined get_coords() function to geocode (convert to geographic coordinates) a list of Singapore postal codes stored in add_list.\n\ncoords &lt;- get_coords(add_list)\n\nThe below code chunk merges the original dataset (pivoted_cc_data) with the geocoded coordinates (coords) using a left join operation, which preserves all records from the primary dataset while matching and appending geographic data where available using the properties of postal_code in both dataframes.\n\ncc_data_final &lt;- pivoted_cc_data %&gt;%\n  left_join(coords, \n            join_by(postal_code == postal_code)\n)\n\n\ncc_sf &lt;- st_as_sf(cc_data_final,\n                  coords = c(\"longitude\", \"latitude\"), #c is use column\n                         crs = 3414)\n\n\nmpsz_land_fixed &lt;- st_make_valid(mpsz_land)\n\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_land_fixed) +\n  tm_polygons()\n\n\ntmap_mode(\"plot\")\ntm_shape(mpsz) +\n  tm_polygons() +\ntm_shape(cc_sf) +\n  tm_dots() #change to name\n\n\nst_crs(cc_sf)\n\n\ntm_shape(mpsz)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "methodology.html#road-network",
    "href": "methodology.html#road-network",
    "title": "Data Preview",
    "section": "3.3 Road Network",
    "text": "3.3 Road Network\nImporting road network\n\nroadnetwork = st_read(\"data/roadnetwork/RoadNetwork.kml\") %&gt;%\n  st_transform(crs=3414)\n\n\nmpsz_road &lt;- st_join(mpsz, roadnetwork, left = TRUE)\n\n\nplot(mpsz_road)"
  },
  {
    "objectID": "methodology.html#density-of-care-centre-in-each-subzone",
    "href": "methodology.html#density-of-care-centre-in-each-subzone",
    "title": "Data Preview",
    "section": "5.2 Density of Care Centre in Each Subzone",
    "text": "5.2 Density of Care Centre in Each Subzone\nrefer chapt 1\n\nmpsz$`CC Count` &lt;- lengths(st_intersects(mpsz, ))"
  },
  {
    "objectID": "methodology.html#population-data",
    "href": "methodology.html#population-data",
    "title": "Data Preview",
    "section": "2.2 Population Data",
    "text": "2.2 Population Data\nThe population datasets are retrieved from SingStat.gov.sg. Specifically, Singapore Resident by Planning Area/Subzone, Single Year of Age and Sex, June 2024, 2023, 2022, 2021, and, 2011-2020 are selected for this research. As the version January 2025 will be not be ready in due time for this research project, thus, the latest available dataset, June 2024 will be used."
  },
  {
    "objectID": "methodology.html#webscraping-of-care-centres",
    "href": "methodology.html#webscraping-of-care-centres",
    "title": "Data Preview",
    "section": "2.3 WebScraping of Care Centres",
    "text": "2.3 WebScraping of Care Centres\nDue to the lack of a centralised data of all care centres, web scraping is warranted in retrieving the geographical information of the care centres. The geographical locations of the Care Centres alongside the centre names such as Active Ageing Centre, Day Care, Community Rehabilitation Centre, Centre-based Nursing were extracted using a web scraping tool, Web Scraper, available in Chrome web store as Seen in Figure x. As there is no centralised file that consist of the centre names and their locations, the location of each centre has to be manually extracted, filtered by each centre type, from the Care Services webpage of the Agency of Integrated Care as seen in Figure x.\nStep 0: Download Web Scraper from Chrome web store\nWeb Scraper is used as it is free, works reasonably well and available in both Chrome and Firefox web store. In the below steps, Chrome will be the default web browser used.\n\n\n\nFigure 2: Web Scraper\n\n\nStep 1: Navigate to Developer Tools in Chrome Web Browser\nAfter downloading the extension from Chrome Web Store, press onto the menu bar at the right of the browser and locate Developer Tools while onto the website you would like to scrape information from.\n\n\n\nFigure 3: Web Scraper\n\n\nStep 2: Interface for Webscraper\nAfter clicking onto Developer Tools, click onto the Web Scraper in the menu bar (in black). Following which the below interface will appear.\n\n\n\nFigure 4: Interface with Website\n\n\nStep 3: Create New Sitemap\nClick onto “create new sitemap”, thereafter “Create Sitemap”. Sitemap Name will be the overarching term used for these information; in this instance, it will be AAC. The Start URL will be the HTML link that you would like the information to be scraped from.\n\n\n\nFigure 5: Creating New Sitemap\n\n\nStep 4: Add New Selector\nAfter creating a new sitemap, the following interface will appear. Click onto the “Add new selector” to select the information to scrape.\n\n\n\nFigure 6: Adding New Selector\n\n\nStep 5: Selecting Whole Box\nFirstly, the id will be the column name. For Type, select Element Attribute from the drop down selection. Thereafter, press on Select under Selector and select two boxes of each centre as seen in the figure below (the remaining boxes will be highlighted through its intelligent function) and press onto Done Selecting in the green box.\n\n\n\nFigure 7: Selecting Element\n\n\nStep 6: Sitemap Interface\nAfter adding a new selector, the sitemap page will appear the selector that you’ve inputted.\n\n\n\nFigure 8: Create New Sitemap\n\n\nStep 7: Selecting Name of Care Centre\nFirstly, the id will be name (with reference to the name of care centre), serving as the column name. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 names of the care centres (The remaining care centres will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple names and root parent selector will be root and press onto Save Selector.\n\n\n\nFigure 10: Selecting Name of Care Centre\n\n\nStep 8: Misconfiguration Detected\nA popup window will be prompted and Group selectors was selected.\n\n\n\nFigure 11: Misconfiguration Detected\n\n\nStep 9: Selecting Address of Care Centre\nSimilar to Step 7, the id will be address. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 addresses of the care centres (Remaining addresses will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple addresses and parent selector will be wrapper_for_main_name (as we grouped selectors in step 8) and press onto Save Selector.\n\n\n\nFigure 12: Selecting Address of Care Centre\n\n\nStep 10: Data Preview\nPrior to data scraping, the data is previewed in ensuring each name of the care centre is correctly tagged to the address using the main website to verify.\n\n\n\nFigure 13: Data Preview\n\n\nStep 11: Commence Scraping\nHead over to sitemap aac and click onto Scrape. A new browser will appear indicating that it is in process of scraping. It will be closed automatically once the process has ended.\n\n\n\nFigure 14: Commence Scraping\n\n\nStep 11: Export Data\nExport data is selected upon clicking sitemap aac. 2 file options are offered: csv and xlsx. The former was chosen as CSV files are simple and portable which doesn’t complicate data processing. Thereafter the data will be downloaded.\n\n\n\nFigure 15: Export Data\n\n\nStep 11: View CSV File\nIn ensuring the web scraping successful and accurate, the csv. file is opened and examined.\n\n\n\nFigure 16: View CSV File\n\n\nThe above steps were repeated for each type of care centre. All of the Care Centre data were extracted on 7th February 2025."
  },
  {
    "objectID": "methodology.html#survival-analysis-from-2025---2030",
    "href": "methodology.html#survival-analysis-from-2025---2030",
    "title": "Data Preview",
    "section": "3.3 Survival Analysis from 2025 - 2030",
    "text": "3.3 Survival Analysis from 2025 - 2030\nThis code defines a function called clean_age_column that takes a dataframe (df) as input and processes its age column to ensure consistent numeric values. The function uses the %&gt;% (pipe) operator from the dplyr package to chain a series of data transformations. First, it trims any leading or trailing whitespace from the age column using str_trim. Next, it replaces the label \"90_and_over\" with \"90\" to standardize the representation of ages 90 and above. Then, it converts the age column to numeric values using as.numeric, while suppressing any warnings that might arise from non-numeric entries (e.g., empty strings or invalid values). Finally, the function filters out any rows where the age could not be converted to a numeric value (resulting in NA), ensuring only valid numeric ages remain in the dataframe. The cleaned dataframe is then returned as the output. This function is useful for preparing age data for analysis by ensuring uniformity and removing invalid entries.\n\nclean_age_column &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      age = str_trim(age),  # Trim whitespace\n      age = if_else(age == \"90_and_over\", \"90\", age),  # Replace label\n      age = suppressWarnings(as.numeric(age))  # Convert safely\n    ) %&gt;%\n    filter(!is.na(age))  # Remove rows that still couldn't be converted\n}\n\n\n3.3.1 Step 1: Load data\nFirstly, we will read the rds file. Thereafter, we will apply `clean_age_column()` function that was defined earlier to the loaded dataframe.\n\npop20 &lt;- read_rds(\"data/rds/popdata/refined/popdata20_c.rds\") %&gt;% \n  clean_age_column()\npop21 &lt;- read_rds(\"data/rds/popdata/refined/popdata21_c.rds\") %&gt;% \n  clean_age_column()\npop22 &lt;- read_rds(\"data/rds/popdata/refined/popdata22_c.rds\") %&gt;% \n  clean_age_column()\npop23 &lt;- read_rds(\"data/rds/popdata/refined/popdata23_c.rds\") %&gt;% \n  clean_age_column()\npop24 &lt;- read_rds(\"data/rds/popdata/refined/popdata24_c.rds\") %&gt;% \n  clean_age_column()\n\n\n\n3.3.2 Step 2: Compute survival rates for each year-to-year transition\nThis code defines a function called compute_survival_rate that calculates survival rates between two consecutive time periods (e.g., years) for a population dataset. The function takes two dataframes (df1 and df2) as inputs, representing population data from two different time points (e.g., 2020 and 2021).\nFirst, the function filters df1 to include only individuals aged 59 to 90, as survival analysis is often focused on older populations. It then increments the age by 1 (using mutate(age = age + 1)) to align the ages with the next time period (df2). Next, it performs an inner join between the modified df1 and df2 using matching columns (age, sex, pa [possibly a region code], and sz [possibly another demographic grouping]). The join suffixes (_prev and _next) distinguish between the population counts from the two time periods.\nAfter joining, the function computes the survival rate (rate) by dividing the population in the later period (pop_next) by the population in the earlier period (pop_prev). Finally, it selects and returns only the relevant columns (pa, sz, age, sex, rate, pop_prev, pop_next) for further analysis.\nIn summary, this function helps estimate how many people from an initial cohort (in df1) survived into the next period (in df2) by age, sex, and other groupings, providing key insights for demographic or actuarial studies.\n\ncompute_survival_rate &lt;- function(df1, df2) {\n  df1 %&gt;%\n    filter(age &gt;= 59 & age &lt; 91) %&gt;%\n    mutate(age = age + 1) %&gt;%\n    inner_join(df2, \n               by = c(\"age\", \"sex\", \"pa\", \"sz\"),\n               suffix = c(\"_prev\", \"_next\")) %&gt;%\n    mutate(rate = pop_next / pop_prev) %&gt;%\n    select(pa, sz, age, sex, rate, pop_prev, pop_next)\n}\n#to add in inputation \n\n\nrates_2020_2021 &lt;- compute_survival_rate(pop20, pop21)\n\n\ninvalid_rates_2020_2021 &lt;- rates_2020_2021 %&gt;%\n  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))\ninvalid_rates_rows\n\n\nrates_2021_2022 &lt;- compute_survival_rate(pop21, pop22)\n\n\ninvalid_rates_2021_2022 &lt;- rates_2021_2022 %&gt;%\n  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))\ninvalid_rates_rows\n\n\nrates_2022_2023 &lt;- compute_survival_rate(pop22, pop23)\n\n\ninvalid_rates_2022_2023 &lt;- rates_2022_2023 %&gt;%\n  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))\ninvalid_rates_rows\n\n\nrates_2023_2024 &lt;- compute_survival_rate(pop23, pop24)\n\n\ninvalid_rates_2023_2024 &lt;- rates_2023_2024 %&gt;%\n  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))\ninvalid_rates_rows\n\n\n\n3.3.3 Step 2: Refined\nThe function processes survival rate data through four sequential steps, now correctly using dplyr’s grouping mechanism. First, it groups the data by both age and sex, which is crucial because survival rates likely vary significantly across these demographics. Within these groups, it performs three key operations: (1) capping any rates above 1.0 at 1.0 (assuming these represent survival probabilities that shouldn’t exceed 100%), (2) replacing infinite values (which occur when dividing by zero) with the maximum finite rate found in the data, and (3) imputing missing values (NaN) with the median rate for that specific age-sex group. After these grouped operations, it ungroups the data and performs a final safety check, replacing any remaining missing values with 1.0 (a neutral value indicating no change).\n\nclean_rates &lt;- function(df) {\n  df %&gt;%\n    group_by(age, sex) %&gt;%\n    mutate(\n      # Step 1: Cap rates &gt;1.0 at 1.0 (if survival probability)\n      rate = ifelse(rate &gt; 1.0 & !is.na(rate), 1.0, rate),\n      # Step 2: Replace Inf with max finite rate\n      rate = ifelse(is.infinite(rate), max(rate[is.finite(rate)], na.rm = TRUE), rate),\n      # Step 3: Impute NaN with group median (now works because we're grouped)\n      rate = ifelse(is.na(rate), median(rate, na.rm = TRUE), rate)\n    ) %&gt;%\n    ungroup() %&gt;%\n    # Step 4: Fill any remaining NaN with 1.0 (after ungrouping)\n    mutate(rate = ifelse(is.na(rate), 1.0, rate))\n}\n\n\nrates_2020_2021 &lt;- compute_survival_rate(pop20, pop21) %&gt;% clean_rates()\n\n\nrates_2021_2022 &lt;- compute_survival_rate(pop21, pop22) %&gt;% clean_rates()\n\n\nrates_2022_2023 &lt;- compute_survival_rate(pop22, pop23) %&gt;% clean_rates()\n\n\nrates_2023_2024 &lt;- compute_survival_rate(pop23, pop24) %&gt;% clean_rates()\n\n\n\n3.3.4 Step 3: Average the survival rates\n\navg_survival_rates &lt;- bind_rows(\n  rates_2020_2021,\n  rates_2021_2022,\n  rates_2022_2023,\n  rates_2023_2024\n) %&gt;%\n  group_by(age, sex, pa, sz) %&gt;%\n  summarise(avg_rate = mean(rate, na.rm = TRUE), .groups = \"drop\")\n\n\n\n3.3.5 Step 4: Forecast each year 2025 to 2029\n\nforecast_year &lt;- function(base_pop, rates, year) {\n  next_pop &lt;- base_pop %&gt;%\n    filter(age &gt;= 59 & age &lt; 90) %&gt;%\n    mutate(age = age + 1) %&gt;%\n    left_join(rates, by = c(\"age\", \"sex\", \"pa\", \"sz\")) %&gt;%\n    mutate(pop = pop * avg_rate) %&gt;%\n    select(age, sex, pa, sz, pop)\n\n  # Handle 90+\n  age_89 &lt;- base_pop %&gt;%\n    filter(age == 89) %&gt;%\n    select(sex, pa, sz, pop) %&gt;%\n    mutate(age = 90)\n\n  age_90plus &lt;- base_pop %&gt;%\n    filter(age == 90) %&gt;%\n    select(sex, pa, sz, pop)\n\n  pop_90 &lt;- bind_rows(age_89, age_90plus) %&gt;%\n    group_by(sex, pa, sz) %&gt;%\n    summarise(age = 90, pop = sum(pop), .groups = \"drop\")\n\n  bind_rows(next_pop, pop_90) %&gt;%\n    mutate(year = year) %&gt;%\n    arrange(pa, sz, sex, age)\n}\n\n\n\n3.3.6 Step 5: Loop over the years\n\n# List to store forecasts\nforecast_list &lt;- list()\nbase_pop &lt;- pop24\n\nfor (y in 2025:2029) {\n  next_forecast &lt;- forecast_year(base_pop, avg_survival_rates, y)\n  forecast_list[[as.character(y)]] &lt;- next_forecast\n  base_pop &lt;- next_forecast %&gt;% select(pa, sz, age, sex,  pop)\n}\n\n\n\n3.3.7 Final Output of Population Forecast\n\nall_forecasts &lt;- bind_rows(forecast_list)"
  },
  {
    "objectID": "methodology.html#precursor",
    "href": "methodology.html#precursor",
    "title": "Data Preview",
    "section": "3.5 Precursor",
    "text": "3.5 Precursor\nThere two sf data layers, namely:\n\nhexagon: sf object of hexagonal polygons.\nmpsz: sf object of planning subzones. Beside other field, there is a field contains the target population called aged_pop.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease ensure that both layers are in the same CRS (coordinate reference system, 3414 for svy21 ) and use projected units (e.g., meters)."
  },
  {
    "objectID": "methodology.html#check-and-transform-crs-to-projected",
    "href": "methodology.html#check-and-transform-crs-to-projected",
    "title": "Data Preview",
    "section": "3.6 Check and Transform CRS to Projected",
    "text": "3.6 Check and Transform CRS to Projected\nUse the code chunk below to check if the data layers are in svy21 projected coornates system.\n\nglimpse(hex_centroids)\n\nIf it is not projected, use the code chunk to transform the sf data layers into svy21.\n\nhexagon &lt;- st_transform(hexagon, 3414) \nmpsz &lt;- st_transform(psz, 3414)"
  },
  {
    "objectID": "methodology.html#calculate-the-area-of-each-subzone",
    "href": "methodology.html#calculate-the-area-of-each-subzone",
    "title": "Data Preview",
    "section": "3.7 Calculate the Area of Each Subzone",
    "text": "3.7 Calculate the Area of Each Subzone\nNext, the code below will computer the area of each polygon in mpsz.\n\nmpsz &lt;- mpsz %&gt;%\n  mutate(mpsz_area = st_area(geometry))"
  },
  {
    "objectID": "methodology.html#intersect-hexagons-with-subzones",
    "href": "methodology.html#intersect-hexagons-with-subzones",
    "title": "Data Preview",
    "section": "3.8 Intersect Hexagons with Subzones",
    "text": "3.8 Intersect Hexagons with Subzones\nThe code chunk below derive the overlapping geometry between hexagons and subzones.\n\nhex_mpsz_intersection &lt;- st_intersection(hex_centroids, mpsz)"
  },
  {
    "objectID": "methodology.html#calculate-area-of-each-intersection",
    "href": "methodology.html#calculate-area-of-each-intersection",
    "title": "Data Preview",
    "section": "3.9 Calculate Area of Each Intersection",
    "text": "3.9 Calculate Area of Each Intersection\n\nhex_mpsz_intersection &lt;- hex_mpsz_intersection %&gt;% \n  mutate(intersection_area =\n           st_area(geometry))"
  },
  {
    "objectID": "methodology.html#estimate-hexagon-population",
    "href": "methodology.html#estimate-hexagon-population",
    "title": "Data Preview",
    "section": "3.10 Estimate Hexagon Population",
    "text": "3.10 Estimate Hexagon Population\nAssume uniform population density within each subzone. So, the hexagon’s share of the population is:\n\n\nhex_mpsz_intersection &lt;-\n  hex_psz_intersection %&gt;%\n  mutate(hex_pop = as.numeric(\n    intersection_area / mpsz_area * \n      population))\n\n#to push population up"
  },
  {
    "objectID": "methodology.html#aggregate-estimated-population-to-each-hexagon",
    "href": "methodology.html#aggregate-estimated-population-to-each-hexagon",
    "title": "Data Preview",
    "section": "3.11 Aggregate Estimated Population to Each Hexagon",
    "text": "3.11 Aggregate Estimated Population to Each Hexagon\nIf a hexagon overlaps multiple subzones, sum the estimated populations.\n\nhexagon_population &lt;- \n  hex_mpsz_intersection %&gt;%\n  group_by(hex_id = row_number()) %&gt;% \n  summarise(estimated_population =\n              sum(hex_pop, na.rm = TRUE))"
  },
  {
    "objectID": "methodology.html#keeping-original-hexagon-attributes",
    "href": "methodology.html#keeping-original-hexagon-attributes",
    "title": "Data Preview",
    "section": "3.12 Keeping original hexagon attributes:",
    "text": "3.12 Keeping original hexagon attributes:\n\nhexagon_final &lt;- hexagon %&gt;% \n  mutate(hex_id = row_number()) %&gt;%\n  left_join(hexagon_population, \n            by = \"hex_id\")\n\nhexagon_final now includes a new column estimated_population, which is the area-weighted population estimate for each hexagon."
  },
  {
    "objectID": "Area-weighted Population Estimate.html",
    "href": "Area-weighted Population Estimate.html",
    "title": "Area-weighted Population Estimate",
    "section": "",
    "text": "There two sf data layers, namely:\n\nhexagon: sf object of hexagonal polygons.\nmpsz: sf object of planning subzones. Beside other field, there is a field contains the target population called aged_pop.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease ensure that both layers are in the same CRS (coordinate reference system, 3414 for svy21 ) and use projected units (e.g., meters)."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#precursor",
    "href": "Area-weighted Population Estimate.html#precursor",
    "title": "Area-weighted Population Estimate",
    "section": "",
    "text": "There two sf data layers, namely:\n\nhexagon: sf object of hexagonal polygons.\nmpsz: sf object of planning subzones. Beside other field, there is a field contains the target population called aged_pop.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease ensure that both layers are in the same CRS (coordinate reference system, 3414 for svy21 ) and use projected units (e.g., meters)."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#loading-the-packages",
    "href": "Area-weighted Population Estimate.html#loading-the-packages",
    "title": "Area-weighted Population Estimate",
    "section": "Loading the Packages",
    "text": "Loading the Packages"
  },
  {
    "objectID": "Area-weighted Population Estimate.html#check-and-transform-crs-to-projected",
    "href": "Area-weighted Population Estimate.html#check-and-transform-crs-to-projected",
    "title": "Area-weighted Population Estimate",
    "section": "Check and Transform CRS to Projected",
    "text": "Check and Transform CRS to Projected\nUse the code chunk below to check if the data layers are in svy21 projected coornates system.\nIf it is not projected, use the code chunk to transform the sf data layers into svy21."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#calculate-the-area-of-each-subzone",
    "href": "Area-weighted Population Estimate.html#calculate-the-area-of-each-subzone",
    "title": "Area-weighted Population Estimate",
    "section": "Calculate the Area of Each Subzone",
    "text": "Calculate the Area of Each Subzone\nNext, the code below will computer the area of each polygon in mpsz."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#intersect-hexagons-with-subzones",
    "href": "Area-weighted Population Estimate.html#intersect-hexagons-with-subzones",
    "title": "Area-weighted Population Estimate",
    "section": "Intersect Hexagons with Subzones",
    "text": "Intersect Hexagons with Subzones\nThe code chunk below derive the overlapping geometry between hexagons and subzones."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#calculate-area-of-each-intersection",
    "href": "Area-weighted Population Estimate.html#calculate-area-of-each-intersection",
    "title": "Area-weighted Population Estimate",
    "section": "Calculate Area of Each Intersection",
    "text": "Calculate Area of Each Intersection"
  },
  {
    "objectID": "Area-weighted Population Estimate.html#estimate-hexagon-population",
    "href": "Area-weighted Population Estimate.html#estimate-hexagon-population",
    "title": "Area-weighted Population Estimate",
    "section": "Estimate Hexagon Population",
    "text": "Estimate Hexagon Population\nAssume uniform population density within each subzone. So, the hexagon’s share of the population is:"
  },
  {
    "objectID": "Area-weighted Population Estimate.html#aggregate-estimated-population-to-each-hexagon",
    "href": "Area-weighted Population Estimate.html#aggregate-estimated-population-to-each-hexagon",
    "title": "Area-weighted Population Estimate",
    "section": "Aggregate Estimated Population to Each Hexagon",
    "text": "Aggregate Estimated Population to Each Hexagon\nIf a hexagon overlaps multiple subzones, sum the estimated populations."
  },
  {
    "objectID": "Area-weighted Population Estimate.html#keeping-original-hexagon-attributes",
    "href": "Area-weighted Population Estimate.html#keeping-original-hexagon-attributes",
    "title": "Area-weighted Population Estimate",
    "section": "Keeping original hexagon attributes:",
    "text": "Keeping original hexagon attributes:\nhexagon_final now includes a new column estimated_population, which is the area-weighted population estimate for each hexagon."
  },
  {
    "objectID": "methodology.html#mpsz-singapores-master-plan-2019-subzone-boundary",
    "href": "methodology.html#mpsz-singapores-master-plan-2019-subzone-boundary",
    "title": "Data Preview",
    "section": "3.3 MPSZ: Singapore’s Master Plan 2019 Subzone Boundary",
    "text": "3.3 MPSZ: Singapore’s Master Plan 2019 Subzone Boundary\n\n3.3.1 Overview\nIn this section, two datsets will be imported namely Master Plan 2019 Subzone Boundary and Master Plan 2019 Land Use. The dataset will be cleaned and transformed thereafter hexagons will be created to facilitate the analysis in the next chapter.\n\n\n3.3.2 Importing Data\nUsing st_read of the `sf` package, the ESRI shapefile was imported and it contains 323 data entries and 15 fields. Each of the data entry consists of a multi-polygon shape, with geospatial coordinates with a geographic coordinate system (GCS) of WGS84. The features of GCS include using a 3D spherical model of earth with coordinates of longitude, latitude and altitude whereas PCS uses a 2D plane model with linear measurements (i.e. metres). However, the file doesn’t churn the full details of the Master Plan Boundary. Therefore, the dataset was adjusted using QGIS and downloaded for this project with the assistance of Prof Kam.\n\nmpsz = st_read(dsn = \"data/planningarea/\",\n               layer = \"mpsz2019\")\n\nUsing glimpse(), we are able to visualise the columns, column types and properties within the columns. Notably, there are numerous columns that are not needed for this analysis.\n\nglimpse(mpsz)\n\nIn the code chunk below, we use the select() from `dplyr` package to select the columns that are required for the anlaysis.\n\nmpsz &lt;- mpsz %&gt;%\n  select(SUBZONE_N, geometry)\n\nBy employing tmap, we are able to create a static map visualisation of the subzones in Singapore. tm_shape is the base layer for the visualation which we will be using mpsz while tm_polygons renders the the subzone boundaries as filled polygon using SUBZONE_N.\n\ntmap_mode(\"plot\")\ntm_shape(mpsz) +\n  tm_polygons(\"SUBZONE_N\")\n\n\n\n3.3.3 Transforming Coordinate Reference System\nAs the research’s focus is exploring the accessibility of care centres in Singapore, Projected Coordinate System (PCS) would be appropriate in this context as it measures the distance between the elderly’ residence and the care centre. Thus, in ensuring accurate measurement, the function st_transform with a Coordinate Reference System (CRS) of 3414 was used (Kam, 2022) in the code chunk below.\n\nmpsz &lt;- mpsz %&gt;%\n  st_transform(crs = 3414)\n\n\n\n3.3.4 Checking for Shared Boundaries\nThe below codechunk uses st_is_valid in assessing if there are shared boundaries in the mpsz data. This is important as shared boundaries can lead to inconsistent data impacting the research findings. 9 polygons with self-intersection (‘Ring Self-Intersection’) issues were returned.\n\nst_is_valid(mpsz, reason = TRUE)\n\nThis code chunk visualises the boundaries that are affected such as the smaller islands outside of Singapore Main Island: P. Ubin, P. Tekong, Sentosa Island\n\ninvalid_polygons &lt;- mpsz[!st_is_valid(mpsz),]\nplot(invalid_polygons)\n\nIn addressing the above point, we will use st_buffer of sf package to compute a 5-metre buffer around the data.\n\nmpsz &lt;- st_buffer(mpsz, dist = 5)\n\n\n\n3.3.5 Combining MPSZ & Population Forecast - correct\nThis codechunk performs a spatial join between MPSZ and `Population forecast data, with data cleaning to handle missing values. The first section creates cleaned versions of both datasets by adding a standardised join key - mpsz_clean takes the subzone geometry data and creates a join_key by converting the subzone names to uppercase and removing any whitespace using str_trim() and str_to_upper(). Similarly, pivoted_clean processes the population forecast data by creating the same type of standardised join key from the sz column, ensuring consistent formatting for matching between the two datasets. The main join operation uses left_join() to combine the subzone geometries with the population data based on the matching join keys, preserving all subzones even if they don’t have corresponding population data. After joining, the temporary join_key column is removed using select(-join_key) to clean up the final dataset. The final step uses mutate(across(where(is.numeric), ~replace_na(.x, 0))) to identify all numeric columns in the joined dataset and replace any NA values with 0, ensuring that subzones without population forecasts display as having zero population rather than missing data.\ncorrect\n\n# Create temporary cleaned versions for joining\nmpsz_clean &lt;- mpsz %&gt;%\n  mutate(join_key = str_trim(str_to_upper(SUBZONE_N)))\n\npivoted_clean &lt;- pivoted_data %&gt;%\n  mutate(join_key = str_trim(str_to_upper(sz)))\n\n# Join and clean up, replacing NA with 0\nmpsz_popforecast &lt;- mpsz_clean %&gt;%\n  left_join(pivoted_clean, by = \"join_key\") %&gt;%\n  select(-join_key) %&gt;%\n  mutate(across(where(is.numeric), ~replace_na(.x, 0))) #replace NA with 0\n\ntry\n\ncombined_original_case &lt;- mpsz %&gt;%\n  distinct(SUBZONE_N) %&gt;%\n  mutate(join_key = toupper(SUBZONE_N)) %&gt;%\n  full_join(\n    pivoted_data %&gt;%\n      distinct(sz) %&gt;%\n      mutate(join_key = toupper(sz)),\n    by = \"join_key\"\n  ) %&gt;%\n  select(SUBZONE_N, sz) %&gt;%\n  arrange(SUBZONE_N, sz)\n\n###Combining MPSZ & Population Forecast\n\nmpsz_forecast &lt;- mpsz %&gt;%\n  left_join(pivoted_clean, by = c(\"SUBZONE_N\" = \"sz\"))"
  },
  {
    "objectID": "data/planningarea/mpsz2019.html",
    "href": "data/planningarea/mpsz2019.html",
    "title": "Accessibility of Eldercare Services in Singapore",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],MEMBER[“World Geodetic System 1984 (G2296)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "data/landuse/mp2019_landuse.html",
    "href": "data/landuse/mp2019_landuse.html",
    "title": "Accessibility of Eldercare Services in Singapore",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],MEMBER[“World Geodetic System 1984 (G2296)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "methodology.html#combining-hex-population",
    "href": "methodology.html#combining-hex-population",
    "title": "Data Preview",
    "section": "3.4 Combining Hex & population",
    "text": "3.4 Combining Hex & population\n#i have this sf “hex_res” with columns geometry and hex_id and mpsz_popforecast with columns age, sex, pa, sz, pop_2025, pop_2026, pop_2027, pop_2028, pop_2029, geometry. i want the hex_res to be with base layer and mpsz_popforecast to be on top of it."
  },
  {
    "objectID": "methodology.html#population",
    "href": "methodology.html#population",
    "title": "Data Preview",
    "section": "5.1 Population",
    "text": "5.1 Population\n\n5.1.1 EDA of Population Data\n\n### Step 1: Define a function to filter and aggregate age distribution data\nget_age_distribution &lt;- function(data, min_age = 60, max_age = 90) {\n  data %&gt;%\n    filter(age &gt;= min_age & age &lt;= max_age) %&gt;%\n    group_by(time, age, sex) %&gt;%\n    summarise(total_pop = sum(pop), .groups = \"drop\")\n}\n\n\n### Step 2: Use the function to process data\nage_dist &lt;- get_age_distribution(popdata20_c)  # Default: ages 60-90\n\n### Step 3: Plot age distributions by year and sex\nplot_age_distribution &lt;- function(age_data) {\n  ggplot(age_data, aes(x = age, y = total_pop, fill = sex)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    facet_wrap(~time, scales = \"free_y\") +\n    labs(\n      title = \"Age Distribution (60-90 Years) by Sex and Year\",\n      x = \"Age\",\n      y = \"Total Population\",\n      fill = \"Sex\"\n    ) +\n    scale_x_continuous(breaks = seq(60, 90, by = 5)) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n}\n\n# Generate the plot\nplot_age_distribution(age_dist)\n\nIn generating the population pyramid, the below oko\n\n#load data\nget_age_distribution &lt;- function(data, min_age = 60, max_age = 90) {\n  data %&gt;%\n    filter(age &gt;= min_age & age &lt;= max_age) %&gt;%\n    mutate(\n      # Standardize sex labels to match your data\n      sex = factor(sex, levels = c(\"Males\", \"Females\"))\n    ) %&gt;%\n    group_by(time, age, sex) %&gt;%\n    summarise(total_pop = sum(pop), .groups = \"drop\")\n}\n\n\n#2020\nage_dist &lt;- get_age_distribution(popdata20_c)\n# 3. Create population pyramid plot function \nplot_population_pyramid &lt;- function(age_data, plot_year = max(age_data$time)) {\n  # Filter and transform data\n  plot_data &lt;- age_data %&gt;% \n    filter(time == plot_year) %&gt;%\n    mutate(total_pop = ifelse(sex == \"Males\", -total_pop, total_pop))\n  \n  # Calculate axis limits\n  max_pop &lt;- max(abs(plot_data$total_pop))\n  \n  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +\n    geom_bar(stat = \"identity\", width = 0.8) +\n    geom_hline(yintercept = 0, color = \"black\") +\n    coord_flip() +\n    scale_y_continuous(\n      labels = function(x) comma(abs(x)),\n      breaks = pretty_breaks(n = 6),\n      limits = c(-max_pop * 1.1, max_pop * 1.1)\n    ) +\n    scale_fill_manual(\n      values = c(\"Males\" = \"#3498db\", \"Females\" = \"#e74c3c\"),\n      labels = c(\"Males\" = \"Male\", \"Females\" = \"Female\") # Optional: display cleaner labels\n    ) +\n    labs(\n      title = paste(\"Population Pyramid of Age 60 & Above -\", plot_year),\n      x = \"Age\",\n      y = \"Population Count\",\n      fill = \"Gender\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      legend.position = \"bottom\"\n    )\n}\n\n# 4. Generate and save the plot ------------------------------------------------\nfinal_plot &lt;- plot_population_pyramid(age_dist)\nprint(final_plot)\n\n     ### Survival Analysis from 2025 - 2030\n\ntmpa_mode(\"view\")\n\n\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))"
  },
  {
    "objectID": "methodology.html#singapore-master-plan-2019-subzone-boundary",
    "href": "methodology.html#singapore-master-plan-2019-subzone-boundary",
    "title": "Data Preview",
    "section": "2.1 Singapore Master Plan 2019 Subzone Boundary",
    "text": "2.1 Singapore Master Plan 2019 Subzone Boundary\nThe Singapore Master Plan 2019 Planning Subzone Boundary (No Sea) and Amendment to Master Plan 2019 Land Use Layer are ESRI shapefiles that are downloaded from Data.gov.sg."
  },
  {
    "objectID": "methodology.html#mpsz-land-use",
    "href": "methodology.html#mpsz-land-use",
    "title": "Data Preview",
    "section": "3.4 MPSZ Land Use",
    "text": "3.4 MPSZ Land Use\n\n3.4.1 Importing Data\nUsing st_read, the file is loaded with an addition of transforming to the CRS to 3414.\n\nmpsz_land = st_read(dsn = \"data/landuse/\",\n               layer = \"mp2019_landuse\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\n3.4.2 Checking for Shared Boundaries\nSimilar to the step in MPSZ, this function is used to check if there are any shared boundaries.\n\nst_is_valid(mpsz_land, reason = TRUE)\n\nUsing unique(), it returns a list of properties in the column LU_DESC. As the focus of the research is accessibility to care centre, all residential related areas will be factored in this research.\n\nunique(mpsz_land$LU_DESC)\n\n\nmpsz_land &lt;- st_buffer(mpsz_land, dist = 1)\n\n\n\n3.4.3 Filtering to Residential Use\nIn the code chunk below, we use the select()function to pick the columns that are required for the anlaysis. Additionally, filter() the column LU_DESC for those words containing ‘residential’. Regular expression was used alongside ignoring captital letters.\n\nmpsz_res &lt;- mpsz_land %&gt;%\n  select(LU_DESC, geometry) %&gt;%\n  filter(str_detect(LU_DESC, regex(\"residential\", ignore_case = TRUE)))\n\n\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_res) +\n  tm_polygons()\n\n\n\n3.4.4 Sampling Grid\nIn measuring spatial accessibility, analytical grids of the sampling fields is used to standardise the means of measurement (Kam, 2022). These analytical grids can be comprised of equilateral triangles, squares or hexagon due to its ability to tessellate (ESRI, 2025). From the code output below, we noticed that the sampling fields in the mpsz_res are not equal for measurement. Therefore, it is imperative to select an appropriate analytical grid in measuring spatial accessibility. Hexagons grids is chosen over triangles and squares due to several factors. Firstly, the shape of a hexagon has a low-perimeter-to-area ratio, hence the edge effect of the grid shape reduces sampling bias. Secondly, when comparing equal area, any point inside a hexagon is closer to the centriod than any given point in an equal-area square or triangle due to the more acute angles of square and triangle versus the hexagon (Burdziej, 2018; ESRI, 2025). This is particular important in this research as we are using the centroid as a proxy in comparing accessibility to the various care centres. The centre of each hexagon is an Origin, additionally, acting as a starting point in ascertaining the shortest distance to the care centres.\n\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntmap_options(max.categories = n_distinct(mpsz_res$LU_DESC))  # Show all categories\ntm_shape(mpsz_res) +\n  tm_polygons(\"LU_DESC\")\n\n\n3.4.4.1 Step 1: Creating polygons\nIn the HealthierSG White Paper 2022, the Ministry of Health has indicated its intention to “expand the network [”care centres”] to 220 by 2025”. Furthermore, through the Ministry’s estimation, 8 in 10 seniors will have a care centre in the vicinity of their homes (Ministry of Health, 2022). Hence, we will assume that the maximum distance to the care centres are 100 metres. Hence, in the code chunk below, st_make_grid from sf package constructed hexagonal grids encompassing the Singapore Master Plan 2019 Planning Subzone Boundary using cellsize that defines the radius of 100 metres and square to be false to generate a hexagonal grid.\n\nhexagon &lt;- st_make_grid(mpsz_res,\n                         cellsize = 100,\n                         what = \"polygon\",\n                         square = FALSE) %&gt;%\n  st_sf()\n\n\ntmap_mode(\"plot\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(hex_res) +\n  tm_polygons()\n\n\nhex_res &lt;- hexagon[st_intersects(hexagon, mpsz_res, sparse = FALSE) %&gt;% apply(1, any), ]\n#sparse = exclude zone without any intersection\n\n\nhex_res$hex_id &lt;- sprintf(\"H%04d\", seq_len(nrow(hex_res))) %&gt;% as.factor()\nhead(hex_res)\n\n\n\n3.4.4.2 Step 2: Calculate the Area of Each Subzone\nNext, the code below will computer the area of each polygon in mpsz_.\n\nmpsz &lt;- mpsz %&gt;%\n  mutate(mpsz_area = st_area(geometry))\n\n\n\n3.4.4.3 Step 3: Intersect Hexagons with Subzones\nThe code chunk below derive the overlapping geometry between hexagons and subzones.\n\nhex_mpsz_intersection &lt;- st_intersection(hex_res, mpsz)"
  },
  {
    "objectID": "methodology.html#od-matrix",
    "href": "methodology.html#od-matrix",
    "title": "Data Preview",
    "section": "3.5 ?OD MATRIx",
    "text": "3.5 ?OD MATRIx\n\n3.5.1 Visualing hex_grids & res\n\n# Set tmap mode and options\ntmap_mode(\"plot\")  # Use \"view\" for interactive map\ntmap_options(check.and.fix = TRUE)\ntmap_options(max.categories = n_distinct(mpsz_res$LU_DESC))\n\n\n# Create the base map with land use\nbase_map &lt;- tm_shape(mpsz_res) +\n  tm_shape(hex_grid) +\n  tm_polygons(\"LU_DESC\",\n              palette = \"Set3\",\n              title = \"Land Use Type\",\n              alpha = 0.7,\n              border.col = \"gray30\",\n              lwd = 0.3) +\n  tm_layout(legend.outside = TRUE,\n            frame = FALSE)\n\n\nbase_map\n\n\n# Add hexagon grid overlay\nfinal_map &lt;- base_map +\n  tm_shape(hex_grid) +\n  tm_borders(col = \"black\",\n             lwd = 0.5,\n             alpha = 0.5) +\n  tm_text(\"hex_id\",\n          size = 0.5,\n          col = \"black\",\n          alpha = 0.7)\n\n\n# Display the map\nfinal_map"
  }
]