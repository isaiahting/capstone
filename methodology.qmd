---
title: "Data Preview"
date: "21st Feb, 2025"
date-modified: "last-modified"
format:
  html:
    code-fold: false
    code-summary: "Code Chunk"
    number-sections: true
execute: 
  eval: false #r will run through all codes
  echo: true #r will display all code chunk
  warning: false #for mark down
  freeze: true #r will not render all existing  html files
  message: false #avoid printing warning message
editor: source
---

# Overview - describe challenges

In this section, we will acquire the data sets from various government open-sources data repository and websites. Thereafter, we will install the necessary R packages and import the data sets. In each data set, we will delve into the necessary checks, issues faced and steps into resolving the issue. Thereafter, exploratory data analysis is done on a micro and macro level. In each proces

# Data Acquisition

In this research, 3 main spectrum of data will be required for this research, namely the Population Data, Master Plan 2019 Subzone boundary and Care Centres. These datasets are gathered from multiple open-source websites which includes Singapore Department of Statistics, Data.gov.sg, and Agency of Integrated Care.

![Figure x: Data Overview](/metho_images/data_overview.png){fig-align="center"}

##Singapore Master Plan 2019 Subzone Boundary

The Singapore Master Plan 2019 Planning Subzone Boundary is a ESRI shapefile that is obtained from Data.gov.sg.

## Population Data

The Singapore Resident by Planning Area/Subzone, Single Year of Age and Sex, June 2024 is selected for this

As the version January 2025 will be not be ready in due time for this research project, thus, the version of June 2024 is used.

## WebScraping of Care Centres

Due to the lack of a centralised data of all care centres, web scraping is warranted in obtaining the information of the care centres. The geographical locations of the Care Centres alongside the centre names such as Active Ageing Centre, Day Care, Community Rehabilitation Centre, Centre-based Nursing were extracted using a web scraping tool, Web Scraper, available in Chrome web store as Seen in Figure x. As there is no centralised file that consist of the centre names and their locations, the location of each centre has to be manually extracted from the [Care Services](https://www.aic.sg/care-services/) webpage of the Agency of Integrated Care as seen in Figure x.

Step 0: Download Web Scraper from Chrome web store

Web Scraper is used as it is free, works reasonably well and available in both Chrome and Firefox web store. In the below steps, Chrome will be the default web browser used.

![*Figure x: Web Scraper*](/metho_images/step0.png)

Step 1: Navigate to Developer Tools in Chrome Web Browser

After downloading the extension from Chrome Web Store, press onto the menu bar at the right of the browser and locate Developer Tools while onto the website you would like to scrape information from.

![*Figure x: Web Scraper*](/metho_images/step1.png)

Step 2: Interface for Webscraper

After clicking onto Developer Tools, click onto the Web Scraper in the menu bar (in black). Following which the below interface will appear.

[![](/metho_images/step2.png){width="499"}](Figure%20x:%20Locate%20Developer%20Tools)

Step 3: Create New Sitemap

Click onto "create new sitemap", thereafter "Create Sitemap". Sitemap Name will be the overarching term used for these information; in this instance, it will be AAC. The Start URL will be the HTML link that you would like the information to be scraped from.

![Figure x:](/metho_images/step3.png){fig-align="center"}

Step 4: Add New Selector

After creating a new sitemap, the following interface will appear. Click onto the "Add new selector" to select the information to scrape.

![Figure x:](/metho_images/step4.png){fig-align="center"}

Step 5: Selecting Whole Box

Firstly, the id will be the column name. For Type, select Element Attribute from the drop down selection. Thereafter, press on Select under Selector and select two boxes of each centre as seen in the figure below (the remaining boxes will be highlighted through its intelligent function) and press onto Done Selecting in the green box.

![Figure x:](/metho_images/step5.png){fig-align="center"}

Step 6: Sitemap Interface

After adding a new selector, the sitemap page will appear the selector that you've inputted.

![Figure x: Step 6 - Create New Sitemap](/metho_images/step6.png){fig-align="center"}

Step 7: Selecting Name of Care Centre

Firstly, the id will be name (with reference to the name of care centre), serving as the column name. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 names of the care centres (The remaining care centres will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple names and root parent selector will be root and press onto Save Selector.

![Figure x: Step 7 - Selecting Name of Care Centre](/metho_images/step7.png){fig-align="center"}

Step 8: Create New Sitemap

A popup window will be prompted and Group selectors was selected.

![Figure x: Step 8 - Create New Sitemap](/metho_images/step8.png){fig-align="center"}

Step 9: Selecting Address of Care Centre

Similar to Step 7, the id will be address. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 addresses of the care centres (Remaining addresses will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple addresses and parent selector will be wrapper_for_main_name (as we grouped selectors in step 8) and press onto Save Selector.

![Figure x: Step 9 - Selecting Address of Care Centre](/metho_images/step9.png){fig-align="center"}

Step 10: Data Preview

Prior to data scraping, the data is previewed in ensuring each name of the care centre is correctly tagged to the address using the main website to verify.

![Figure x: Step 10 - Data Preview](/metho_images/step10.png){fig-align="center"}

Step 11: Commence Scraping

Head over to sitemap aac and click onto Scrape. A new browser will appear indicating that it is in process of scraping. It will be closed automatically once the process has ended.

![Figure x: Step 11 - Commence Scraping](/metho_images/step11.png){fig-align="center"}

Step 11: Export Data

Export data is selected upon clicking sitemap aac. 2 file options are offered: csv and xlsx. The former was chosen as CSV files are simple and portable which doesn't complicate data processing. Thereafter the data will be downloaded.

![Figure x:](/metho_images/step12.png){fig-align="center"}

Step 11: View CSV File

In ensuring the web scraping successful and accurate, the csv. file is opened and examined.

![Figure x: Step 11 - View CSV File](/metho_images/step13.png){fig-align="center"}

The above steps were repeated for each type of care centre. All of the Care Centre data were extracted on 7th February 2025.

# Data Cleaning & Manipulation

## Installing Packages

::: panel-tabset
The following packages are required for this section:

| Package | Description |
|-------------------------------------|-----------------------------------|
| [**tidyverse**](https://www.tidyverse.org/) | For non-spatial data wrangling that includes dplyr, tibble, ggplot2, readr, tidyr, stringr, forcats, lubridate and purr |
| [**sf**](https://r-spatial.github.io/sf/) | For importing, managing, and handling geospatial data |
| [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/index.html) | For interacting with a web API |
| [**rvest**](https://cran.r-project.org/web/packages/rvest/index.html) | A wrapper around xml2 and httr packages |
| [**sfdep**](https://sfdep.josiahparry.com/) | Used to compute spatial weights, global and local spatial autocorrelation statistics |
| [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) | for creating elegent and cartographic quality thematic maps |
| [**knitr**](https://cran.r-project.org/web/packages/knitr/index.html) | For dynamic report generation |
| [**patchwork**](https://patchwork.data-imaginist.com/) | For plot manipulation |
| [**leaflet**](https://rstudio.github.io/leaflet/) | For interactive maps |
| [**scales**](https://scales.r-lib.org/) | For scaling graphs |
| [**lubridate**](https://lubridate.tidyverse.org/) | For date-time manipulation |
| [**tmap**](https://r-graph-gallery.com/package/tmap.html) | For creating interactive maps |
| [**DT**](https://rstudio.github.io/DT/) | provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, and many other features in the tables. |

The code chunk below, using `p_load` function of the [**pacman**](https://cran.r-project.org/web/packages/pacman/pacman.pdf) package, ensures that packages required are installed and loaded in R.

```{r}
pacman::p_load(tidyverse, sf,
               jsonlite, rvest, knitr, patchwork,
               leaflet, scales, lubridate, tmap, DT, geojsonsf, scales,
               VIM, mice, broom)
```
:::

## Population Data

### Overview

In this section, a total of 4 population datasets from year 2020 to year 2024 will be used in the analysis. The datasets will be cleaned. Survival analysis will be done in estimating the population for age 60 and above for the period 2025 to 2029.

### Importing Data

In importing data, `read_csv()` and `rename_with()` of tidyverse package are used to perform column name standardisation by converting all variable names in the respective datasets to lowercase.

:::: panel-tabset
### Population Data 2024

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2024.ashx)

```{r}
popdata24 <- read_csv("data/popdata/respopagesex2024.csv") %>%
  rename_with(tolower)
```

### Population Data 2023

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2023](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2023.ashx)

```{r}
popdata23 <- read_csv("data/popdata/respopagesex2023.csv") %>%
  rename_with(tolower)
```

### Population Data 2022

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2022.ashx) 

```{r}
popdata22 <- read_csv("data/popdata/respopagesex2022.csv") %>%
  rename_with(tolower)
```

::: callout-warning
PARSING ERROR\*

```         
Warning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:
  dat <- vroom(...)
  problems(dat)Rows: 60424 Columns: 6── Column specification
```
:::

### Population Data 2021

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2021](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2021.ashx)

```{r}
popdata21 <- read_csv("data/popdata/respopagesex2021.csv") %>%
  rename_with(tolower)
```

### Population Data 2020

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2011-2020](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2011to2020.ashx)

```{r}
popdata20 <- read_csv("data/popdata/respopagesex2011to2020.csv") %>%
  rename_with(tolower) %>%
  filter(time == 2020)
```
::::

```{r}
#| echo: false
#| eval: false
write_rds(popdata24, "data/rds/popdata/original/popdata24.rds")
write_rds(popdata23, "data/rds/popdata/original/popdata23.rds")
write_rds(popdata21, "data/rds/popdata/original/popdata21.rds")
write_rds(popdata20, "data/rds/popdata/original/popdata20.rds")
```

```{r}
#| echo: false
#| eval: false
popdata24 = read_rds("data/rds/popdata/original/popdata24.rds")
popdata23 = read_rds("data/rds/popdata/original/popdata23.rds")
popdata21 = read_rds("data/rds/popdata/original/popdata21.rds")
popdata20 = read_rds("data/rds/popdata/original/popdata20.rds")
```

```{r}
glimpse(popdata23)
```

### Checking for Missing Values

To check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.

The is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.

Results: We noticed that there are 30 missing values popdata22 specifically under the column pop.

```{r}
popdata20_missing <- popdata20 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata20_missing)

popdata21_missing <- popdata21 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata21_missing)

popdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata22_missing)

popdata23_missing <- popdata23 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata23_missing)

popdata24_missing <- popdata24 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata24_missing)


```

### Issue with `POPDATA22`

Using the below code, we are able to see clearly the rows that are affected and in the pop column, it appears as NA. The csv file (respopagesex2022.csv) was opened using excel and each row returned in the below output was then cross checked in excel. Whole numbers with comma appeared in excel. This may be because read_csv() function expects a numeric value (double) in one of the columns, but instead, it found a string (the values in the column are likely formatted with commas, such as "1,020"). This is why the parser is raising an issue earlier on.

```{r}
na_rows <- popdata22[is.na(popdata22$pop), ]
print(na_rows)
```

Referencing from [Stackoverflow](https://stackoverflow.com/questions/1523126/how-to-read-data-when-some-numbers-contain-commas-as-thousand-separator), the first line of the code is necessary as it defines a new class called `"num.with.commas"`. This class is intended to handle numeric values that are stored as strings with commas (e.g., `"1,000"`). Thereafter, the second line of the code defines a method to convert a `character` type to the custom `"num.with.commas"` class.

-   The `gsub(",", "", from)` function removes commas from the string (e.g., `"1,000"` becomes `"1000"`)

-   The `as.numeric()` function then converts the cleaned string into a numeric value (e.g., `"1000"` becomes `1000`)

This ensures that numbers with commas are properly converted to numeric values during data import.

```{r}
setClass("num.with.commas")
setAs("character", "num.with.commas", 
        function(from) as.numeric(gsub(",", "", from) ) )
```

The file is then re-imported again and specifically, the column 'pop' is parsed as a character field in facilitating the next step in removing commas within the population itself.

```{r}
popdata22 <- read_csv("data/popdata/respopagesex2022.csv", 
                      col_types = cols(
                        PA = col_character(),
                        SZ = col_character(),
                        Age = col_character(),
                        Sex = col_character(),
                        Pop = col_character(),
                        Time = col_number()  # Adjust if necessary
                      )) %>%
  rename_with(tolower)
```

As previously stated, commas are present in the 'pop' column, hence, `mutate()`

```{r}
popdata22 <- popdata22 %>%
  mutate(pop = as.numeric(str_replace_all(pop, ",", "")))
```

```{r}
#| echo: false
#| eval: false
write_rds(popdata22, "data/rds/popdata/original/popdata22.rds")
```

In the below codechunk, it was verified that there is no missing values and the above steps taken were successful.

```{r}
names(popdata22) <- tolower(names(popdata22))
popdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata22_missing)
```

In the below `code chunk a & b`, we noticed that it returned two different outputs: `90_and_over` and `90_and_Over`. This may explain why the error `NAs introduced by coercion` was returned.

```{r}
#code chunk a
popdata22 %>% 
  summarise(max_age = max(age, na.rm = TRUE))
```

```{r}
#code chunk b
popdata24 %>% 
  summarise(max_age = max(age, na.rm = TRUE))
```

Hence, in addressing the above issue, the below code chunk was developed. First, The code defines a function called `convert_age` that takes a dataframe (`df`) as input. Inside the function, it modifies the `age` column using `mutate()`. It checks each value in the `age` column to see if it contains either "\_and_over" or "\_and_Over" (case-insensitive match). When a match is found, it extracts just the numeric part (e.g., "90" from "90_and_over") using `str_extract()`. If no match is found, it keeps the original value. The second `mutate()` converts the cleaned `age` column to numeric values, ensuring all ages are stored as numbers. The function returns the modified dataframe with standardised age values.

```{r}
convert_age <- function(df) {
  df %>%
    mutate(age = if_else(
      str_detect(age, regex("_and_Over|_and_over", ignore_case = TRUE)),
      str_extract(age, "\\d+"),  # Extract just the numeric part
      age
    )) %>%
    mutate(age = as.numeric(age))
}
```

```{r}
popdata20_c <- convert_age(popdata20)
```

```{r}
popdata21_c <- convert_age(popdata21)
```

```{r}
popdata22_c <- convert_age(popdata22)
```

```{r}
popdata23_c <- convert_age(popdata23)
```

```{r}
popdata24_c <- convert_age(popdata24)
```

Another layer of confirmation of missing values was executed in ensuring no missing values were returned during the abovementioned process and it returns 0 for each dataset.

```{r}
popdata20_missing <- popdata20_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata20_missing)
popdata21_missing <- popdata21_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata21_missing)
popdata22_missing <- popdata22_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata22_missing)
popdata23_missing <- popdata23_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata23_missing)
popdata24_missing <- popdata24_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata24_missing)
```

### Returning rows where pop is o

```{r}
# Return all rows where 'pop' is exactly 0
popdata24_c %>% 
  filter(age>=60, pop==0)
```

```{r}
popdata23_c %>% 
  filter(age>=60, pop==0)
```

```{r}
popdata22_c %>% 
  filter(age>=60, pop==0)
```

```{r}
popdata21_c %>% 
  filter(age>=60, pop==0)
```

```{r}
popdata20_c %>% 
  filter(age>=60, pop==0)
```

### Duplicate Check

The code provided checks for duplicate rows in each dataset by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \> 1) keeps only the rows that appear more than once (i.e., duplicates).

For each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.

Thus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.

```{r}
# Check for duplicates in 'aac'
popdata20_duplicate <- popdata20_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata20_duplicate)


# Check for duplicates in 'counselling'
popdata21_duplicate <- popdata21_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata21_duplicate)

# Check for duplicates in 'daycare'
popdata22_duplicate <- popdata22_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata22_duplicate)

# Check for duplicates in 'dementia'
popdata23_duplicate <- popdata23_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata23_duplicate)

# Check for duplicates in 'hospice'
popdata24_duplicate <- popdata24_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata24_duplicate)

```

```{r}
#| echo: false
#| eval: false
write_rds(popdata24_c, "data/rds/popdata/refined/popdata24_c.rds")
write_rds(popdata23_c, "data/rds/popdata/refined/popdata23_c.rds")
write_rds(popdata22_c, "data/rds/popdata/refined/popdata22_c.rds")
write_rds(popdata21_c, "data/rds/popdata/refined/popdata21_c.rds")
write_rds(popdata20_c, "data/rds/popdata/refined/popdata20_c.rds")
```

```{r}
#| echo: false
#| eval: false
popdata24_c = read_rds("data/rds/popdata/refined/popdata24_c.rds")
popdata23_c = read_rds("data/rds/popdata/refined/popdata23_c.rds")
popdata22_c = read_rds("data/rds/popdata/refined/popdata22_c.rds")
popdata21_c = read_rds("data/rds/popdata/refined/popdata21_c.rds")
popdata20_c = read_rds("data/rds/popdata/refined/popdata20_c.rds")
```

### EDA of Population Data

```{r}

### Step 1: Define a function to filter and aggregate age distribution data
get_age_distribution <- function(data, min_age = 60, max_age = 90) {
  data %>%
    filter(age >= min_age & age <= max_age) %>%
    group_by(time, age, sex) %>%
    summarise(total_pop = sum(pop), .groups = "drop")
}
```

```{r}
### Step 2: Use the function to process data
age_dist <- get_age_distribution(popdata20_c)  # Default: ages 60-90

### Step 3: Plot age distributions by year and sex
plot_age_distribution <- function(age_data) {
  ggplot(age_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~time, scales = "free_y") +
    labs(
      title = "Age Distribution (60-90 Years) by Sex and Year",
      x = "Age",
      y = "Total Population",
      fill = "Sex"
    ) +
    scale_x_continuous(breaks = seq(60, 90, by = 5)) +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# Generate the plot
plot_age_distribution(age_dist)
```

```{r}
#| echo: false #not show code
#| eval: false
```

In generating the population pyramid, the below oko

```{r}
#load data
get_age_distribution <- function(data, min_age = 60, max_age = 90) {
  data %>%
    filter(age >= min_age & age <= max_age) %>%
    mutate(
      # Standardize sex labels to match your data
      sex = factor(sex, levels = c("Males", "Females"))
    ) %>%
    group_by(time, age, sex) %>%
    summarise(total_pop = sum(pop), .groups = "drop")
}
```

```{r}
#2020
age_dist <- get_age_distribution(popdata20_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata21_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata22_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata23_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata24_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

### Testing for Missing Values

```{r}
# Compare means of other variables between zero/non-zero groups
pop20_i %>% 
  group_by(zero_pop = (pop == 0)) %>% 
  summarise(across(c(age, time), mean))
```

### Percentage of Missing Values

pop20

```{r}
pop20_geri <- pop20 %>%
  filter(age>=60)
```

```{r}
zero_pop20 <- pop20 %>% 
  filter(pop == 0, age>=60)
```

```{r}
(nrow(zero_pop20) / nrow(pop20_geri)) * 100
```

pop21

```{r}
pop21_geri <- pop21 %>%
  filter(age>=60)
```

```{r}
zero_pop21 <- pop21 %>% 
  filter(pop == 0, age>=60)
```

```{r}
(nrow(zero_pop21) / nrow(pop21_geri)) * 100
```

pop22

```{r}
pop22_geri <- pop22 %>%
  filter(age>=60)
```

```{r}
zero_pop22 <- pop22 %>% 
  filter(pop == 0, age>=60)
```

```{r}
(nrow(zero_pop22) / nrow(pop22_geri)) * 100
```

pop23

```{r}
pop23_geri <- pop23 %>%
  filter(age>=60)
```

```{r}
zero_pop23 <- pop23 %>% 
  filter(pop == 0, age>=60)
```

```{r}
(nrow(zero_pop23) / nrow(pop23_geri)) * 100
```

pop24

```{r}
pop24_geri <- pop24 %>%
  filter(age>=60)
```

```{r}
zero_pop24 <- pop24 %>% 
  filter(pop == 0, age>=60)
```

```{r}
(nrow(zero_pop24) / nrow(pop24_geri)) * 100
```

### Imputation

```{r}
# Create zero indicator
pop20_i <- pop20 %>%
  mutate(
    zero_pop = (pop == 0),
    log_pop = ifelse(pop > 0, log(pop), NA)  # For visualization
)
```

```{r}
# Check zero patterns
zero_summary <- pop20_i %>%
  filter(age>=60) %>%
  group_by(pa, sz, age, sex) %>%
  summarise(
    n_zeros = sum(zero_pop),
    prop_zeros = mean(zero_pop),
    .groups = 'drop'
  )
```

```{r}
# Visualize zeros
ggplot(zero_summary, aes(x = age, y = prop_zeros, fill = sex)) +
  geom_boxplot() +
  facet_wrap(~pa) +
  labs(title = "Proportion of Zero Population by Demographics")
```

```{r}
ggplot(pop20_i, aes(x=time, y=pop)) + 
  geom_point(alpha=0.5) +
  facet_grid(sex ~ age) +
  labs(title="Population Distribution by Demographics")
```

### Survival Analysis





![Figure x: Population Pyramid 2020](/metho_images/2020.png) ![Figure x: Population Pyramid 2021](/metho_images/2021.png) ![Figure x: Population Pyramid 2022](/metho_images/2022.png) ![Figure x: Population Pyramid 2023](/metho_images/2023.png) ![Figure x: Population Pyramid 2024](/metho_images/2024.png) \### Survival Analysis from 2025 - 2030

#### Introduction

In this section,

The aim of this is to calculate the sruvival rate, in this instance, death is the event of interest.

As the population data is used, we will assume that the data is non-parametric in nature. **Kaplan-Meier**

It is assumed that residents will stay in the same address with in relation to their subzones throughout the study period. Hence, if there is a decrease of population in the subzone, the reasonable explanation of this outcome is death.

This code defines a function called `clean_age_column` that takes a dataframe (`df`) as input and processes its `age` column to ensure consistent numeric values. The function uses the `%>%` (pipe) operator from the `dplyr` package to chain a series of data transformations. First, it trims any leading or trailing whitespace from the `age` column using `str_trim`. Next, it replaces the label `"90_and_over"` with `"90"` to standardize the representation of ages 90 and above. Then, it converts the `age` column to numeric values using `as.numeric`, while suppressing any warnings that might arise from non-numeric entries (e.g., empty strings or invalid values). Finally, the function filters out any rows where the `age` could not be converted to a numeric value (resulting in `NA`), ensuring only valid numeric ages remain in the dataframe. The cleaned dataframe is then returned as the output. This function is useful for preparing age data for analysis by ensuring uniformity and removing invalid entries.

```{r}
clean_age_column <- function(df) {
  df %>%
    mutate(
      age = str_trim(age),  # Trim whitespace
      age = if_else(age == "90_and_over", "90", age),  # Replace label
      age = suppressWarnings(as.numeric(age))  # Convert safely
    ) %>%
    filter(!is.na(age))  # Remove rows that still couldn't be converted
}
```

#### Step 1: Load data

Firstly, we will read the rds file. Thereafter, we will apply \`clean_age_column()\` function that was defined earlier to the loaded dataframe.

```{r}
pop20 <- read_rds("data/rds/popdata/refined/popdata20_c.rds") %>% 
  clean_age_column()
pop21 <- read_rds("data/rds/popdata/refined/popdata21_c.rds") %>% 
  clean_age_column()
pop22 <- read_rds("data/rds/popdata/refined/popdata22_c.rds") %>% 
  clean_age_column()
pop23 <- read_rds("data/rds/popdata/refined/popdata23_c.rds") %>% 
  clean_age_column()
pop24 <- read_rds("data/rds/popdata/refined/popdata24_c.rds") %>% 
  clean_age_column()
```

#### Step 2: Compute survival rates for each year-to-year transition

This code defines a function called `compute_survival_rate` that calculates survival rates between two consecutive time periods (e.g., years) for a population dataset. The function takes two dataframes (`df1` and `df2`) as inputs, representing population data from two different time points (e.g., 2020 and 2021).

First, the function filters `df1` to include only individuals aged **59 to 90**, as survival analysis is often focused on older populations. It then increments the age by 1 (using `mutate(age = age + 1)`) to align the ages with the next time period (`df2`). Next, it performs an **inner join** between the modified `df1` and `df2` using matching columns (`age`, `sex`, `pa` \[possibly a region code\], and `sz`. The join suffixes (`_prev` and `_next`) distinguish between the population counts from the two time periods.

After joining, the function computes the **survival rate** (`rate`) by dividing the population in the later period (`pop_next`) by the population in the earlier period (`pop_prev`). Finally, it selects and returns only the relevant columns (`pa`, `sz`, `age`, `sex`, `rate`, `pop_prev`, `pop_next`) for further analysis.

In summary, this function helps estimate how many people from an initial cohort (in `df1`) survived into the next period (in `df2`) by age, sex, and other groupings, providing key insights for demographic or actuarial studies.

```{r}
compute_survival_rate <- function(df1, df2) {
  df1 %>%
    filter(age >= 59 & age < 91) %>%
    mutate(age = age + 1) %>%
    inner_join(df2, 
               by = c("age", "sex", "pa", "sz"),
               suffix = c("_prev", "_next")) %>%
    mutate(rate = pop_next / pop_prev) %>%
    select(pa, sz, age, sex, rate, pop_prev, pop_next)
}
```

```{r}
rates_2020_2021 <- compute_survival_rate(pop20, pop21)
```

```{r}
invalid_rates_2020_2021 <- rates_2020_2021 %>%
  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))
invalid_rates_rows
```

```{r}
rates_2021_2022 <- compute_survival_rate(pop21, pop22)
```

```{r}
invalid_rates_2021_2022 <- rates_2021_2022 %>%
  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))
invalid_rates_rows
```

```{r}
rates_2022_2023 <- compute_survival_rate(pop22, pop23)
```

```{R}
invalid_rates_2022_2023 <- rates_2022_2023 %>%
  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))
invalid_rates_rows
```

```{r}
rates_2023_2024 <- compute_survival_rate(pop23, pop24)
```

```{r}
invalid_rates_2023_2024 <- rates_2023_2024 %>%
  filter(!between(rate, 0, 1) | is.na(rate) | is.infinite(rate))
invalid_rates_rows
```

### Step 2: Refined

The function processes survival rate data through four sequential steps, now correctly using dplyr's grouping mechanism. First, it groups the data by both age and sex, which is crucial because survival rates likely vary significantly across these demographics. Within these groups, it performs three key operations: (1) capping any rates above 1.0 at 1.0 (assuming these represent survival probabilities that shouldn't exceed 100%), (2) replacing infinite values (which occur when dividing by zero) with the maximum finite rate found in the data, and (3) imputing missing values (NaN) with the median rate for that specific age-sex group. After these grouped operations, it ungroups the data and performs a final safety check, replacing any remaining missing values with 1.0 (a neutral value indicating no change).

```{r}
clean_age_column <- function(df) {
  df %>%
    mutate(
      age = str_trim(age),  # Trim whitespace
      age = if_else(age == "90_and_over", "90", age),  # Replace label
      age = suppressWarnings(as.numeric(age))  # Convert safely
    ) %>%
    filter(!is.na(age))  # Remove rows that still couldn't be converted
}
pop20 <- read_rds("data/rds/popdata/refined/popdata20_c.rds") %>% 
  clean_age_column()
pop21 <- read_rds("data/rds/popdata/refined/popdata21_c.rds") %>% 
  clean_age_column()
pop22 <- read_rds("data/rds/popdata/refined/popdata22_c.rds") %>% 
  clean_age_column()
pop23 <- read_rds("data/rds/popdata/refined/popdata23_c.rds") %>% 
  clean_age_column()
pop24 <- read_rds("data/rds/popdata/refined/popdata24_c.rds") %>% 
  clean_age_column()
```

```{r}
compute_survival_rate <- function(df1, df2) {
  df1 %>%
    filter(age >= 59 & age < 91) %>%
    mutate(age = age + 1) %>%
    inner_join(df2, 
               by = c("age", "sex", "pa", "sz"),
               suffix = c("_prev", "_next")) %>%
    mutate(rate = pop_next / pop_prev) %>%
    select(pa, sz, age, sex, rate, pop_prev, pop_next)
}
```

The `clean_rates` function serves as a natural post-processing step for the output of `compute_survival_rate`, addressing several statistical and data quality considerations inherent to survival rate calculations. The compute_survival_rate function generates initial survival rate estimates by computing population ratios between two time periods (pop_next/pop_prev) for specific demographic groups (defined by age, sex, pa, and sz). This calculation can produce three types of problematic outputs that clean_rates systematically addresses: (1) undefined values (infinities) resulting from zero denominators, (2) rates outside the valid [0,1] range for probabilities, and (3) missing values from the inner join operation or zero populations.

The cleaning process employs statistically appropriate methods: replacement of infinite values with NAs followed by median imputation within age-sex strata maintains the grouped structure used in the original calculation. The bounding of values preserves the probabilistic interpretation of survival rates. The two functions together implement a complete analytical workflow where compute_survival_rate performs the demographic-specific calculation and `clean_rates` ensures the results meet necessary statistical assumptions for subsequent analysis. This separation of concerns between calculation and validation follows established data processing paradigms, with each function handling a distinct phase of the data transformation pipeline.

adjusted code chunk for prov_next more than pop_prev
```{r}
  #use this
clean_rates <- function(df, rate_type = "survival") {
  df %>%
    group_by(age, sex) %>%
    mutate(
      # Handle infinites first - replace with NA for proper imputation
      rate = ifelse(is.infinite(rate), NA, rate),
      
      # Detect illogical population scenario: pop_next > pop_prev
      # This suggests population growth rather than mortality/migration
      pop_growth_detected = ifelse(exists("pop_next") && exists("pop_prev"), 
                                  pop_next > pop_prev, FALSE),
      
      # If population growth detected, impute rate based on rate_type
      rate = case_when(
        pop_growth_detected & rate_type == "survival" ~ 1.0,  # Perfect survival
        pop_growth_detected & rate_type == "probability" ~ 0.0,  # Zero probability of death
        pop_growth_detected & rate_type == "hazard" ~ 0.0,  # Zero hazard rate
        pop_growth_detected ~ NA_real_,  # Set to NA for other rate types
        TRUE ~ rate  # Keep original rate if no population growth issue
      ),
      
      # Cap rates only if they represent probabilities
      rate = if(rate_type == "survival" || rate_type == "probability") {
        pmin(pmax(rate, 0), 1)  # Bound between 0 and 1
      } else {
        pmax(rate, 0)  # Only ensure non-negative for hazard rates
      },
      
      # Impute missing values with group median
      rate = ifelse(is.na(rate), median(rate, na.rm = TRUE), rate)
    ) %>%
    ungroup() %>%
    # Final check - if entire groups had no valid data
    mutate(
      rate = ifelse(is.na(rate), 
                   if(rate_type == "survival") 1.0 else 0.0, 
                   rate)
    ) %>%
    # Clean up temporary column
    select(-pop_growth_detected)
}
```




```{r}
rates_2020_2021 <- compute_survival_rate(pop20, pop21) %>% clean_rates()
```
```{r}
rates_2021_2022 <- compute_survival_rate(pop21, pop22) %>% clean_rates()
```
```{r}
rates_2022_2023 <- compute_survival_rate(pop22, pop23) %>% clean_rates()
```
```{r}
rates_2023_2024 <- compute_survival_rate(pop23, pop24) %>% clean_rates()
```

### Step 3: Average the survival rates

```{r}
avg_survival_rates <- bind_rows(
  rates_2020_2021,
  rates_2021_2022,
  rates_2022_2023,
  rates_2023_2024
) %>%
  group_by(age, sex, pa, sz) %>%
  summarise(avg_rate = mean(rate, na.rm = TRUE), .groups = "drop") %>%
  distinct(age, sex, pa, sz, .keep_all = TRUE) #remove potential duplicates 
```

```{r}
#| echo: false
#| eval: false
write_rds(avg_survival_rates, "data/rds/popdata/avg_survival_rates.rds")
```

```{r}
#| echo: false
#| eval: false
avg_survival_rates = read_rds("data/rds/popdata/avg_survival_rates.rds)
```

### Step 4: Forecast each year 2025 to 2029

```{r}
forecast_year <- function(base_pop, rates, year) {
  # Ensure base_pop has no duplicates before processing
  base_pop_clean <- base_pop %>%
    group_by(age, sex, pa, sz) %>%
    summarise(pop = sum(pop, na.rm = TRUE), .groups = "drop")
  
  # Main forecasting for ages 59-88 (they become 60-89)
  next_pop <- base_pop_clean %>%
    filter(age >= 59 & age < 89) %>%  # Changed from < 90 to < 89
    mutate(age = age + 1) %>%
    left_join(rates, by = c("age", "sex", "pa", "sz")) %>%
    mutate(
      # Handle missing rates
      avg_rate = ifelse(is.na(avg_rate), 
                       mean(rates$avg_rate, na.rm = TRUE), 
                       avg_rate),
      pop = pop * avg_rate
    ) %>%
    select(age, sex, pa, sz, pop)
  
  # Handle age 89 and 90 - both stay at age 90 (terminal age)
  age_89_to_90 <- base_pop_clean %>%
    filter(age == 89) %>%
    left_join(rates %>% filter(age == 90), by = c("sex", "pa", "sz")) %>%
    mutate(
      age = 90,
      avg_rate = ifelse(is.na(avg_rate), 
                       mean(rates$avg_rate[rates$age == 90], na.rm = TRUE), 
                       avg_rate),
      pop = pop * avg_rate
    ) %>%
    select(age, sex, pa, sz, pop)
  
  # Age 90 stays at 90 (no aging beyond 90)
  age_90_stays <- base_pop_clean %>%
    filter(age == 90) %>%
    left_join(rates %>% filter(age == 90), by = c("sex", "pa", "sz")) %>%
    mutate(
      avg_rate = ifelse(is.na(avg_rate), 
                       mean(rates$avg_rate[rates$age == 90], na.rm = TRUE), 
                       avg_rate),
      pop = pop * avg_rate
    ) %>%
    select(age, sex, pa, sz, pop)
  
  # Combine age 89->90 and existing 90 populations
  pop_90 <- bind_rows(age_89_to_90, age_90_stays) %>%
    group_by(age, sex, pa, sz) %>%
    summarise(pop = sum(pop, na.rm = TRUE), .groups = "drop")
  
  # Combine all forecasted populations
  final_forecast <- bind_rows(next_pop, pop_90) %>%
    mutate(year = year) %>%
    # Ensure no duplicates in final output
    group_by(year, age, sex, pa, sz) %>%
    summarise(pop = sum(pop, na.rm = TRUE), .groups = "drop") %>%
    arrange(pa, sz, sex, age)
  
  return(final_forecast)
}
```


```{r}
#| echo: false
#| eval: false
write_rds(forecast_year, "data/rds/popdata/forecast_year.rds")
```

```{r}
#| echo: false
#| eval: false
forecast_year = read_rds("data/rds/popdata/forecast_year.rds)
```

### Step 5: Loop over the years

```{r}
# List to store forecasts
forecast_list <- list()
base_pop <- pop24

# Single loop with duplicate checking built in
for (y in 2025:2029) {
  cat("Forecasting year", y, "\n")
  
  # Generate forecast
  next_forecast <- forecast_year(base_pop, avg_survival_rates, y)
  
  # Check for duplicates in this year's forecast
  dupes <- next_forecast %>%
    group_by(age, sex, pa, sz) %>%
    filter(n() > 1)
  
  if(nrow(dupes) > 0) {
    warning(paste("Duplicates found in year", y, "- Count:", nrow(dupes)))
    print(dupes)
  } else {
    cat("✓ No duplicates found in year", y, "\n")
  }
  
  # Store the forecast
  forecast_list[[as.character(y)]] <- next_forecast
  
  # Update base population for next iteration
  # Make sure all required columns exist
  base_pop <- next_forecast %>% 
    select(age, sex, pa, sz, pop)  # Keep all demographic columns
  
  cat("Base population for next year has", nrow(base_pop), "rows\n\n")
}
```


### Final Output of Population Forecast

The below code chunk includes rounding up the `pop` as previously it was in decimal place. 
```{r}
all_forecasts <- bind_rows(forecast_list) %>%
  mutate(pop = ceiling(pop))  # Round UP to nearest integer
```



```{r}
#| echo: false
#| eval: false
write_rds(all_forecasts, "data/rds/popdata/all_forecasts.rds")
```

```{r}
#| echo: false
#| eval: false
all_forecasts = read_rds("data/rds/popdata/all_forecasts.rds")
```


```{r}
pivoted_data <- all_forecasts %>%
  pivot_wider(
    names_from = year,
    values_from = pop,
    names_prefix = "aged_",
    names_glue = "aged_{year}",
    values_fn = list(pop = list)  # Keeps multiple values as lists
  )
```


## MPSZ: Singapore's Master Plan 2019 Subzone Boundary

### Overview

Using `st_read`, the ESRI shapefile was imported and it contains 323 data entries and 15 fields. Each of the data entry consists of a multi-polygon shape, with geospatial coordinates with a geographic coordinate system (GCS) of WGS84. The features of GCS include using a 3D spherical model of earth with coordinates of longitude, latitude and altitude whereas PCS uses a 2D plane model with linear measurements (i.e. metres).

As the data file is in KML version. However, the file doesn't churn the full details of the Master Plan Boundary

```{r}
mpsz = st_read(dsn = "data/planningarea/",
               layer = "mpsz2019")
```

Using `glimpse()`, we are able to visualise the columns, column types and properties within the columns. Notably, there are numerous columns that are not needed for this analysis.

```{r}
glimpse(mpsz)
```

In the code chunk below, we use the `select()`function to pick the columns that are required for the anlaysis.

```{r}
mpsz <- mpsz %>%
  select(SUBZONE_N, PLN_AREA_N, REGION_N, geometry)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons("SUBZONE_N")  # Horizontal legend

```

```{r}
plot(mpsz["SUBZONE_N"])
```

### Transforming CRS

As the research's focus is exploring the accessibility of care centres in Singapore, PCS would be appropriate in this context as it measures the distance between the elderly' residence and the care centre. Thus, in ensuring accurate measurement, the function \`st_transform\` with a crs of 3414 was used (Kam, 2022) in the code chunk below.

```{r}
mpsz <- mpsz %>%
  st_transform(crs = 3414)
```

### Shared Boundaries

The below codechunk uses `st_is_valid` in assessing if there are shared boundaries in the `mpsz` data. This is important as shared boundaries can lead to inconsistent data impacting the research findings. 9 polygons with self-intersection ('Ring Self-Intersection') issues were returned.

```{r}
st_is_valid(mpsz, reason = TRUE)
```

This code chunk visualises the boundaries that are affected such as the smaller islands outside of Singapore Main Island: P. Ubin, P. Tekong, Sentosa Island

```{r}
invalid_polygons <- mpsz[!st_is_valid(mpsz),]
plot(invalid_polygons)
```

In addressing the above point, we will use st_buffer() of sf package to compute a 5-metres buffers around the data.

```{r}
mpsz <- st_buffer(mpsz, dist = 5)
```

```{r}
#| echo: false
#| eval: false
write_rds(mpsz, "data/rds/mpsz/mpsz.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz = read_rds("data/rds/mpsz/mpsz.rds")
```

### MPSZ Land Use

```{r}
mpsz_land = st_read(dsn = "data/landuse/",
               layer = "mp2019_landuse")
```

```{r}
st_crs(mpsz_land)
```

```{r}
st_is_valid(mpsz_land, reason = TRUE)
```

Using `unique()`, it returns a list of properties in the column `LU_DESC`. As the focus of the research is accessibility to care centre, all residential aspects will be factored in this research.

```{r}
unique(mpsz_land$LU_DESC)
```

In the code chunk below, we use the `select()`function to pick the columns that are required for the anlaysis. Additionally, `filter()` the column 'LU_DESC' for those words containing 'residential' Regular expression was used alongside ignoring captital letters.

```{r}
mpsz_res <- mpsz_land %>%
  select(LU_DESC, geometry) %>%
  filter(str_detect(LU_DESC, regex("residential", ignore_case = TRUE)))
```

### Sampling Grid

In measuring spatial accessibility, analytical grids of the sampling fields is used to standardise the means of measurement (Kam, 2022). These analytical grids can be comprised of equilateral triangles, squares or hexagon due to its ability to tessellate (ESRI, 2025). From the code output below, we noticed that the sampling fields in the `mpsz_res` are not equal for measurement. Therefore, it is imperative to select an appropriate analytical grid in measuring spatial accessibility. Hexagons grids is chosen over triangles and squares due to several factors. Firstly, the shape of a hexagon has a low-perimeter-to-area ratio, hence the edge effect of the grid shape reduces sampling bias. Secondly, when comparing equal area, any point inside a hexagon is closer to the centriod than any given point in an equal-area square or triangle due to the more acute angles of square and triangle versus the hexagon (Burdziej, 2018; ESRI, 2025). This is particular important in this research as we are using the centroid as a proxy in comparing accessibility to the various care centres. The centre of each hexagon is an Origin, additionally, acting as a starting point in ascertaining the shortest distance to the care centres.

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tmap_options(max.categories = n_distinct(mpsz_res$LU_DESC))  # Show all categories
tm_shape(mpsz_res) +
  tm_polygons("LU_DESC")
```

In the HealthierSG White Paper 2022, the Ministry of Health has indicated its intention to "expand the network \["care centres"\] to 220 by 2025". Furthermore, through the Ministry's estimation, 8 in 10 seniors will have a care centre in the vicinity of their homes (Ministry of Health, 2022). Hence, we will assume that the maximum distance to the care centres are 100 metres. Hence, in the code chunk below, `st_make_grid` from `sf` package constructed hexagonal grids encompassing the Singapore Master Plan 2019 Planning Subzone Boundary using `cellsize` that defines the radius of 100 metres and `square` to be false to generate a hexagonal grid.

```{r}
hex_res <- st_make_grid(mpsz_res,
                         cellsize = 100,
                         what = "polygon",
                         square = FALSE) %>%
  st_sf()
```

```{r}
hex_res$hex_id <- sprintf("H%04d", seq_len(nrow(hex_res))) %>% as.factor()
head(hex_res)
```

```{r}
#| echo: false
#| eval: false
write_rds(hex_res, "data/rds/mpsz/hex_res.rds")
```

```{r}
#| echo: false
#| eval: false
hex_res = read_rds("data/rds/mpsz/hex_res.rds")
```

### Visualing hex_grids & res

```{r}
# Set tmap mode and options
tmap_mode("plot")  # Use "view" for interactive map
tmap_options(check.and.fix = TRUE)
tmap_options(max.categories = n_distinct(mpsz_res$LU_DESC))
```

```{r}
# Create the base map with land use
base_map <- tm_shape(mpsz_res) +
  tm_shape(hex_grid) +
  tm_polygons("LU_DESC",
              palette = "Set3",
              title = "Land Use Type",
              alpha = 0.7,
              border.col = "gray30",
              lwd = 0.3) +
  tm_layout(legend.outside = TRUE,
            frame = FALSE)
```

```{r}
base_map
```

```{r}
# Add hexagon grid overlay
final_map <- base_map +
  tm_shape(hex_grid) +
  tm_borders(col = "black",
             lwd = 0.5,
             alpha = 0.5) +
  tm_text("hex_id",
          size = 0.5,
          col = "black",
          alpha = 0.7)
```

```{r}
# Display the map
final_map
```

### Cross-checking subzones in MPSZ & Forecast data.

```{r}
combined_original_case <- mpsz %>%
  distinct(SUBZONE_N) %>%
  mutate(join_key = toupper(SUBZONE_N)) %>%
  full_join(
    all_forecasts %>%
      distinct(sz) %>%
      mutate(join_key = toupper(sz)),
    by = "join_key"
  ) %>%
  select(SUBZONE_N, sz) %>%
  arrange(SUBZONE_N, sz)
```

###Combining MPSZ & Population Forecast

```{r}
mpsz_forecast <- mpsz %>%
  left_join(all_forecasts, by = c("SUBZONE_N" = "sz"))
```

### Precursor

There two sf data layers, namely:

-   hexagon: sf object of hexagonal polygons.
-   mpsz: sf object of planning subzones. Beside other field, there is a field contains the target population called aged_pop.

::: callout-note
Please ensure that both layers are in the same CRS (coordinate reference system, 3414 for svy21 ) and use projected units (e.g., meters).
:::

### Check and Transform CRS to Projected

Use the code chunk below to check if the data layers are in svy21 projected coornates system.

```{r}
glimpse(hex_centroids)
```

If it is not projected, use the code chunk to transform the sf data layers into svy21.

```{r}
hexagon <- st_transform(hexagon, 3414) 
mpsz <- st_transform(psz, 3414)
```

### Calculate the Area of Each Subzone

Next, the code below will compute the area of each polygon in mpsz.

```{r}
mpsz <- mpsz %>%
  mutate(mpsz_area = st_area(geometry))
```

### Intersect Hexagons with Subzones

The code chunk below Creates new polygons where hexagons and subzones overlap

```{r}
hex_mpsz_intersection <- st_intersection(hex_centroids, mpsz)
```

### Calculate Area of Each Intersection

```{r}
hex_mpsz_intersection <- hex_mpsz_intersection %>% 
  mutate(intersection_area =
           st_area(geometry))
```

### Estimate Hexagon Population

Assume uniform population density within each subzone. So, the hexagon’s share of the population is:

![](images/areaproportion.png)

```{r}
hex_mpsz_intersection <-
  hex_psz_intersection %>%
  mutate(hex_pop = as.numeric(
    intersection_area / mpsz_area * 
      population))

#to push population up
```

### Aggregate Estimated Population to Each Hexagon

If a hexagon overlaps multiple subzones, sum the estimated populations.

```{r}
hexagon_population <- 
  hex_mpsz_intersection %>%
  group_by(hex_id = row_number()) %>% 
  summarise(estimated_population =
              sum(hex_pop, na.rm = TRUE))
```

### Keeping original hexagon attributes:

```{r}
hexagon_final <- hexagon %>% 
  mutate(hex_id = row_number()) %>%
  left_join(hexagon_population, 
            by = "hex_id")
```

```{r}
#| echo: false
#| eval: false
write_rds(hexagon_final, "data/rds/mpsz/hexagon_final.rds")
```

```{r}
#| echo: false
#| eval: false
hexagon_final = read_rds("data/rds/mpsz/hexagon_final.rds")
```

*hexagon_final* now includes a new column *estimated_population*, which is the area-weighted population estimate for each hexagon.

## Care Centre

### Overview

```{r}
aac <- read_csv("data/carecentre/activeageingcentre.csv")
```

```{r}
counselling <- read_csv("data/carecentre/counselling.csv")
```

```{r}
daycare <- read_csv("data/carecentre/daycare.csv")
```

```{r}
dementia <- read_csv("data/carecentre/dementiadaycare.csv")
```

```{r}
hospice <- read_csv("data/carecentre/dayhospice.csv")
```

```{r}
maintenance <- read_csv("data/carecentre/maintenancedaycare.csv")
```

```{r}
nhrespite <- read_csv("data/carecentre/nhrespite.csv")
```

```{r}
nursing <- read_csv("data/carecentre/centrebasednursing.csv")
```

```{r}
rehab <- read_csv("data/carecentre/communityrehabcentre.csv")
```

```{r}
#| echo: false
#| eval: false
write_rds(aac, "data/rds/carecentre/original/aac.rds")
write_rds(counselling, "data/rds/carecentre/original/counselling.rds")
write_rds(daycare, "data/rds/carecentre/original/daycare.rds")
write_rds(dementia, "data/rds/carecentre/original/dementia.rds")
write_rds(hospice, "data/rds/carecentre/original/hospice.rds")
write_rds(maintenance, "data/rds/carecentre/original/maintenance.rds")
write_rds(nhrespite, "data/rds/carecentre/original/nhrespite.rds")
write_rds(nursing, "data/rds/carecentre/original/nursing.rds")
write_rds(rehab, "data/rds/carecentre/original/rehab.rds")
```

```{r}
#| echo: false
#| eval: false

aac = read_rds("data/rds/carecentre/original/aac.rds")
counselling = read_rds("data/rds/carecentre/original/counselling.rds")
daycare = read_rds("data/rds/carecentre/original/daycare.rds")
dementia = read_rds("data/rds/carecentre/original/dementia.rds")
hospice = read_rds("data/rds/carecentre/original/hospice.rds")
maintenance = read_rds("data/rds/carecentre/original/maintenance.rds")
nhrespite = read_rds("data/rds/carecentre/original/nhrespite.rds")
nursing = read_rds("data/rds/carecentre/original/nursing.rds")
rehab = read_rds("data/rds/carecentre/original/rehab.rds")
```

### Cursory View

Using the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns "web-scraper-order" and "web-scraper-start-url" are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.

```{r}
glimpse(aac)
```

### Deleting Unwanted Codes

The following R code is used to remove the columns "web-scraper-order" and "web-scraper-start-url" from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.

```{r}
aac <- aac %>% select(-"web-scraper-order", -"web-scraper-start-url")
counselling <- counselling %>% select(-"web-scraper-order", -"web-scraper-start-url")
daycare <- daycare %>% select(-"web-scraper-order", -"web-scraper-start-url")
dementia <- dementia %>% select(-"web-scraper-order", -"web-scraper-start-url")
hospice <- hospice %>% select(-"web-scraper-order", -"web-scraper-start-url")
maintenance <- maintenance %>% select(-"web-scraper-order", -"web-scraper-start-url")
nhrespite <- nhrespite %>% select(-"web-scraper-order", -"web-scraper-start-url")
nursing <- nursing %>% select(-"web-scraper-order", -"web-scraper-start-url")
rehab <- rehab %>% select(-"web-scraper-order", -"web-scraper-start-url")
```

After removing the two columns, each data set has two columns, namely name and address only.

```{r}
glimpse(aac)
```

### Checking for Missing Values

To check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.

The is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.

```{r}
# Checking for missing or null values in 'name' and 'address' columns
aac_missing <- aac %>% summarise(across(c(name, address), ~sum(is.na(.))))
counselling_missing <- counselling %>% summarise(across(c(name, address), ~sum(is.na(.))))
daycare_missing <- daycare %>% summarise(across(c(name, address), ~sum(is.na(.))))
dementia_missing <- dementia %>% summarise(across(c(name, address), ~sum(is.na(.))))
hospice_missing <- hospice %>% summarise(across(c(name, address), ~sum(is.na(.))))
maintenance_missing <- maintenance %>% summarise(across(c(name, address), ~sum(is.na(.))))
nhrespite_missing <- nhrespite %>% summarise(across(c(name, address), ~sum(is.na(.))))
nursing_missing <- nursing %>% summarise(across(c(name, address), ~sum(is.na(.))))
rehab_missing <- rehab %>% summarise(across(c(name, address), ~sum(is.na(.))))

```

### Duplicate Check

The code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \> 1) keeps only the rows that appear more than once (i.e., duplicates).

For each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.

Thus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.

```{r}
# Check for duplicates in 'aac'
aac_duplicate <- aac %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'counselling'
counselling_duplicate <- counselling %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'daycare'
daycare_duplicate <- daycare %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'dementia'
dementia_duplicate <- dementia %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'hospice'
hospice_duplicate <- hospice %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'maintenance'
maintenance_duplicate <- maintenance %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'nhrespite'
nhrespite_duplicate <- nhrespite %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'nursing'
nursing_duplicate <- nursing %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'rehab'
rehab_duplicate <- rehab %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

```

### Separating postal code from address

The code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.

```{r}
# Active Ageing Centre
aac <-mutate(aac,
    postal_code = str_extract(address, "[0-9]{6}$"),  # Extract postal code
    address = str_remove(address, "[,]?\\s*[0-9]{6}$")  # Remove postal code from address
  )
```

```{r}
# Counselling
counselling <- mutate(counselling,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Daycare
daycare <- mutate(daycare,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Dementia
dementia <- mutate(dementia,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Day Hospice
hospice <- mutate(hospice,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Maintenance Daycare
maintenance <- mutate(maintenance,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# NH Respite
nhrespite <- mutate(nhrespite,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Centre Based Nursing
nursing <- mutate(nursing,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Community Rehab Centre
rehab <- mutate(rehab,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Checking for missing or null values in 'name' and 'address' columns
aac_missing <- aac %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
counselling_missing <- counselling %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
daycare_missing <- daycare %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
dementia_missing <- dementia %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
hospice_missing <- hospice %>% summarise(across(c(name, address, postal_code),~sum(is.na(.))))
maintenance_missing <- maintenance %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
nhrespite_missing <- nhrespite %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
nursing_missing <- nursing %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
rehab_missing <- rehab %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
```

### Labelling Dataset

The below code chunk adds a column and naming it as "label" in relation to the name of the dataset hence we are able to identify the type of services provided by the care centres.

```{r}
aac <- aac %>%
  mutate(label = "aac")
```

```{r}
counselling <- counselling %>%
  mutate(label = "counselling")
```

```{r}
daycare <- daycare %>%
  mutate(label = "daycare")
```

```{r}
dementia <- dementia %>%
  mutate(label = "dementia")
```

```{r}
hospice <- hospice %>%
  mutate(label = "hospice")
```

```{r}
maintenance <- maintenance %>%
  mutate(label = "maintenance")
```

```{r}
nhrespite <- nhrespite %>%
  mutate(label = "nhrespite")
```

```{r}
nursing <- nursing %>%
  mutate(label = "nursing")
```

```{r}
rehab <- rehab %>%
  mutate(label = "rehab")
```

```{r}
#| echo: false
#| eval: false
write_rds(aac, "data/rds/carecentre/refined/aac.rds")
write_rds(counselling, "data/rds/carecentre/refined/counselling.rds")
write_rds(daycare, "data/rds/carecentre/refined/daycare.rds")
write_rds(dementia, "data/rds/carecentre/refined/dementia.rds")
write_rds(hospice, "data/rds/carecentre/refined/hospice.rds")
write_rds(maintenance, "data/rds/carecentre/refined/maintenance.rds")
write_rds(nhrespite, "data/rds/carecentre/refined/nhrespite.rds")
write_rds(nursing, "data/rds/carecentre/refined/nursing.rds")
write_rds(rehab, "data/rds/carecentre/refined/rehab.rds")
```

```{r}
#| echo: false
#| eval: false

aac = read_rds("data/rds/carecentre/refined/aac.rds")
counselling = read_rds("data/rds/carecentre/refined/counselling.rds")
daycare = read_rds("data/rds/carecentre/refined/daycare.rds")
dementia = read_rds("data/rds/carecentre/refined/dementia.rds")
hospice = read_rds("data/rds/carecentre/refined/hospice.rds")
maintenance = read_rds("data/rds/carecentre/refined/maintenance.rds")
nhrespite = read_rds("data/rds/carecentre/refined/nhrespite.rds")
nursing = read_rds("data/rds/carecentre/refined/nursing.rds")
rehab = read_rds("data/rds/carecentre/refined/rehab.rds")
```

### Append all Care Centres into one dataset

The code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named c_data using the bind_rows() function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting c_data will contain all the rows from the individual datasets, assuming they have the same column structure.

```{r}
cc_data <- bind_rows(
  aac, 
  counselling,
  daycare,
  dementia,
  hospice,
  maintenance,
  nhrespite,
  nursing,
  rehab,
)
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_data, "data/rds/carecentre/refined/cc_data.rds")
```

```{r}
#| echo: false
#| eval: false
cc_data = read_rds("data/rds/carecentre/refined/cc_data.rds")
```

### Transforming Categorical Data to Binary Indicator

This code transforms the dataset `cc_data` from a long format to a wide format by pivoting on the categorical values in the `label` column, effectively converting them into binary indicator columns. The process begins by removing the `address` column using `select(-address)` to exclude it from the transformation. Next, a new column called `present` is created using `mutate(present = 1)`, where every row is assigned a value of 1 to indicate the presence of a label. The key reshaping operation is performed using `pivot_wider()`, which spreads the unique values from the `label` column into separate columns. The `names_from = label` argument specifies that the new column names should be derived from the distinct categories in `label`, while `values_from = present` fills these new columns with the corresponding 1s from the `present` column. Any missing combinations (where a particular label does not appear for a given record) are automatically filled with 0s due to the `values_fill = list(0)` argument. The final output, stored in `pivoted_cc_data`, is a wider dataframe where each original row now has binary flags (1 or 0) indicating the presence or absence of each label category, making it suitable for analyses that require a one-hot encoded or dummy variable representation of categorical data.

correct code

```{r}

pivoted_cc_data <- cc_data %>%
  select(-address) %>%

  mutate(present = 1) %>%  # Create a column to indicate presence (1)

  pivot_wider(

    names_from = label,    # Pivot based on the 'label' column

    values_from = present,
    values_fill = list(0)# Use the 'present' column for the values
    
  )
```

using arrange(0, we are able to see that that the centres are arranged in alphabetically order and they are similar ones.

```{r}
arrange(pivoted_cc_data)
```

```{r}
pivoted_cc_data$name[duplicated(pivoted_cc_data$name)]
```

```{r}
duplicate_rows <- pivoted_cc_data %>%
  # Count occurrences of each name
  add_count(name) %>%
  # Filter for names that appear more than once
  filter(n > 1) %>%
  # Remove the count column
  select(-n) %>%
  # Arrange by name for better readability
  arrange(name)

if (nrow(duplicate_rows) > 0) {
  print(duplicate_rows)
} else {
  message("No duplicates found in the 'name' column.")
}
```

```{r}
#| echo: false
#| eval: false
write_rds(pivoted_cc_data, "data/rds/carecentre/refined/pivoted_cc_data.rds")
```

```{r}
#| echo: false
#| eval: false
pivoted_cc_data = read_rds("data/rds/carecentre/refined/pivoted_cc_data.rds")
```

### Adding coordinates to care centre

This code prepares a list of unique postal codes from a dataset called `pivoted_cc_data` to be used for geocoding via an API. The line first extracts the `postal_code` column from the dataframe, then applies `unique()` to eliminate duplicate postal codes - this optimization reduces the number of API calls needed since the same postal code will return the same coordinates. The `sort()` function then arranges these unique postal codes in ascending order, which serves two purposes: it makes the list more organized for human review (easier to locate specific codes during debugging or verification), and it may help with processing efficiency when matching the geocoded results back to the original dataset. The resulting sorted unique list is stored in `add_list`, which can then be passed to a geocoding API (like the OneMap API in Singapore) that typically accepts individual addresses or postal codes rather than entire dataframes. This preprocessing step is crucial because APIs often have rate limits or usage constraints, so minimizing duplicate requests helps maximize efficiency and reduce potential errors or bottlenecks in the geocoding process. The comment "parse a list as API cannot read df" explicitly notes that this conversion from dataframe column to simple list is necessary because the target API expects individual values rather than dataframe structures as input.

```{r}
add_list <- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df
#unique reduces records to pass to portal
#sort is used to easier to find geo codes
```

The below codechunk defines a function called get_coords that takes a list of Singapore postal codes as input and retrieves their geographic coordinates from the OneMap API (a Singapore government mapping service). The function first initializes an empty data frame to store the results. For each postal code in the input list, it makes an HTTP GET request to the OneMap API, which returns the location data in JSON format. The function then processes the response differently depending on how many matches are found: if there's exactly one match, it extracts those coordinates; if there are multiple matches, it looks for an exact postal code match; and if no matches are found, it records NA values. The valid coordinates (in WGS84 latitude/longitude format) are converted into an sf (simple features) spatial object, which is then transformed to Singapore's SVY21 projected coordinate system (EPSG:3414). The function extracts these SVY21 coordinates and merges them back into the original data frame, preserving any rows that had invalid or missing coordinates. The final output is a data frame containing the original postal codes, any matching postal codes found, WGS84 coordinates, SVY21 coordinates, and the geometric data as an sf geometry column. This function is particularly useful for geocoding Singapore addresses and preparing spatial data for analysis with Singapore-specific geographic information systems.

```{r}
get_coords <- function(postal_list){
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (postal in postal_list){
    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=postal,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each postal code
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal_code <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(postal_code = postal, 
                           postal_found = postal_code, 
                           latitude_wgs84 = lat,
                           longitude_wgs84 = lng)
    }
    
    # If multiple results, use the exact postal code match
    else if (found > 1){
      # Find exact match for postal code
      res_match <- res[res$POSTAL == postal, ]
      
      # If exact match found, use it
      if (nrow(res_match) > 0) {
        postal_code <- res_match$POSTAL[1]
        lat <- res_match$LATITUDE[1]
        lng <- res_match$LONGITUDE[1]
        new_row <- data.frame(postal_code = postal,
                             postal_found = postal_code,
                             latitude_wgs84 = lat,
                             longitude_wgs84 = lng)
      }
      # If no exact match, set as NA
      else {
        new_row <- data.frame(postal_code = postal,
                             postal_found = NA,
                             latitude_wgs84 = NA,
                             longitude_wgs84 = NA)
      }
    }
    # If no results found
    else {
      new_row <- data.frame(postal_code = postal,
                           postal_found = NA,
                           latitude_wgs84 = NA,
                           longitude_wgs84 = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  
  # Convert to sf object with WGS84 coordinates (EPSG:4326)
  # Filter out rows with NA coordinates first
  valid_coords <- postal_coords[!is.na(postal_coords$latitude_wgs84) & 
                              !is.na(postal_coords$longitude_wgs84), ]
  
  if(nrow(valid_coords) > 0) {
    coords_sf <- st_as_sf(valid_coords, 
                         coords = c("longitude_wgs84", "latitude_wgs84"),
                         crs = 4326)
    
    # Transform to SVY21 (EPSG:3414)
    coords_svy21 <- st_transform(coords_sf, 3414)
    
    # Extract coordinates
    coords_matrix <- st_coordinates(coords_svy21)
    
    # Add SVY21 coordinates back to the original dataframe
    valid_coords$longitude <- coords_matrix[, 1]  # SVY21 X coordinate
    valid_coords$latitude <- coords_matrix[, 2]   # SVY21 Y coordinate

    
    # Merge back with rows that had NA coordinates
    result <- merge(postal_coords, 
                   valid_coords[c("postal_code", "longitude", "latitude")], 
                   by = "postal_code", all.x = TRUE)
  } else {
    # If no valid coordinates, add empty SVY21 columns
    result <- postal_coords
    result$longitude <- NA
    result$latitude <- NA
  }
  
  return(result)
}


```

The code `coords <- get_coords(add_list)` calls the previously defined `get_coords()` function to geocode (convert to geographic coordinates) a list of Singapore postal codes stored in `add_list`.

```{r}
coords <- get_coords(add_list)
```

The below code chunk merges the original dataset (`pivoted_cc_data`) with the geocoded coordinates (`coords`) using a left join operation, which preserves all records from the primary dataset while matching and appending geographic data where available using the properties of `postal_code` in both dataframes.

```{r}
cc_data_final <- pivoted_cc_data %>%
  left_join(coords, 
            join_by(postal_code == postal_code)
)
```

```{r}
cc_sf <- st_as_sf(cc_data_final,
                  coords = c("longitude", "latitude"), #c is use column
                         crs = 3414)
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_data_final, "data/rds/carecentre/refined/cc_data_final.rds")
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_sf, "data/rds/carecentre/refined/cc_sf.rds")
```

```{r}
#| echo: false
#| eval: false
cc_data_final = read_rds("data/rds/carecentre/refined/cc_data_final.rds")
```

```{r}
#| echo: false
#| eval: false
cc_sf = read_rds("data/rds/carecentre/refined/cc_sf.rds")
```

```{r}
mpsz_land_fixed <- st_make_valid(mpsz_land)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz_land_fixed) +
  tm_polygons()
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(cc_sf) +
  tm_dots() #change to name
```

```{r}
st_crs(cc_sf)
```

```{r}
tm_shape(mpsz)+
  tm_fill("DEPENDENCY", 
          style = "quantile", 
          palette = "Blues",
          title = "Dependency ratio") +
  tm_layout(main.title = "Distribution of Dependency Ratio by planning subzone",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2) +
  tm_credits("Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\n and Population data from Department of Statistics DOS", 
             position = c("left", "bottom"))
```

# Exploratory Data Analysis

```{r}
tmpa_mode("view")
```

```{r}
tm_shape(mpsz)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

## Density of Care Centre in Each Subzone

refer chapt 1

```{r}
mpsz$`CC Count` <- lengths(st_intersects(mpsz, ))
```

# References

Burdziej, J. (2019). Using hexagonal grids and network analysis for spatial accessibility assessment in urban environments-A case study of public amenities in Toruń. Miscellanea Geographica. Regional Studies on Development, 23(2), 99-110.

ESRI. (2025). Why hexagons?. Retrieved on May, 2025 from https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-whyhexagons.htm

Kam, T. S. (2024). R for Geospatial Data Science and Analytics. Retrieved on April 2, 2, 2025 from https://r4gdsa.netlify.app/

Kam, T. S. (2022). GIS for Urban PlanningL QGIS methods. Retrieved on May 2, 2025 from https://gis4urbplan.netlify.app/

Tan, K. (2023). Take-home Exercise 1: Geospatial Analytics for Public Good. Retrieved from <https://isss624-kytjy.netlify.app/take-home_ex/take-home_ex1/the1#background>

Ministry of Health Singapore. (2022). Healthier SG White Paper. Retrieved from https://file.go.gov.sg/healthiersg-whitepaper-pdf.pdf

Urban Redevelopment Authority. (2023). Master Plan 2019 Planning Area Boundary (No Sea) (2024) \[Dataset\]. data.gov.sg. Retrieved February 23, 2025 from https://data.gov.sg/datasets/d_6c6d7361dd826d97b91bac914ca6b2ac/view
