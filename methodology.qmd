---
title: "Methodology"
date: "February 21, 2025"
date-modified: "last-modified"
format:
 html:
    code-fold: false
    code-summary: "Code Chunk"
    number-sections: true
execute: 
  eval: true #r will run through all codes
  echo: true #r will display all code chunk
  warning: false #for markdown
  freeze: true #r will not render all existing html files
  message: false #avoid printing warning message
editor: source
---

# Methodology Overview

In this section, we will acquire the various data sets required for this research from various government open-sources data repositories and websites. Thereafter, we will install the necessary R packages and import the data sets. In each data set, we will delve into the necessary data checks, issues and transformation. Lastly, exploratory data analysis is done.

# Data Acquisition

In this research, 3 main spectrums of data will be required for this research as depicted in Figure 1, namely the Population Data, Master Plan 2019 Subzone boundary and Care Centres. These data sets are gathered from multiple open-source websites which includes Singapore Department of Statistics, Data.gov.sg, and Agency of Integrated Care.

![Figure 1: Data Overview](/metho_images/data_overview.png){fig-align="center"}

## Singapore Master Plan 2019 Subzone Boundary

The first spectrum of data sets will be the Singapore Master Plan 2019 Planning Subzone Boundary (No Sea) and Amendment to Master Plan 2019 Land Use Layer are ESRI shapefiles that were downloaded from Data.gov.sg. These will act as a base layer for geographical mapping of the project.

## Population Data

The second spectrum of data sets will be the population datasets that were retrieved from [SingStat.gov.sg](https://www.singstat.gov.sg/find-data/search-by-theme/population/geographic-distribution/latest-data). Specifically, Singapore Resident by Planning Area/Subzone, Single Year of Age and Sex, June 2024, 2023, 2022, 2021, and, 2011-2020 were selected for this research. As the version January 2025 will be not be ready in due time for this research project, thus, the latest available dataset, June 2024 will be used. This data sets are used to project the older adults populations in Singapore.

## WebScraping of Care Centres

The third spectrum of dataset is the Care Centre. Due to the lack of a centralised data of all care centres, web scraping is warranted in retrieving the geographical information of the care centres. The geographical locations of the Care Centres alongside the centre names such as Active Ageing Centre, Day Care, Community Rehabilitation Centre, Centre-based Nursing were extracted using a web scraping tool, Web Scraper, available in Chrome web store as Seen in Figure 2. As there is no centralised file that consist of the centre names and their locations, the location of each centre has to be manually extracted, filtered by each centre type, from the [Care Services](https://www.aic.sg/care-services/) webpage of the Agency of Integrated Care in the following figures. The steps of data scraping will be laid out in details in the next paragraph.

Step 0: Download Web Scraper from Chrome web store

Web Scraper is used as it is free, works reasonably well and available in both Chrome and Firefox web store. In the below steps, Chrome will be the default web browser used.

![*Figure 2: Web Scraper*](/metho_images/step0.png)

Step 1: Navigate to Developer Tools in Chrome Web Browser

After downloading the extension from Chrome Web Store, press onto the menu bar at the right of the browser and locate Developer Tools while onto the website you would like to scrape information from.

![*Figure 3: Web Scraper*](/metho_images/step1.png)

Step 2: Interface for Webscraper

After clicking onto Developer Tools, click onto the Web Scraper in the menu bar (in black). Following which the below interface will appear.

[![Figure 4: Interface with Website](/metho_images/step2.png){width="627"}](Figure%20x:%20Locate%20Developer%20Tools)

Step 3: Create New Sitemap

Click onto "create new sitemap", thereafter "Create Sitemap". Sitemap Name will be the overarching term used for these information; in this instance, it will be AAC. The Start URL will be the HTML link that you would like the information to be scraped from.

![Figure 5: Creating New Sitemap](/metho_images/step3.png){fig-align="center"}

Step 4: Add New Selector

After creating a new sitemap, the following interface will appear. Click onto the "Add new selector" to select the information to scrape.

![Figure 6: Adding New Selector](/metho_images/step4.png){fig-align="center"}

Step 5: Selecting Whole Box

Firstly, the id will be the column name. For Type, select Element Attribute from the drop down selection. Thereafter, press on Select under Selector and select two boxes of each centre as seen in the figure below (the remaining boxes will be highlighted through its intelligent function) and press onto Done Selecting in the green box.

![Figure 7: Selecting Element](/metho_images/step5.png){fig-align="center"}

Step 6: Sitemap Interface

After adding a new selector, the sitemap page will appear the selector that you've inputted.

![Figure 8: Create New Sitemap](/metho_images/step6.png){fig-align="center"}

Step 7: Selecting Name of Care Centre

Firstly, the id will be name (with reference to the name of care centre), serving as the column name. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 names of the care centres (The remaining care centres will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple names and root parent selector will be root and press onto Save Selector.

![Figure 10: Selecting Name of Care Centre](/metho_images/step7.png){fig-align="center"}

Step 8: Misconfiguration Detected

A popup window will be prompted and Group selectors was selected.

![Figure 11: Misconfiguration Detected](/metho_images/step8.png){fig-align="center"}

Step 9: Selecting Address of Care Centre

Similar to Step 7, the id will be address. Text will be chosen under Type thereafter press Select under Selector and highlight the first 2 addresses of the care centres (Remaining addresses will be highlighted through its intelligent function) and press onto Done selecting in the green box. Multiple box will be selected as we would like to scrap multiple addresses and parent selector will be wrapper_for_main_name (as we grouped selectors in step 8) and press onto Save Selector.

![Figure 12: Selecting Address of Care Centre](/metho_images/step9.png){fig-align="center"}

Step 10: Data Preview

Prior to data scraping, the data is previewed in ensuring each name of the care centre is correctly tagged to the address using the main website to verify.

![Figure 13: Data Preview](/metho_images/step10.png){fig-align="center"}

Step 11: Commence Scraping

Head over to sitemap aac and click onto Scrape. A new browser will appear indicating that it is in process of scraping. It will be closed automatically once the process has ended.

![Figure 14: Commence Scraping](/metho_images/step11.png){fig-align="center"}

Step 11: Export Data

Export data is selected upon clicking sitemap aac. 2 file options are offered: csv and xlsx. The former was chosen as CSV files are simple and portable which doesn't complicate data processing. Thereafter the data will be downloaded.

![Figure 15: Export Data](/metho_images/step12.png){fig-align="center"}

Step 11: View CSV File

In ensuring the web scraping successful and accurate, the csv. file is opened and examined.

![Figure 16: View CSV File](/metho_images/step13.png){fig-align="center"}

The above steps were repeated for each type of care centre. All of the Care Centre data were extracted on 7th February 2025.

# Data Cleaning & Manipulation

## Installing Packages

::: panel-tabset
The following packages are required for this section:

| Package | Description |
|----|----|
| [**tidyverse**](https://www.tidyverse.org/) | For non-spatial data wrangling that includes dplyr, tibble, ggplot2, readr, tidyr, stringr, forcats, lubridate and purr |
| [**sf**](https://r-spatial.github.io/sf/) | For importing, managing, and handling geospatial data |
| [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/index.html) | For interacting with a web API |
| [**rvest**](https://cran.r-project.org/web/packages/rvest/index.html) | Wrapper around xml2 and httr packages |
| [**sfdep**](https://sfdep.josiahparry.com/) | Used to compute spatial weights, global and local spatial autocorrelation statistics |
| [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) | For creating elegent and cartographic quality thematic maps |
| [**knitr**](https://cran.r-project.org/web/packages/knitr/index.html) | For dynamic report generation |
| [**patchwork**](https://patchwork.data-imaginist.com/) | For plot manipulation |
| [**leaflet**](https://rstudio.github.io/leaflet/) | For interactive maps |
| [**scales**](https://scales.r-lib.org/) | For scaling graphs |
| [**lubridate**](https://lubridate.tidyverse.org/) | For date-time manipulation |
| [**tmap**](https://r-graph-gallery.com/package/tmap.html) | For creating interactive maps |
| [**DT**](https://rstudio.github.io/DT/) | Provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, and many other features in the tables. |
| [httr](https://cran.r-project.org/web/packages/httr/index.html) | Tools for working with URLs and HTTP |

The code chunk below, using `p_load` function of the [**pacman**](https://cran.r-project.org/web/packages/pacman/pacman.pdf) package, ensures that packages required are installed and loaded in R.

```{r}
pacman::p_load(tidyverse, sf,
               jsonlite, rvest, knitr, patchwork,
               leaflet, scales, lubridate, tmap, DT, httr,SpatialAcc)
```
:::

## Population Data

### Overview

In this section, a total of 5 population datasets from year 2020 to year 2024 will be used in the analysis. The datasets will be cleaned and transformed. Following which, survival analysis will be done in estimating the population for age 60 and above for the period 2025 to 2029.

### Importing Data

In importing data, `read_csv()` and `rename_with()` of tidyverse package are used to perform column name standardisation by converting all variable names in the respective datasets to lowercase.

:::: panel-tabset
#### Population Data 2024

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2024.ashx)

```{r}
popdata24 <- read_csv("data/popdata/respopagesex2024.csv") %>%
  rename_with(tolower)
```

#### Population Data 2023

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2023](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2023.ashx)

```{r}
popdata23 <- read_csv("data/popdata/respopagesex2023.csv") %>%
  rename_with(tolower)
```

#### Population Data 2022

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2022](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2022.ashx) 

```{r}
popdata22 <- read_csv("data/popdata/respopagesex2022.csv") %>%
  rename_with(tolower)
```

::: callout-warning
PARSING ERROR\*

```         
Warning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:
  dat <- vroom(...)
  problems(dat)Rows: 60424 Columns: 6── Column specification
```
:::

#### Population Data 2021

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2021](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2021.ashx)

```{r}
popdata21 <- read_csv("data/popdata/respopagesex2021.csv") %>%
  rename_with(tolower)
```

#### Population Data 2020

[Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2011-2020](https://www.singstat.gov.sg/-/media/files/find_data/population/statistical_tables/respopagesex2011to2020.ashx)

```{r}
popdata20 <- read_csv("data/popdata/respopagesex2011to2020.csv") %>%
  rename_with(tolower) %>%
  filter(time == 2020)
```
::::

```{r}
#| echo: false
#| eval: false
write_rds(popdata24, "data/rds/popdata/original/popdata24.rds")
write_rds(popdata23, "data/rds/popdata/original/popdata23.rds")
write_rds(popdata21, "data/rds/popdata/original/popdata21.rds")
write_rds(popdata20, "data/rds/popdata/original/popdata20.rds")
```

```{r}
#| echo: false
#| eval: false
popdata24 = read_rds("data/rds/popdata/original/popdata24.rds")
popdata23 = read_rds("data/rds/popdata/original/popdata23.rds")
popdata21 = read_rds("data/rds/popdata/original/popdata21.rds")
popdata20 = read_rds("data/rds/popdata/original/popdata20.rds")
```

```{r}
glimpse(popdata23)
```

### Checking for Missing Values

To check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.

The is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.

Results: We noticed that there are 30 missing values popdata22 specifically under the column pop.

```{r}
popdata20_missing <- popdata20 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata20_missing)

popdata21_missing <- popdata21 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata21_missing)

popdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata22_missing)

popdata23_missing <- popdata23 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata23_missing)

popdata24_missing <- popdata24 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata24_missing)


```

### Issue with `POPDATA22`

Using the below code, we are able to see clearly the rows that are affected and in the pop column, it appears as NA. The csv file (respopagesex2022.csv) was opened using excel and each row returned in the below output was then cross checked in excel. Whole numbers with comma appeared in excel. This may be because read_csv() function expects a numeric value (double) in one of the columns, but instead, it found a string (the values in the column are likely formatted with commas, such as "1,020"). This is why the parser is raising an issue earlier on.

```{r}
na_rows <- popdata22[is.na(popdata22$pop), ]
print(na_rows)
```

Referencing from [Stackoverflow](https://stackoverflow.com/questions/1523126/how-to-read-data-when-some-numbers-contain-commas-as-thousand-separator), the first line of the code is necessary as it defines a new class called `"num.with.commas"`. This class is intended to handle numeric values that are stored as strings with commas (e.g., `"1,000"`). Thereafter, the second line of the code defines a method to convert a `character` type to the custom `"num.with.commas"` class.

-   The `gsub(",", "", from)` function removes commas from the string (e.g., `"1,000"` becomes `"1000"`)

-   The `as.numeric()` function then converts the cleaned string into a numeric value (e.g., `"1000"` becomes `1000`)

This ensures that numbers with commas are properly converted to numeric values during data import.

```{r}
setClass("num.with.commas")
setAs("character", "num.with.commas", 
        function(from) as.numeric(gsub(",", "", from) ) )
```

The file is then re-imported again and specifically, the column 'pop' is parsed as a character field in facilitating the next step in removing commas within the population itself.

```{r}
popdata22 <- read_csv("data/popdata/respopagesex2022.csv", 
                      col_types = cols(
                        PA = col_character(),
                        SZ = col_character(),
                        Age = col_character(),
                        Sex = col_character(),
                        Pop = col_character(),
                        Time = col_number()  # Adjust if necessary
                      )) %>%
  rename_with(tolower)
```

As previously stated, commas are present in the 'pop' column, hence, `mutate()`

```{r}
popdata22 <- popdata22 %>%
  mutate(pop = as.numeric(str_replace_all(pop, ",", "")))
```

```{r}
#| echo: false
#| eval: false
popdata22 = read_rds("data/rds/popdata/original/popdata22.rds")
```

```{r}
#| echo: false
#| eval: false
write_rds(popdata22, "data/rds/popdata/original/popdata22.rds")
```

In the below codechunk, it was verified that there is no missing values and the above steps taken were successful.

```{r}
names(popdata22) <- tolower(names(popdata22))
popdata22_missing <- popdata22 %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.))))
print(popdata22_missing)
```

In the below `code chunk a & b`, we noticed that it returned two different outputs: `90_and_over` and `90_and_Over`. This may explain why the error `NAs introduced by coercion` was returned.

```{r}
#code chunk a
popdata22 %>% 
  summarise(max_age = max(age, na.rm = TRUE))
```

```{r}
#code chunk b
popdata24 %>% 
  summarise(max_age = max(age, na.rm = TRUE))
```

Hence, in addressing the above issue, the below code chunk was developed. First, The code defines a function called `convert_age` that takes a dataframe (`df`) as input. Inside the function, it modifies the `age` column using `mutate()`. It checks each value in the `age` column to see if it contains either "\_and_over" or "\_and_Over" (case-insensitive match). When a match is found, it extracts just the numeric part (e.g., "90" from "90_and_over") using `str_extract()`. If no match is found, it keeps the original value. The second `mutate()` converts the cleaned `age` column to numeric values, ensuring all ages are stored as numbers. The function returns the modified dataframe with standardised age values.

```{r}
convert_age <- function(df) {
  df %>%
    mutate(age = if_else(
      str_detect(age, regex("_and_Over|_and_over", ignore_case = TRUE)),
      str_extract(age, "\\d+"),  # Extract just the numeric part
      age
    )) %>%
    mutate(age = as.numeric(age))
}
```

```{r}
popdata20_c <- convert_age(popdata20)
popdata21_c <- convert_age(popdata21)
popdata22_c <- convert_age(popdata22)
popdata23_c <- convert_age(popdata23)
popdata24_c <- convert_age(popdata24)
```

Another layer of confirmation of missing values was executed in ensuring no missing values were returned during the abovementioned process and it returns 0 for each dataset.

```{r}
popdata20_missing <- popdata20_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata20_missing)
popdata21_missing <- popdata21_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata21_missing)
popdata22_missing <- popdata22_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata22_missing)
popdata23_missing <- popdata23_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata23_missing)
popdata24_missing <- popdata24_c %>% summarise(across(c(pa,sz,age,sex,pop,time), ~sum(is.na(.)))) 
print(popdata24_missing)
```

### Duplicate Check

The code provided checks for duplicate rows in each dataset by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \> 1) keeps only the rows that appear more than once (i.e., duplicates).

For each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.

Thus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.

```{r}
# Check for duplicates in 'aac'
popdata20_duplicate <- popdata20_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata20_duplicate)


# Check for duplicates in 'counselling'
popdata21_duplicate <- popdata21_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata21_duplicate)

# Check for duplicates in 'daycare'
popdata22_duplicate <- popdata22_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata22_duplicate)

# Check for duplicates in 'dementia'
popdata23_duplicate <- popdata23_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata23_duplicate)

# Check for duplicates in 'hospice'
popdata24_duplicate <- popdata24_c %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()
show(popdata24_duplicate)

```

```{r}
#| echo: false
#| eval: false
write_rds(popdata24_c, "data/rds/popdata/refined/popdata24_c.rds")
write_rds(popdata23_c, "data/rds/popdata/refined/popdata23_c.rds")
write_rds(popdata22_c, "data/rds/popdata/refined/popdata22_c.rds")
write_rds(popdata21_c, "data/rds/popdata/refined/popdata21_c.rds")
write_rds(popdata20_c, "data/rds/popdata/refined/popdata20_c.rds")
```

```{r}
#| echo: false
#| eval: false
popdata24_c = read_rds("data/rds/popdata/refined/popdata24_c.rds")
popdata23_c = read_rds("data/rds/popdata/refined/popdata23_c.rds")
popdata22_c = read_rds("data/rds/popdata/refined/popdata22_c.rds")
popdata21_c = read_rds("data/rds/popdata/refined/popdata21_c.rds")
popdata20_c = read_rds("data/rds/popdata/refined/popdata20_c.rds")
```

### Percentage of Missing Values

In `pop(year)_geri_zero`, the dataframe consists of older adults aged 60 & above where the population is zero meanwhile `pop20_geri` consists of the full dataframe of the older adults aged 60 & above. Lastly, the code chunk returns the percentage of missing values in df. From 2020 to 2024, there are 43 - 41% of missing values.

```{r}
pop20_geri_zero <- popdata20_c %>%
  filter(pop == 0, age>=60)
pop20_geri <- popdata20_c %>%
  filter(age>=60)
(nrow(pop20_geri_zero) / nrow(pop20_geri)) * 100

pop21_geri_zero <- popdata21_c %>%
  filter(pop == 0, age>=60)
pop21_geri <- popdata21_c %>%
  filter(age>=60)
(nrow(pop21_geri_zero) / nrow(pop21_geri)) * 100

pop22_geri_zero <- popdata22_c %>%
  filter(pop == 0, age>=60)
pop22_geri <- popdata22_c %>%
  filter(age>=60)
(nrow(pop22_geri_zero) / nrow(pop22_geri)) * 100

pop23_geri_zero <- popdata23_c %>%
  filter(pop == 0, age>=60)
pop23_geri <- popdata23_c %>%
  filter(age>=60)
(nrow(pop23_geri_zero) / nrow(pop23_geri)) * 100

pop24_geri_zero <- popdata24_c %>%
  filter(pop == 0, age>=60)
pop24_geri <- popdata24_c %>%
  filter(age>=60)
(nrow(pop24_geri_zero) / nrow(pop24_geri)) * 100
```

### Survival Analysis

#### Introduction

In this section, the population data spanning from 2020 to 2024 will undergo cleaning and transformation processes to prepare it for advanced analytical modeling. This is necessary as it projects the demand (older adults) population from 2025 to 2029 using historical data from 2020 to 2024. In the next few chapters, the supply (care centres) will be populated against the forecast older adults population. Following this data preparation phase, survival analysis methodologies will be employed to predict the older adult population demographics from 2025 to 2029. The primary objective of this analytical approach is to calculate survival rates, where death serves as the event of interest in our survival framework. Given the nature of the population data being utilised, we adopt a non-parametric analytical stance, implementing the Kaplan-Meier estimator as our core statistical method for survival probability estimation. A fundamental assumption underlying this analysis is that residents maintain residential stability within their respective subzones throughout the entire study period. Under this assumption, any observed decrease in population within a given subzone can be reasonably attributed to mortality events, as migration and relocation factors are held constant. This methodological framework allows for a more accurate assessment of demographic changes driven primarily by natural population dynamics rather than residential mobility patterns.

#### Step 0: Defining `clean_age_column`Function

This code defines a function called `clean_age_column` that takes a dataframe as input and processes its `age` column to ensure consistent numeric values. First, it trims any leading or trailing whitespace from the `age` column using `str_trim`. Next, it replaces the label `"90_and_over"` with `"90"` to standardise the representation of ages 90 and above. Then, it converts the `age` column to numeric values using `as.numeric`, while suppressing any warnings that might arise from non-numeric entries (e.g., empty strings or invalid values). Finally, the function filters out any rows where the `age` could not be converted to a numeric value (resulting in `NA`), ensuring only valid numeric ages remain in the dataframe. The cleaned dataframe is then returned as the output. This function ensures uniformity and removing invalid entries.

```{r}
clean_age_column <- function(df) {
  df %>%
    mutate(
      age = str_trim(age),  # Trim whitespace
      age = if_else(age == "90_and_over", "90", age),  # Replace label
      age = suppressWarnings(as.numeric(age))  # Convert safely
    ) %>%
    filter(!is.na(age))  # Remove rows that still couldn't be converted
}
```

#### Step 1: Load data

First, we will read the population data from year 2020 to 2024 of the rds file. Thereafter, we will apply `clean_age_column()` function that was defined in step 0 to the loaded dataframe.

```{r}
pop20 <- read_rds("data/rds/popdata/refined/popdata20_c.rds") %>% 
  clean_age_column()
pop21 <- read_rds("data/rds/popdata/refined/popdata21_c.rds") %>% 
  clean_age_column()
pop22 <- read_rds("data/rds/popdata/refined/popdata22_c.rds") %>% 
  clean_age_column()
pop23 <- read_rds("data/rds/popdata/refined/popdata23_c.rds") %>% 
  clean_age_column()
pop24 <- read_rds("data/rds/popdata/refined/popdata24_c.rds") %>% 
  clean_age_column()
```

#### Step 2: Compute survival rates for each year-to-year transition

This code defines a function called `compute_survival_rate` that calculates survival rates between two consecutive time periods (e.g., years) for a population dataset. The function takes two dataframes (`df1` and `df2`) as inputs, representing population data from two different time points (e.g., 2020 and 2021).

First, the function filters `df1` to include only individuals aged **59 to 90**, as survival analysis is often focused on older populations. It then increments the age by 1 (using `mutate(age = age + 1)`) to align the ages with the next time period (`df2`). Next, it performs an **inner join** between the modified `df1` and `df2` using matching columns (`age`, `sex`, `pa` \[possibly a region code\], and `sz`. The join suffixes (`_prev` and `_next`) distinguish between the population counts from the two time periods.

After joining, the function computes the **survival rate** (`rate`) by dividing the population in the later period (`pop_next`) by the population in the earlier period (`pop_prev`). Finally, it selects and returns only the relevant columns (`pa`, `sz`, `age`, `sex`, `rate`, `pop_prev`, `pop_next`) for further analysis.

In summary, this function helps estimate how many people from an initial cohort (in `df1`) survived into the next period (in `df2`) by age, sex, and other groupings, providing key insights for demographic or actuarial studies.

```{r}
compute_survival_rate <- function(df1, df2) {
  df1 %>%
    filter(age >= 55 & age < 91) %>%
    mutate(age = age + 1) %>%
    inner_join(df2, 
               by = c("age", "sex", "pa", "sz"),
               suffix = c("_prev", "_next")) %>%
    mutate(rate = pop_next / pop_prev) %>%
    select(pa, sz, age, sex, rate, pop_prev, pop_next)
}
```

```{r}
rates_2020_2021 <- compute_survival_rate(pop20, pop21)
rates_2021_2022 <- compute_survival_rate(pop21, pop22)
rates_2022_2023 <- compute_survival_rate(pop22, pop23)
rates_2023_2024 <- compute_survival_rate(pop23, pop24)
```

However, upon closer inspection of the df `rates_2020_2021`, we noticed the column `rate` consists of `Inf` rate. Upon closer examination of the columns `pop_past` and \`pop_next\`, there is an increase of population which is not logical. This predicament is consistent throughout the remaining 3 dataframes too. Therefore, step 2 needs to be refined in addressing this issue.

```{r}
rates_2020_2021[is.infinite(rates_2020_2021$rate), ]
```

#### Step 2: Refined Computing Survival Rates

The function processes survival rate data through four sequential steps, now correctly using dplyr's grouping mechanism. First, it groups the data by both age and sex, which is crucial because survival rates likely vary significantly across these demographics. Within these groups, it performs three key operations: (1) capping any rates above 1.0 at 1.0 (assuming these represent survival probabilities that shouldn't exceed 100%), (2) replacing infinite values (which occur when dividing by zero) with the maximum finite rate found in the data, and (3) imputing missing values (NaN) with the median rate for that specific age-sex group. After these grouped operations, it ungroups the data and performs a final safety check, replacing any remaining missing values with 1.0 (a neutral value indicating no change).

```{r}
clean_age_column <- function(df) {
  df %>%
    mutate(
      age = str_trim(age),  # Trim whitespace
      age = if_else(age == "90_and_over", "90", age),  # Replace label
      age = suppressWarnings(as.numeric(age))  # Convert safely
    ) %>%
    filter(!is.na(age))  # Remove rows that still couldn't be converted
}
pop20 <- read_rds("data/rds/popdata/refined/popdata20_c.rds") %>% 
  clean_age_column()
pop21 <- read_rds("data/rds/popdata/refined/popdata21_c.rds") %>% 
  clean_age_column()
pop22 <- read_rds("data/rds/popdata/refined/popdata22_c.rds") %>% 
  clean_age_column()
pop23 <- read_rds("data/rds/popdata/refined/popdata23_c.rds") %>% 
  clean_age_column()
pop24 <- read_rds("data/rds/popdata/refined/popdata24_c.rds") %>% 
  clean_age_column()
```

```{r}
compute_survival_rate <- function(df1, df2) {
  df1 %>%
    filter(age >= 55 & age < 91) %>%
    mutate(age = age + 1) %>%
    inner_join(df2, 
               by = c("age", "sex", "pa", "sz"),
               suffix = c("_prev", "_next")) %>%
    mutate(rate = pop_next / pop_prev) %>%
    select(pa, sz, age, sex, rate, pop_prev, pop_next)
}
```

The `clean_rates` function serves as a natural post-processing step for the output of `compute_survival_rate`, addressing several statistical and data quality considerations inherent to survival rate calculations. The compute_survival_rate function generates initial survival rate estimates by computing population ratios between two time periods (pop_next/pop_prev) for specific demographic groups (defined by age, sex, pa, and sz). This calculation can produce three types of problematic outputs that clean_rates systematically addresses: (1) undefined values (infinities) resulting from zero denominators, (2) rates outside the valid \[0,1\] range for probabilities, and (3) missing values from the inner join operation or zero populations.

The cleaning process employs statistically appropriate methods: replacement of infinite values with NAs followed by median imputation within age-sex strata maintains the grouped structure used in the original calculation. The bounding of values preserves the probabilistic interpretation of survival rates. The two functions together implement a complete analytical workflow where compute_survival_rate performs the demographic-specific calculation and `clean_rates` ensures the results meet necessary statistical assumptions for subsequent analysis. This separation of concerns between calculation and validation follows established data processing paradigms, with each function handling a distinct phase of the data transformation pipeline.

```{r}
clean_rates <- function(df, rate_type = "survival") {
  df %>%
    group_by(age, sex) %>%
    mutate(
      # Handle infinites first - replace with NA for proper imputation
      rate = ifelse(is.infinite(rate), NA, rate),
      
      # Detect illogical population scenario: pop_next > pop_prev
      # This suggests population growth rather than mortality/migration
      pop_growth_detected = ifelse(exists("pop_next") && exists("pop_prev"), 
                                  pop_next > pop_prev, FALSE),
      
      # If population growth detected, impute rate based on rate_type
      rate = case_when(
        pop_growth_detected & rate_type == "survival" ~ 1.0,  # Perfect survival
        pop_growth_detected & rate_type == "probability" ~ 0.0,  # Zero probability of death
        pop_growth_detected & rate_type == "hazard" ~ 0.0,  # Zero hazard rate
        pop_growth_detected ~ NA_real_,  # Set to NA for other rate types
        TRUE ~ rate  # Keep original rate if no population growth issue
      ),
      
      # Cap rates only if they represent probabilities
      rate = if(rate_type == "survival" || rate_type == "probability") {
        pmin(pmax(rate, 0), 1)  # Bound between 0 and 1
      } else {
        pmax(rate, 0)  # Only ensure non-negative for hazard rates
      },
      
      # Impute missing values with group median
      rate = ifelse(is.na(rate), median(rate, na.rm = TRUE), rate)
    ) %>%
    ungroup() %>%
    # Final check - if entire groups had no valid data
    mutate(
      rate = ifelse(is.na(rate), 
                   if(rate_type == "survival") 1.0 else 0.0, 
                   rate)
    ) %>%
    # Clean up temporary column
    select(-pop_growth_detected)
}
```

```{r}
rates_2020_2021 <- compute_survival_rate(pop20, pop21) %>% clean_rates()
```

```{r}
rates_2021_2022 <- compute_survival_rate(pop21, pop22) %>% clean_rates()
```

```{r}
rates_2022_2023 <- compute_survival_rate(pop22, pop23) %>% clean_rates()
```

```{r}
rates_2023_2024 <- compute_survival_rate(pop23, pop24) %>% clean_rates()
```

#### Step 3: Average the survival rates

This code calculates average survival rates across multiple years by combining data from four different time periods and then computing the mean rates for each demographic and geographic group. `bind_rows()` function stacks the four datasets (rates_2020_2021, rates_2021_2022, rates_2022_2023, and rates_2023_2024) vertically into one combined dataset, essentially appending all the rows together. Next, the `group_by()` function groups the combined data by four variables: age, sex, planning area (pa), and subzone (sz), creating distinct groups for each unique combination of these characteristics. The `summarise()` function then calculates the average survival rate for each group by taking the mean of the 'rate' column, while the `na.rm = TRUE` parameter ensures that any missing values are excluded from the calculation, and `.groups = "drop"` ungroups the data after summarizing. Finally, the `distinct()` function removes any potential duplicate rows by keeping only unique combinations of age, sex, pa, and sz, while `.keep_all = TRUE` preserves all other columns in the dataset.

```{r}
avg_survival_rates <- bind_rows(
  rates_2020_2021,
  rates_2021_2022,
  rates_2022_2023,
  rates_2023_2024
) %>%
  group_by(age, sex, pa, sz) %>%
  summarise(avg_rate = mean(rate, na.rm = TRUE), .groups = "drop") %>%
  distinct(age, sex, pa, sz, .keep_all = TRUE) #remove potential duplicates 
```

```{r}
#| echo: false
#| eval: false
write_rds(avg_survival_rates, "data/rds/popdata/avg_survival_rates.rds")
```

```{r}
#| echo: false
#| eval: false
avg_survival_rates = read_rds("data/rds/popdata/avg_survival_rates.rds")
```

#### Step 4: Forecast each year 2025 to 2029

The `forecast_year` function defines a vector of required column names (age, sex, pa, sz, pop) that must be present in the input dateset. The function uses a conditional check with `all()` and `%in%` operators to verify that every required column exists in the base population dataset. If any columns are missing, the function immediately terminates execution using `stop()` and provides a detailed error message that specifically lists which columns are absent, calculated using `setdiff()` to find the difference between required and available columns.

Next, the function creates a cleaned version of the base population data by first converting the population column to numeric format using `as.numeric()`. It then removes any rows where the population value is NA (missing), as these would create errors in subsequent calculations. The `select(all_of(required_cols))` ensures that only the necessary columns are retained, reducing memory usage and eliminating potential conflicts from extraneous variables.

In part 3, the aggregation step addresses the possibility of duplicate records or multiple entries for the same demographic-geographic combination. By grouping the data by age, sex, planning area, and subzone, then summing the population values, the function ensures that each unique combination has a single, consolidated population count. This is particularly important in demographic data where the same group might appear multiple times due to data collection methods or administrative boundaries. The `.groups = "drop"` parameter automatically ungroups the data after summarization, preventing issues in subsequent operations.

In part 4 of the code chunk, for individuals aged 55-88, the function implements a cohort-component method where each age group advances by one year (age + 1), simulating the passage of time. The function then performs a left join with the survival rates dataset to match each demographic-geographic group with their appropriate survival rate. This join operation is critical because survival rates typically vary by age, sex, and location due to factors like healthcare access, socioeconomic conditions, and environmental factors. When survival rates are missing for specific groups, the function implements a robust fallback mechanism using `coalesce()` combined with the median survival rate from the entire rates dataset. This approach prevents data loss while providing a reasonable estimate based on the overall population's survival characteristics. The population projection is then calculated by multiplying the current population by the survival rate, effectively reducing each cohort based on expected mortality.

In part 5, The treatment of individuals aged 89 and above reflects demographic modeling conventions where very elderly populations are often grouped together due to small sample sizes and similar demographic characteristics. The function consolidates all individuals aged 89 and above into a single age category (90), recognizing that survival patterns become more uniform at advanced ages. This approach uses survival rates specific to age 90, and when these rates are unavailable, it applies the median survival rate specifically calculated from age-90 data rather than the entire dataset, providing a more age-appropriate estimate.

In part 6, the function combines the forecasted populations from both the main aging process (ages 60-89) and the elderly category (90+) using `bind_rows()`, creating a comprehensive forecast dataset. It adds the forecast year as a new column, enabling tracking of temporal changes across multiple projection periods. The final aggregation step groups by all relevant variables including the year and sums any duplicate combinations that might have emerged from the separate processing of different age ranges, ensuring data consistency and preventing double-counting.

In the second code chunk, the forecasting loop implements a cohort-component projection method where each year's forecast becomes the foundation for the next year's projection. This approach captures the cumulative effects of demographic change over time, as population structure evolves with each iteration. The loop begins with 2024 population data (`pop24`) as the baseline, representing the most recent observed population distribution. The `tryCatch()` mechanism provides comprehensive error handling that prevents the entire forecasting process from failing due to issues with individual years. When errors occur, the system captures detailed debugging information including the data structure of the current population using `str()`, which reveals variable types, dimensions, and sample values. Each successful forecast year produces a new dataset that replaces the current population for the next iteration, creating a sequential chain of demographic projections.

```{r}
#1st code chunk
forecast_year <- function(base_pop, rates, year) {
  # 1. Validate and clean input data
  required_cols <- c("age", "sex", "pa", "sz", "pop")
  if(!all(required_cols %in% names(base_pop))) {
    stop(paste("Missing required columns:", 
              setdiff(required_cols, names(base_pop))))
  }
  
  # 2. Convert pop to numeric safely
  base_pop_clean <- base_pop %>%
    mutate(pop = as.numeric(pop)) %>%
    filter(!is.na(pop)) %>%  # Remove NA cases
    select(all_of(required_cols))
  
  # 3. Summarize with careful NA handling
  base_pop_agg <- base_pop_clean %>%
    group_by(age, sex, pa, sz) %>%
    summarise(pop = sum(pop, na.rm = TRUE), .groups = "drop")
  
  # 4. Forecast for ages 59-88
  next_pop <- base_pop_agg %>%
    filter(between(age, 55, 88)) %>%
    mutate(age = age + 1) %>%
    left_join(
      rates %>% select(age, sex, pa, sz, avg_rate), 
      by = c("age", "sex", "pa", "sz")
    ) %>%
    mutate(
      avg_rate = coalesce(avg_rate, median(rates$avg_rate, na.rm = TRUE)),
      pop = pop * avg_rate
    ) %>%
    select(all_of(required_cols))
  
  # 5. Handle age 89+
  pop_90 <- base_pop_agg %>%
    filter(age >= 89) %>%
    mutate(age = 90) %>%
    left_join(
      rates %>% filter(age == 90) %>% select(sex, pa, sz, avg_rate),
      by = c("sex", "pa", "sz")
    ) %>%
    mutate(
      avg_rate = coalesce(avg_rate, median(rates$avg_rate[rates$age == 90], na.rm = TRUE)),
      pop = pop * avg_rate
    ) %>%
    select(all_of(required_cols))
  
  # 6. Combine results
  bind_rows(next_pop, pop_90) %>%
    mutate(year = year) %>%
    group_by(year, age, sex, pa, sz) %>%
    summarise(pop = sum(pop, na.rm = TRUE), .groups = "drop")
}

#2nd code chunk
# Run forecasting with error handling
forecast_list <- list()
current_pop <- pop24 %>% 
  select(age, sex, pa, sz, pop) %>%
  mutate(pop = as.numeric(pop))

for (y in 2025:2029) {
  tryCatch({
    forecast <- forecast_year(current_pop, avg_survival_rates, y)
    forecast_list[[as.character(y)]] <- forecast
    current_pop <- forecast %>% select(age, sex, pa, sz, pop)
    message("Successfully forecasted year ", y)
  }, error = function(e) {
    message("Error forecasting year ", y, ": ", e$message)
    # Debugging info:
    message("Current population data structure:")
    print(str(current_pop))
    message("Sample of problematic data:")
    print(head(current_pop[current_pop$pop != as.numeric(current_pop$pop), ]))
  })
}
```

```{r}
#| echo: false
#| eval: false
write_rds(forecast_year, "data/rds/popdata/forecast_year.rds")
```

```{r}
#| echo: false
#| eval: false
forecast_year = read_rds("data/rds/popdata/forecast_year.rds")
```

#### Step 5: Loop over the years

This code creates an iterative population forecasting system that projects demographic changes from 2025 to 2029. An empty list called `forecast_list` is initialized to store the forecast results for each year. The variable `base_pop` is set to `pop24`, which contains the 2024 population data used as the starting point. The for loop iterates through years 2025 to 2029, calling the `forecast_year()` function for each year with the current base population, survival rates, and target year as inputs. Each forecast result is stored in `forecast_list` using the year as the key. After each forecast, `base_pop` is updated with the projected population data from the current year, selecting only the essential columns (pa, sz, age, sex, pop). This creates a sequential chain where each year's forecast becomes the input for the next year. This iterative approach captures cumulative demographic effects, as mortality and aging impacts from each year carry forward to subsequent projections. The final result is five separate population forecasts stored in `forecast_list`, representing projected demographics for each year from 2025-2029.

```{r}
# List to store forecasts
forecast_list <- list()
base_pop <- pop24

for (y in 2025:2029) {
  next_forecast <- forecast_year(base_pop, avg_survival_rates, y)
  forecast_list[[as.character(y)]] <- next_forecast
  base_pop <- next_forecast %>% select(pa, sz, age, sex,  pop)
}
```

### Final Output of Population Forecast

This code combines all the individual yearly forecasts into a single comprehensive dataset and ensures population values are whole numbers. The `bind_rows(forecast_list)` function takes all the separate forecast datasets stored in the list (one for each year from 2025-2029) and stacks them vertically into one dataframe, preserving all columns and adding all rows together. The `mutate(pop = ceiling(pop))` applies the ceiling function to round all population values up to the nearest integer, since population counts must be whole numbers and any fractional people resulting from the survival rate calculations need to be converted to realistic integer values. The ceiling function always rounds upward, so a value like 1547.3 becomes 1548, ensuring no population is lost due to rounding and maintaining conservative population estimates. The result is a single dataset containing all forecasted population data across all years, with properly formatted integer population counts ready for analysis or visualisation.

```{r}
all_forecasts <- bind_rows(forecast_list) %>%
  mutate(pop = ceiling(pop))  # Round UP to nearest integer
```

```{r}
#| echo: false
#| eval: false
write_rds(all_forecasts, "data/rds/popdata/all_forecasts.rds")
```

```{r}
#| echo: false
#| eval: false
all_forecasts = read_rds("data/rds/popdata/all_forecasts.rds")
```

```{r}
# Convert from long to wide format
pivoted_data <- all_forecasts %>%
  select(year, age, sex, pa, sz, pop) %>%  # Ensure only necessary columns
  pivot_wider(
    names_from = year,
    values_from = pop,
    names_prefix = "pop_",  # Adds "pop_" before each year
    values_fill = NA        # Fill missing combinations with NA
  ) %>%
  arrange(pa, sz, sex, age)  # Sort logically
```

```{r}
#| echo: false
#| eval: false
write_rds(pivoted_data, "data/rds/popdata/pivoted_data.rds")
```

```{r}
#| echo: false
#| eval: false
pivoted_data = read_rds("data/rds/popdata/pivoted_data.rds")
```

The data aggregation process began by filtering the pivoted dataset using `filter(age >= 60)` to include only individuals aged 60 years and above. The filtered data was then grouped by subzone using `group_by(sz)` to enable population calculations at the geographic level. For each subzone, the total elderly population was calculated using the `summarise()` function, which created new variables (`aged_2025`, `aged_2026`, `aged_2027`, `aged_2028`, `aged_2029`) by summing the population counts for both males and females across the five-year projection period from 2025 to 2029. The `sum()` function with the parameter `na.rm = TRUE` was used to handle missing values by excluding them from the calculations to ensure accurate totals. The `.groups = 'drop'` parameter was included to remove the grouping structure after summarization. This resulted in a summary dataset where each row represents a unique subzone, with columns containing the projected elderly population counts for each year from 2025 through 2029. The original age and sex variables were removed during this aggregation process as they were no longer needed, creating a streamlined dataset focused on subzone-level elderly population projections.

```{r}
# Aggregate population for ages 60 and above by subzone (combining males and females)
geri_forecast <- pivoted_data %>%
  filter(age >= 60) %>%
  group_by(sz) %>%
  summarise(
    aged_2025 = sum(pop_2025, na.rm = TRUE),
    aged_2026 = sum(pop_2026, na.rm = TRUE),
    aged_2027 = sum(pop_2027, na.rm = TRUE),
    aged_2028 = sum(pop_2028, na.rm = TRUE),
    aged_2029 = sum(pop_2029, na.rm = TRUE),
    .groups = 'drop'
  )

```

## MPSZ: Singapore's Master Plan 2019 Subzone Boundary

### Overview

In this section, two datsets will be imported namely Master Plan 2019 Subzone Boundary and Master Plan 2019 Land Use. The dataset will be cleaned and transformed thereafter hexagons will be created to facilitate the analysis in the next chapter.

### Importing Data

Using `st_read` of the \`sf\` package, the ESRI shapefile was imported and it contains 323 data entries and 15 fields. Each of the data entry consists of a multi-polygon shape, with geospatial coordinates with a geographic coordinate system (GCS) of World Geodetic System (WGS) 84. The features of GCS include using a 3D spherical model of earth with coordinates of longitude, latitude and altitude whereas PCS uses a 2D plane model with linear measurements (i.e. metres). However, the file doesn't churn the full details of the Master Plan Boundary. Therefore, the dataset was adjusted using QGIS and downloaded for this project with the assistance of Prof Kam.

```{r}
mpsz = st_read(dsn = "data/planningarea/",
               layer = "mpsz2019")
```

Using `glimpse()`, we are able to visualise the columns, column types and properties within the columns. Notably, there are numerous columns that are not needed for this analysis.

```{r}
glimpse(mpsz)
```

In the code chunk below, we use the `select()` from \`dplyr\` package to select the columns that are required for the anlaysis.

```{r}
mpsz <- mpsz %>%
  select(SUBZONE_N, geometry)
```

By employing `tmap`, we are able to create a static map visualisation of the subzones in Singapore. `tm_shape` is the base layer for the visualation which we will be using `mpsz` while `tm_polygons` renders the the subzone boundaries as filled polygon using `SUBZONE_N`.

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons("SUBZONE_N")
```

### Transforming Coordinate Reference System

As the research's focus is exploring the accessibility of care centres in Singapore, Projected Coordinate System (PCS) would be appropriate in this context as it measures the distance between the elderly' residence and the care centre. Thus, in ensuring accurate measurement, the function `st_transform` with a Coordinate Reference System (CRS) of 3414 was used (Kam, 2022) in the code chunk below.

```{r}
mpsz <- mpsz %>%
  st_transform(crs = 3414)
```

### Checking for Shared Boundaries

The below codechunk uses `st_is_valid` in assessing if there are shared boundaries in the `mpsz` data. This is important as shared boundaries can lead to inconsistent data impacting the research findings. 9 polygons with self-intersection ('Ring Self-Intersection') issues were returned.

```{r}
st_is_valid(mpsz, reason = TRUE)
```

This code chunk visualises the boundaries that are affected such as the smaller islands outside of Singapore Main Island: P. Ubin, P. Tekong, Sentosa Island.

```{r}
invalid_polygons <- mpsz[!st_is_valid(mpsz),]
plot(invalid_polygons)
```

In addressing the above point, we will use `st_buffer` of `sf` package to compute a 5-metre buffer around the data.

```{r}
mpsz <- st_buffer(mpsz, dist = 5)
```

```{r}
#| echo: false
#| eval: false
write_rds(mpsz, "data/rds/mpsz/mpsz.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz = read_rds("data/rds/mpsz/mpsz.rds")
```

### Combining MPSZ & Population Forecast

This codechunk performs a spatial join between `MPSZ` and population forecast data, `pivoted_clean` using similar column: subzone and with data cleaning to handle missing values. The first section creates cleaned versions of both datasets by adding a standardised join key - `mpsz_clean` takes the subzone geometry data and creates a `join_key` by converting the subzone names to uppercase and removing any whitespace using `str_trim()` and `str_to_upper()`. Similarly, `pivoted_clean` processes the population forecast data by creating the same type of standardised join key from the `sz` column, ensuring consistent formatting for matching between the two datasets. The main join operation uses `left_join()` to combine the subzone geometries with the population data based on the matching join keys, preserving all subzones even if they don't have corresponding population data. After joining, the temporary `join_key` column is removed using `select(-join_key)` to clean up the final dataset. The final step uses `mutate(across(where(is.numeric), ~replace_na(.x, 0)))` to identify all numeric columns in the joined dataset and replace any NA values with 0, ensuring that subzones without population forecasts display as having zero population rather than missing data.

```{r}
# Create temporary cleaned versions for joining
mpsz_clean <- mpsz %>%
  mutate(join_key = str_trim(str_to_upper(SUBZONE_N)))

pivoted_clean <- pivoted_data %>%
  mutate(join_key = str_trim(str_to_upper(sz)))

# Join and clean up, replacing NA with 0
mpsz_popforecast <- mpsz_clean %>%
  left_join(pivoted_clean, by = "join_key") %>%
  select(-join_key) %>%
  mutate(across(where(is.numeric), ~replace_na(.x, 0))) #replace NA with 0

mpsz_forecast <- mpsz %>%
  left_join(pivoted_clean, by = c("SUBZONE_N" = "sz"))
```

```{r}
#| echo: false
#| eval: false
write_rds(mpsz_popforecast, "data/rds/mpsz/mpsz_popforecast.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz_popforecast = read_rds("data/rds/mpsz/mpsz_popforecast.rds")
```

```{r}
# Create cleaned versions for matching
mpsz_clean <- mpsz %>%
  mutate(subzone_clean = str_to_upper(str_trim(str_squish(SUBZONE_N))))

geri_clean <- geri_forecast %>%
  mutate(sz_clean = str_to_upper(str_trim(str_squish(sz))))

# Perform the left join
geri_forecast <- mpsz_clean %>%
  left_join(geri_clean, by = c("subzone_clean" = "sz_clean"))
```

```{r}
#| echo: false
#| eval: false
write_rds(geri_forecast, "data/rds/popdata/geri_forecast.rds")
```

```{r}
#| echo: false
#| eval: false
geri_forecast = read_rds("data/rds/popdata/geri_forecast.rds")
```

## Master Plan 2019 Subzone Boundary (Land Use)

### Importing Data

Using `st_read`, the file is loaded with an addition of transforming to the CRS to 3414.

```{r}
mpsz_land = st_read(dsn = "data/landuse/",
               layer = "mp2019_landuse") %>%
  st_transform(crs = 3414)
```

### Checking for Shared Boundaries

Similar to the step in `MPSZ`, this function is used to check if there are any shared boundaries.

```{r}
st_is_valid(mpsz_land, reason = TRUE)
```

Using `unique()`, it returns a list of properties in the column `LU_DESC`. As the focus of the research is accessibility to care centre, all residential related areas will be factored in this research.

```{r}
unique(mpsz_land$LU_DESC)
```

```{r}
mpsz_land <- st_buffer(mpsz_land, dist = 1)
```

### Filtering to Residential Use

In the code chunk below, we use the `select()`function to pick the columns that are required for the analysis. Additionally, `filter() -ing` the column `LU_DESC` for those words containing 'residential': Regular expression was used alongside, ignoring capital letters.

```{r}
mpsz_res <- mpsz_land %>%
  select(LU_DESC, geometry) %>%
  filter(str_detect(LU_DESC, regex("residential", ignore_case = TRUE)))
```

```{r}
#| echo: false
#| eval: false
write_rds(mpsz_res, "data/rds/mpsz/mpsz_res.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz_res = read_rds("data/rds/mpsz/mpsz_res.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz_land = read_rds("data/rds/mpsz/mpsz_land.rds")
```

Using `tmap`, we are able to plot all residential areas. Noticeably, there are major regions that are filled with white spaces such as the North-West as it is the central water catchment area and militarily installations and North-East due to the presence of Paya Lebar Airbase and Seletar Airport.

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz_res) +
  tm_polygons()
```

### Sampling Grid

In measuring spatial accessibility, analytical grids of the sampling fields is used to standardise the means of measurement (Kam, 2022). These analytical grids can be comprised of equilateral triangles, squares or hexagon due to its ability to tessellate (ESRI, 2025). From the code output above, we noticed that the sampling fields in the `mpsz_res` are not equal for measurement. Therefore, it is imperative to select an appropriate analytical grid in measuring spatial accessibility. Hexagons grids is chosen over triangles and squares due to several factors. Firstly, the shape of a hexagon has a low-perimeter-to-area ratio, hence the edge effect of the grid shape reduces sampling bias. Secondly, when comparing equal area, any point inside a hexagon is closer to the centriod than any given point in an equal-area square or triangle due to the more acute angles of square and triangle versus the hexagon (Burdziej, 2018; ESRI, 2025). This is particular important in this research as we are using the centroid as a proxy in comparing accessibility to the various care centres. The centre of each hexagon is an Origin, additionally, acting as a starting point in ascertaining the shortest distance to the care centres.

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tmap_options(max.categories = n_distinct(mpsz_res$LU_DESC))  # Show all categories
tm_shape(mpsz_res) +
  tm_polygons("LU_DESC")
```

#### Step 1: Creating polygons

In the HealthierSG White Paper 2022, the Ministry of Health has indicated its intention to "expand the network \["care centres"\] to 220 by 2025". Furthermore, through the Ministry's estimation, 8 in 10 seniors will have a care centre in the vicinity of their homes (Ministry of Health, 2022). Hence, we will assume that the maximum distance to the care centres are 100 metres. Hence, in the code chunk below, `st_make_grid` from `sf` package constructed hexagonal grids encompassing the Singapore Master Plan 2019 Planning Subzone Boundary using `cellsize` that defines the radius of 100 metres and `square` to be false to generate a hexagonal grid.

```{r}
hexagon <- st_make_grid(mpsz_res,
                         cellsize = 100,
                         what = "polygon",
                         square = FALSE) %>%
  st_sf()
```

In the plot below, we are able to see mulitple hexagons overlaying the residential areas. Upon closer inspection, there are no hexagons that are cut off which translates to successfully creating hexagons of the residential areas.

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tm_shape(hexagon) +
  tm_polygons()
```

#### Step 2: Intersection of Hexagons & MPSZ_RES

The `st_intersects()` function was used to identify geometric intersections between the hexagon grid and the `mpsz_res` sf dataset, with the `sparse = FALSE` parameter ensuring that the result was returned as a dense logical matrix rather than a sparse list format. The `apply(1, any)` function was then applied row-wise to this intersection matrix to determine which hexagon cells had at least one intersection with any planning area boundary. Finally, this logical vector was used to subset the original hexagon dataset, retaining only those hexagonal cells that spatially overlapped with the study area boundaries. This spatial filtering step is crucial for focusing the accessibility analysis on areas where elderly populations actually reside. By removing hexagons that don't intersect with residential areas, the analysis excludes industrial zones, water bodies, nature reserves, airports, and other non-residential land uses where elderly care accessibility is irrelevant.

```{r}
hex_res <- hexagon[st_intersects(hexagon, mpsz_res, sparse = FALSE) %>% apply(1, any), ]
#sparse = exclude zone without any intersection
```

#### Step 3: Assigning Unique Identifier to Hexagon

In the below code, we assign each hexagon with a unique identifier using `Hxxx` for accessibility analysis and results reporting.

```{r}
hex_res$hex_id <- sprintf("H%04d", seq_len(nrow(hex_res))) %>% as.factor()
head(hex_res)
```

```{r}
#| echo: false
#| eval: false
write_rds(hex_res, "data/rds/mpsz/hex_res.rds")
```

```{r}
#| echo: false
#| eval: false
hex_res = read_rds("data/rds/mpsz/hex_res.rds")
```

#### Step 4: Calculate the Area of Each Subzone

Next, the code below will compute the area of each polygon in `mpsz`. This step is essential for population density calculations, enabling the proportional allocation of the population data to hexagon intersections based on their size within each subzone.

```{r}
mpsz <- mpsz %>%
  mutate(mpsz_area = st_area(geometry))
```

#### Step 5: Intersect Hexagons with Subzones

This step creates intersection geometries between hexagons and subzones using `st_intersection()`, resulting in polygon fragments that represent the overlapping portions of hexagons and subzone boundaries. These intersection polygons provide the geometric foundation for accurate population disaggregation, ensuring that elderly population estimates at the hexagon level reflect the actual administrative boundaries where demographic data is collected and reported.

```{r}
hex_mpsz_intersection <- st_intersection(hex_res, mpsz)
```

#### Step 6: Calculate Area of Each Intersection

The below code chunk calculates the area of each intersection polygon, representing the precise spatial extent of hexagon-subzone overlaps, which is essential for proportional population allocation.

```{r}
hex_mpsz_intersection <- hex_mpsz_intersection %>% 
  mutate(intersection_area =
           st_area(geometry))
```

```{r}
#| echo: false
#| eval: false
write_rds(hex_mpsz_intersection, "data/rds/mpsz/hex_mpsz_intersection.rds")
```

```{r}
#| echo: false
#| eval: false
hex_mpsz_intersection = read_rds("data/rds/mpsz/hex_mpsz_intersection.rds")
```

```{r}
#| echo: false
#| eval: false
mpsz_res = read_rds("data/rds/mpsz/mpsz_res.rds")
```

#### Step 7: Data Integration

The below code chunk joins two datasets by matching subzone names. It starts with `hex_mpsz_intersection` and creates a standardized join key by cleaning the `SUBZONE_N` column (removing whitespace and converting to uppercase). It then performs a left join with `geri_forecast`, first removing the spatial geometry from `geri_forecast` since regular joins can't work with spatial objects. The `geri_forecast` dataset gets the same join key treatment using its sz column. After matching records with identical join keys, the temporary join key column is removed. The result combines the spatial geometry from `hex_mpsz_intersection` with the attribute data from `geri_forecast` based on cleaned subzone name matches.

```{r}
joined_data <- hex_mpsz_intersection %>%
  mutate(join_key = str_to_upper(str_trim(str_squish(SUBZONE_N)))) %>%
  left_join(
    geri_forecast %>% 
      st_drop_geometry() %>%  # Add this line to remove geometry
      mutate(join_key = str_to_upper(str_trim(str_squish(sz)))),
    by = "join_key"
  ) %>%
  select(-join_key)
```

```{r}
#| echo: false
#| eval: false
write_rds(joined_data, "data/rds/mpsz/joined_data.rds")
```

```{r}
#| echo: false
#| eval: false
joined_data = read_rds("data/rds/mpsz/joined_data.rds")
```

#### Step 8: Area Weighted Population Estimation in Each Hexagon

In each of the code chunk below, it estimates the elderly population within each hexagon intersection using a proportional allocation formula: `(intersection_area / mpsz_area) * aged_population`, which distributes subzone-level elderly population projections to hexagon fragments based on their relative area coverage. This population disaggregation methodology transforms coarse administrative-level demographic data into fine-grained spatial estimates suitable for accessibility analysis. Each hexagon will ultimately represent a specific elderly population that needs access to care centers, enabling the research to identify areas with high demand but potentially poor accessibility. The temporal dimension (2025-2029 projections) allows for forward-looking accessibility planning, helping policymakers anticipate where new care centers might be needed as Singapore's elderly population grows and concentrates in different areas. This approach ensures that accessibility analysis reflects realistic population distributions rather than treating each hexagon as having equal elderly population, providing more accurate insights for healthcare infrastructure planning.

```{r}
geriesti_hex_2025 <-
  joined_data %>%
  mutate(pop_2025 = as.numeric(
    intersection_area / mpsz_area * 
      aged_2025))
```

```{r}
geriesti_hex_2026 <-
  joined_data %>%
  mutate(pop_2026 = as.numeric(
    intersection_area / mpsz_area * 
      aged_2026))
```

```{r}
geriesti_hex_2027 <-
  joined_data %>%
  mutate(pop_2027 = as.numeric(
    intersection_area / mpsz_area * 
      aged_2027))
```

```{r}
geriesti_hex_2028 <-
  joined_data %>%
  mutate(pop_2028 = as.numeric(
    intersection_area / mpsz_area * 
      aged_2028))
```

```{r}
geriesti_hex_2029 <-
  joined_data %>%
  mutate(pop_2029 = as.numeric(
    intersection_area / mpsz_area * 
      aged_2029))
```

```{r}
#| echo: false
#| eval: false
write_rds(geriesti_hex_2025, "data/rds/mpsz/geriesti_hex_2025.rds")
write_rds(geriesti_hex_2026, "data/rds/mpsz/geriesti_hex_2026.rds")
write_rds(geriesti_hex_2027, "data/rds/mpsz/geriesti_hex_2027.rds")
write_rds(geriesti_hex_2028, "data/rds/mpsz/geriesti_hex_2028.rds")
write_rds(geriesti_hex_2029, "data/rds/mpsz/geriesti_hex_2029.rds")
```

```{r}
#| echo: false
#| eval: false
geriesti_hex_2025 = read_rds("data/rds/mpsz/geriesti_hex_2025.rds")
geriesti_hex_2026 = read_rds("data/rds/mpsz/geriesti_hex_2026.rds")
geriesti_hex_2027 = read_rds("data/rds/mpsz/geriesti_hex_2027.rds")
geriesti_hex_2028 = read_rds("data/rds/mpsz/geriesti_hex_2028.rds")
geriesti_hex_2029 = read_rds("data/rds/mpsz/geriesti_hex_2029.rds")
```

*hexagon_final* now includes a new column *estimated_population*, which is the area-weighted population estimate for each hexagon.e

## Care Centres

### Overview

In this section, multiple types of care centres datasets will be imported, cleaned and transformed. Care Centres entails Active Ageing Centre (`aac`), counselling centres for seniors (`counselling`), senior day care (`daycare`), dementia day care (`dementia`), hospice day care (`hospice`), maintenance centres (`maintenance`), Nursing Home Respites (`nhrespite`), nursing-based centres (`nursing`) and rehabilitation centres (`rehab`). The services for each type of centre is provided below. All of these centres will open during office hours and thereafter, seniors are able to head home.

| Care Centres | Services |
|----|----|
| [Active Ageing Centres](https://supportgowhere.life.gov.sg/services/SVC-AACAAACHASACS/active-ageing-centres-aac) | A go-to point for seniors to build strong social connections, take part in recreational activities, access community health services, and contribute to the community. |
| [Counselling (Seniors)](https://supportgowhere.life.gov.sg/services/ceeJSWE0/counselling-seniors) | Provides counselling to older persons aged 50 and above and/or their caregivers, to improve their mental well-being and resilience to cope with life’s challenges. |
| [Senior Day Care](https://supportgowhere.life.gov.sg/services/SVC-SDC/senior-day-care) | Provides a full day programme for seniors who are suitable for centre-based care setting. Supports seniors to maintain and/or improve their general, physical and social well-being. |
| [Dementia Day Care](https://supportgowhere.life.gov.sg/services/SVC-DDC/dementia-day-care) | Mind stimulating activities, exercises and personal care for seniors with dementia. |
| [Day Hospice Care](https://supportgowhere.life.gov.sg/services/SVC-PDC/day-hospice-care) | A safe and supportive environment for patients with life-limiting illnesses and their caregivers. |
| [Maintenance Day Care Centre](https://supportgowhere.life.gov.sg/services/SVC-MDCC/maintenance-day-care-centre) | Full day programme of engaging activities, exercises and personal care for seniors. |
| [Nursing Home Respite Care](https://supportgowhere.life.gov.sg/services/SVC-NHRC/nursing-home-respite-care) | A stay-in, subsidised care option with healthcare services for seniors with high care needs if caregivers need a short break on an ad hoc basis during the day. |
| [Centre-based Nursing](https://supportgowhere.life.gov.sg/services/SVC-CBN/centre-based-nursing) | Basic nursing care for seniors at a centre near their homes. |
| [Community Rehabilitation Centre](https://supportgowhere.life.gov.sg/services/SVC-CRC/community-rehabilitation-centre) | Therapy sessions to help seniors regain their ability to carry out daily activities. |

### Importing Data

```{r}
aac <- read_csv("data/carecentre/activeageingcentre.csv")
```

```{r}
counselling <- read_csv("data/carecentre/counselling.csv")
```

```{r}
daycare <- read_csv("data/carecentre/daycare.csv")
```

```{r}
dementia <- read_csv("data/carecentre/dementiadaycare.csv")
```

```{r}
hospice <- read_csv("data/carecentre/dayhospice.csv")
```

```{r}
maintenance <- read_csv("data/carecentre/maintenancedaycare.csv")
```

```{r}
nhrespite <- read_csv("data/carecentre/nhrespite.csv")
```

```{r}
nursing <- read_csv("data/carecentre/centrebasednursing.csv")
```

```{r}
rehab <- read_csv("data/carecentre/communityrehabcentre.csv")
```

```{r}
#| echo: false
#| eval: false
write_rds(aac, "data/rds/carecentre/original/aac.rds")
write_rds(counselling, "data/rds/carecentre/original/counselling.rds")
write_rds(daycare, "data/rds/carecentre/original/daycare.rds")
write_rds(dementia, "data/rds/carecentre/original/dementia.rds")
write_rds(hospice, "data/rds/carecentre/original/hospice.rds")
write_rds(maintenance, "data/rds/carecentre/original/maintenance.rds")
write_rds(nhrespite, "data/rds/carecentre/original/nhrespite.rds")
write_rds(nursing, "data/rds/carecentre/original/nursing.rds")
write_rds(rehab, "data/rds/carecentre/original/rehab.rds")
```

```{r}
#| echo: false
#| eval: false

aac = read_rds("data/rds/carecentre/original/aac.rds")
counselling = read_rds("data/rds/carecentre/original/counselling.rds")
daycare = read_rds("data/rds/carecentre/original/daycare.rds")
dementia = read_rds("data/rds/carecentre/original/dementia.rds")
hospice = read_rds("data/rds/carecentre/original/hospice.rds")
maintenance = read_rds("data/rds/carecentre/original/maintenance.rds")
nhrespite = read_rds("data/rds/carecentre/original/nhrespite.rds")
nursing = read_rds("data/rds/carecentre/original/nursing.rds")
rehab = read_rds("data/rds/carecentre/original/rehab.rds")
```

### Cursory View

Using the glimpse() function, we are able to see that various rows in each data set while sharing the same number of columns. Columns "web-scraper-order" and "web-scraper-start-url" are redundant, thus, will be removed. Additionally, the address includes the postal code and it will seperated from the main street name and block number to facilitate the geospatial mapping thereafter.

```{r}
glimpse(aac)
```

### Deleting Unwanted Codes

The following R code is used to remove the columns "web-scraper-order" and "web-scraper-start-url" from multiple datasets: The select() function from the dplyr package is used to select or remove columns from a data frame.

```{r}
aac <- aac %>% select(-"web-scraper-order", -"web-scraper-start-url")
counselling <- counselling %>% select(-"web-scraper-order", -"web-scraper-start-url")
daycare <- daycare %>% select(-"web-scraper-order", -"web-scraper-start-url")
dementia <- dementia %>% select(-"web-scraper-order", -"web-scraper-start-url")
hospice <- hospice %>% select(-"web-scraper-order", -"web-scraper-start-url")
maintenance <- maintenance %>% select(-"web-scraper-order", -"web-scraper-start-url")
nhrespite <- nhrespite %>% select(-"web-scraper-order", -"web-scraper-start-url")
nursing <- nursing %>% select(-"web-scraper-order", -"web-scraper-start-url")
rehab <- rehab %>% select(-"web-scraper-order", -"web-scraper-start-url")
```

After removing the two columns, each data set has two columns, namely name and address only.

```{r}
glimpse(aac)
```

### Checking for Missing Values

To check for missing or null values in the name and address columns of each dataset, the code uses the summarise() function from the dplyr package. The summarise() function computes summary statistics for the specified columns, which in this case are name and address. The across() function is used to apply the sum(is.na(.)) operation to both columns simultaneously, counting the number of missing (NA) values in each column.

The is.na() function checks whether each value in the name and address columns is missing or null, returning TRUE for missing values and FALSE for non-missing values. The sum() function then counts the number of TRUE values, which corresponds to the number of missing values in each column. This process is applied to each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab). In conclusion it is able to identify the number of missing values in the name and address columns across all datasets, which helps assess the completeness of the data and highlights any issues that may require cleaning or imputation before further analysis. It returns 0 missing values.

```{r}
# Checking for missing or null values in 'name' and 'address' columns
aac_missing <- aac %>% summarise(across(c(name, address), ~sum(is.na(.))))
counselling_missing <- counselling %>% summarise(across(c(name, address), ~sum(is.na(.))))
daycare_missing <- daycare %>% summarise(across(c(name, address), ~sum(is.na(.))))
dementia_missing <- dementia %>% summarise(across(c(name, address), ~sum(is.na(.))))
hospice_missing <- hospice %>% summarise(across(c(name, address), ~sum(is.na(.))))
maintenance_missing <- maintenance %>% summarise(across(c(name, address), ~sum(is.na(.))))
nhrespite_missing <- nhrespite %>% summarise(across(c(name, address), ~sum(is.na(.))))
nursing_missing <- nursing %>% summarise(across(c(name, address), ~sum(is.na(.))))
rehab_missing <- rehab %>% summarise(across(c(name, address), ~sum(is.na(.))))

```

### Duplicate Check

The code provided checks for duplicate rows in each dataset (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) by grouping the dataset by all columns using group_by_all(). It then filters out the rows that have duplicate combinations of values across all columns using filter(n() \> 1). The n() function counts the number of occurrences for each combination of values, and filter(n() \> 1) keeps only the rows that appear more than once (i.e., duplicates).

For each dataset, the nrow() function is used to check if there are any rows returned after filtering for duplicates. If there are duplicates (i.e., the number of rows is greater than zero), the dataset with the duplicate rows is returned. However, if no duplicates are found (i.e., nrow() equals zero), the code returns 0 to indicate that there are no duplicates in that dataset.

Thus, the code either returns the rows with duplicate values or 0 if no duplicates are present, providing an indication of whether duplicate entries exist in each dataset.

```{r}
# Check for duplicates in 'aac'
aac_duplicate <- aac %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'counselling'
counselling_duplicate <- counselling %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'daycare'
daycare_duplicate <- daycare %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'dementia'
dementia_duplicate <- dementia %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'hospice'
hospice_duplicate <- hospice %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'maintenance'
maintenance_duplicate <- maintenance %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'nhrespite'
nhrespite_duplicate <- nhrespite %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'nursing'
nursing_duplicate <- nursing %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

# Check for duplicates in 'rehab'
rehab_duplicate <- rehab %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  ungroup()

```

### Separating postal code from address

The code uses the mutate() function to extract the postal code (last 6 digits) from the address column of the individual dataset and store it in a new column called postal_code. The postal code is then removed from the address column.

```{r}
# Active Ageing Centre
aac <-mutate(aac,
    postal_code = str_extract(address, "[0-9]{6}$"),  # Extract postal code
    address = str_remove(address, "[,]?\\s*[0-9]{6}$")  # Remove postal code from address
  )
```

```{r}
# Counselling
counselling <- mutate(counselling,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Daycare
daycare <- mutate(daycare,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Dementia
dementia <- mutate(dementia,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Day Hospice
hospice <- mutate(hospice,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Maintenance Daycare
maintenance <- mutate(maintenance,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# NH Respite
nhrespite <- mutate(nhrespite,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Centre Based Nursing
nursing <- mutate(nursing,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Community Rehab Centre
rehab <- mutate(rehab,
  postal_code = str_extract(address, "[0-9]{6}$"),
  address = str_remove(address, "[,]?\\s*[0-9]{6}$")
)
```

```{r}
# Checking for missing or null values in 'name' and 'address' columns
aac_missing <- aac %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
counselling_missing <- counselling %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
daycare_missing <- daycare %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
dementia_missing <- dementia %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
hospice_missing <- hospice %>% summarise(across(c(name, address, postal_code),~sum(is.na(.))))
maintenance_missing <- maintenance %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
nhrespite_missing <- nhrespite %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
nursing_missing <- nursing %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
rehab_missing <- rehab %>% summarise(across(c(name, address, postal_code), ~sum(is.na(.))))
```

### Labelling Dataset

The below code chunk adds a column and naming it as "label" in relation to the name of the dataset hence we are able to identify the type of services provided by the care centres.

```{r}
aac <- aac %>%
  mutate(label = "aac")
```

```{r}
counselling <- counselling %>%
  mutate(label = "counselling")
```

```{r}
daycare <- daycare %>%
  mutate(label = "daycare")
```

```{r}
dementia <- dementia %>%
  mutate(label = "dementia")
```

```{r}
hospice <- hospice %>%
  mutate(label = "hospice")
```

```{r}
maintenance <- maintenance %>%
  mutate(label = "maintenance")
```

```{r}
nhrespite <- nhrespite %>%
  mutate(label = "nhrespite")
```

```{r}
nursing <- nursing %>%
  mutate(label = "nursing")
```

```{r}
rehab <- rehab %>%
  mutate(label = "rehab")
```

```{r}
#| echo: false
#| eval: false
write_rds(aac, "data/rds/carecentre/refined/aac.rds")
write_rds(counselling, "data/rds/carecentre/refined/counselling.rds")
write_rds(daycare, "data/rds/carecentre/refined/daycare.rds")
write_rds(dementia, "data/rds/carecentre/refined/dementia.rds")
write_rds(hospice, "data/rds/carecentre/refined/hospice.rds")
write_rds(maintenance, "data/rds/carecentre/refined/maintenance.rds")
write_rds(nhrespite, "data/rds/carecentre/refined/nhrespite.rds")
write_rds(nursing, "data/rds/carecentre/refined/nursing.rds")
write_rds(rehab, "data/rds/carecentre/refined/rehab.rds")
```

```{r}
#| echo: false
#| eval: false

aac = read_rds("data/rds/carecentre/refined/aac.rds")
counselling = read_rds("data/rds/carecentre/refined/counselling.rds")
daycare = read_rds("data/rds/carecentre/refined/daycare.rds")
dementia = read_rds("data/rds/carecentre/refined/dementia.rds")
hospice = read_rds("data/rds/carecentre/refined/hospice.rds")
maintenance = read_rds("data/rds/carecentre/refined/maintenance.rds")
nhrespite = read_rds("data/rds/carecentre/refined/nhrespite.rds")
nursing = read_rds("data/rds/carecentre/refined/nursing.rds")
rehab = read_rds("data/rds/carecentre/refined/rehab.rds")
```

### Append all Care Centres into One Dataset

The code combines multiple datasets (aac, counselling, daycare, dementia, hospice, maintenance, nhrespite, nursing, and rehab) into a single dataset named `cc_data` using the `bind_rows()` function. This function appends the rows of each dataset, stacking them vertically, to create one consolidated dataset. The resulting `cc_data` will contain all the rows from the individual datasets.

```{r}
cc_data <- bind_rows(
  aac, 
  counselling,
  daycare,
  dementia,
  hospice,
  maintenance,
  nhrespite,
  nursing,
  rehab,
)
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_data, "data/rds/carecentre/refined/cc_data.rds")
```

```{r}
#| echo: false
#| eval: false
cc_data = read_rds("data/rds/carecentre/refined/cc_data.rds")
```

### Transforming Categorical Data to Binary Indicator

This code transforms the dataset `cc_data` from a long format to a wide format by pivoting on the categorical values in the `label` column, effectively converting them into binary indicator columns. The process begins by removing the `address` column using `select(-address)` to exclude it from the transformation. Next, a new column called `present` is created using `mutate(present = 1)`, where every row is assigned a value of 1 to indicate the presence of a label. The key reshaping operation is performed using `pivot_wider()`, which spreads the unique values from the `label` column into separate columns. The `names_from = label` argument specifies that the new column names should be derived from the distinct categories in `label`, while `values_from = present` fills these new columns with the corresponding 1s from the `present` column. Any missing combinations (where a particular label does not appear for a given record) are automatically filled with 0s due to the `values_fill = list(0)` argument. The final output, stored in `pivoted_cc_data`, is a wider dataframe where each original row now has binary flags (1 or 0) indicating the presence or absence of each label category, making it suitable for analyses that require a one-hot encoded or dummy variable representation of categorical data.

```{r}
pivoted_cc_data <- cc_data %>%
  select(-address) %>%

  mutate(present = 1) %>%  # Create a column to indicate presence (1)

  pivot_wider(

    names_from = label,    # Pivot based on the 'label' column

    values_from = present,
    values_fill = list(0)# Use the 'present' column for the values
    
  )
```

### Checking for Duplicates

```{r}
duplicates <- pivoted_cc_data %>%
  
  group_by(postal_code) %>%
  filter(n() > 1) %>%
  arrange(postal_code)
```

```{r}
# Method 1: Unnest all list columns at once
service_columns <- c("aac", "counselling", "daycare", "dementia", "hospice", 
                    "maintenance", "nhrespite", "nursing", "rehab")

# Find which service columns are actually lists
list_service_cols <- names(duplicates)[sapply(duplicates, is.list) & names(duplicates) %in% service_columns]

cat("List columns to unnest:", paste(list_service_cols, collapse = ", "), "\n")

# Unnest the list columns
if (length(list_service_cols) > 0) {
  duplicates_unnested <- duplicates %>%
    unnest(cols = all_of(list_service_cols))
} else {
  duplicates_unnested <- duplicates
}
```

```{r}
#Function to merge center names
merge_center_names <- function(names) {
  if (length(names) == 1) return(names)
  
  names <- unique(names)
  if (length(names) == 1) return(names)
  
  # Extract location pattern
  location_pattern <- "\\(([^)]+)\\)$"
  locations <- str_extract(names, location_pattern)
  common_location <- locations[!is.na(locations)][1]
  
  # Remove location from names
  main_names <- str_remove(names, location_pattern) %>% str_trim()
  
  # Find common prefix
  words_matrix <- str_split(main_names, "\\s+", simplify = TRUE)
  common_prefix_words <- c()
  
  if (nrow(words_matrix) > 1) {
    max_cols <- ncol(words_matrix)
    for (i in 1:max_cols) {
      if (i <= ncol(words_matrix)) {
        current_column <- words_matrix[, i]
        current_column <- current_column[current_column != ""]
        
        if (length(unique(current_column)) == 1 && length(current_column) > 1) {
          common_prefix_words <- c(common_prefix_words, current_column[1])
        } else {
          break
        }
      }
    }
  }
  
  # Build merged name
  if (length(common_prefix_words) > 0) {
    common_prefix <- paste(common_prefix_words, collapse = " ")
    remaining_parts <- str_remove(main_names, paste0("^", str_escape(common_prefix), "\\s*")) %>%
      str_trim() %>%
      .[. != ""]
    
    remaining_parts <- unique(remaining_parts)
    
    if (length(remaining_parts) > 0) {
      merged_main <- paste0(common_prefix, " ", paste(remaining_parts, collapse = " & "))
    } else {
      merged_main <- common_prefix
    }
  } else {
    merged_main <- paste(main_names, collapse = " & ")
  }
  
  final_name <- if (!is.na(common_location)) {
    paste0(merged_main, " ", common_location)
  } else {
    merged_main
  }
  
  return(str_trim(final_name))
}
```

```{r}
# Now merge the centers with unnested columns
pivoted_cc_data <- duplicates_unnested %>%
  group_by(postal_code) %>%
  summarise(
    name = merge_center_names(name),
    aac = max(aac, na.rm = TRUE),
    counselling = max(counselling, na.rm = TRUE),
    daycare = max(daycare, na.rm = TRUE),
    dementia = max(dementia, na.rm = TRUE),
    hospice = max(hospice, na.rm = TRUE),
    maintenance = max(maintenance, na.rm = TRUE),
    nhrespite = max(nhrespite, na.rm = TRUE),
    nursing = max(nursing, na.rm = TRUE),
    rehab = max(rehab, na.rm = TRUE),
    .groups = "drop"
  )
```

```{r}
# Show before and after
cat("BEFORE MERGING (unnested):\n")
print(duplicates_unnested %>% 
      arrange(postal_code, name) %>%
      select(postal_code, name, aac, daycare, nursing, rehab))

cat("\nAFTER MERGING:\n")
print(merged_duplicates %>% 
      arrange(postal_code) %>%
      select(postal_code, name, aac, daycare, nursing, rehab))
```

# method 2

```{r}
# Step 1: Extract duplicates from main dataframe
duplicates <- pivoted_cc_data %>%
  group_by(postal_code) %>%
  filter(n() > 1) %>%
  arrange(postal_code)
```

```{r}
non_duplicates <- pivoted_cc_data %>%
  group_by(postal_code) %>%
  filter(n() == 1) %>%
  ungroup() %>%
  # Convert list columns to numeric for consistency
  mutate(
    aac = as.numeric(aac),
    counselling = as.numeric(counselling),
    daycare = as.numeric(daycare),
    dementia = as.numeric(dementia),
    hospice = as.numeric(hospice),
    maintenance = as.numeric(maintenance),
    nhrespite = as.numeric(nhrespite),
    nursing = as.numeric(nursing),
    rehab = as.numeric(rehab)
  )
```

```{r}
# Step 3: Process duplicates - unnest list columns
service_columns <- c("aac", "counselling", "daycare", "dementia", "hospice", 
                    "maintenance", "nhrespite", "nursing", "rehab")

# Find which service columns are actually lists
list_service_cols <- names(duplicates)[sapply(duplicates, is.list) & names(duplicates) %in% service_columns]
cat("List columns to unnest:", paste(list_service_cols, collapse = ", "), "\n")

# Unnest the list columns
if (length(list_service_cols) > 0) {
  duplicates_unnested <- duplicates %>%
    unnest(cols = all_of(list_service_cols))
} else {
  duplicates_unnested <- duplicates
}
```

```{r}
# Step 4: Function to merge center names (your existing function)
merge_center_names <- function(names) {
  if (length(names) == 1) return(names)
  
  names <- unique(names)
  if (length(names) == 1) return(names)
  
  # Extract location pattern
  location_pattern <- "\\(([^)]+)\\)$"
  locations <- str_extract(names, location_pattern)
  common_location <- locations[!is.na(locations)][1]
  
  # Remove location from names
  main_names <- str_remove(names, location_pattern) %>% str_trim()
  
  # Find common prefix
  words_matrix <- str_split(main_names, "\\s+", simplify = TRUE)
  common_prefix_words <- c()
  
  if (nrow(words_matrix) > 1) {
    max_cols <- ncol(words_matrix)
    for (i in 1:max_cols) {
      if (i <= ncol(words_matrix)) {
        current_column <- words_matrix[, i]
        current_column <- current_column[current_column != ""]
        
        if (length(unique(current_column)) == 1 && length(current_column) > 1) {
          common_prefix_words <- c(common_prefix_words, current_column[1])
        } else {
          break
        }
      }
    }
  }
  
  # Build merged name
  if (length(common_prefix_words) > 0) {
    common_prefix <- paste(common_prefix_words, collapse = " ")
    remaining_parts <- str_remove(main_names, paste0("^", str_escape(common_prefix), "\\s*")) %>%
      str_trim() %>%
      .[. != ""]
    
    remaining_parts <- unique(remaining_parts)
    
    if (length(remaining_parts) > 0) {
      merged_main <- paste0(common_prefix, " ", paste(remaining_parts, collapse = " & "))
    } else {
      merged_main <- common_prefix
    }
  } else {
    merged_main <- paste(main_names, collapse = " & ")
  }
  
  final_name <- if (!is.na(common_location)) {
    paste0(merged_main, " ", common_location)
  } else {
    merged_main
  }
  
  return(str_trim(final_name))
}
```

```{r}

# Step 5: Merge the duplicates with amendments
merged_duplicates <- duplicates_unnested %>%
  group_by(postal_code) %>%
  summarise(
    name = merge_center_names(name),
    aac = max(aac, na.rm = TRUE),
    counselling = max(counselling, na.rm = TRUE),
    daycare = max(daycare, na.rm = TRUE),
    dementia = max(dementia, na.rm = TRUE),
    hospice = max(hospice, na.rm = TRUE),
    maintenance = max(maintenance, na.rm = TRUE),
    nhrespite = max(nhrespite, na.rm = TRUE),
    nursing = max(nursing, na.rm = TRUE),
    rehab = max(rehab, na.rm = TRUE),
    .groups = "drop"
  )
```

```{r}
# Step 6: Combine back with non-duplicates to recreate the main dataframe
pivoted_cc_data <- bind_rows(non_duplicates, merged_duplicates) %>%
  arrange(postal_code)
```

```{r}
#| echo: false
#| eval: false
write_rds(pivoted_cc_data, "data/rds/carecentre/refined/pivoted_cc_data.rds")
```

```{r}
#| echo: false
#| eval: false
pivoted_cc_data = read_rds("data/rds/carecentre/refined/pivoted_cc_data.rds")
```

### Adding coordinates to care centre

This code prepares a list of unique postal codes from a dataset called `pivoted_cc_data` to be used for geocoding via an API. The line first extracts the `postal_code` column from the dataframe, then applies `unique()` to eliminate duplicate postal codes - this optimization reduces the number of API calls needed since the same postal code will return the same coordinates. The `sort()` function then arranges these unique postal codes in ascending order, which serves two purposes: it makes the list more organized for human review (easier to locate specific codes during debugging or verification), and it may help with processing efficiency when matching the geocoded results back to the original dataset. The resulting sorted unique list is stored in `add_list`, which can then be passed to a geocoding API (like the OneMap API in Singapore) that typically accepts individual addresses or postal codes rather than entire dataframes. This preprocessing step is crucial because APIs often have rate limits or usage constraints, so minimizing duplicate requests helps maximize efficiency and reduce potential errors or bottlenecks in the geocoding process. The comment "parse a list as API cannot read df" explicitly notes that this conversion from dataframe column to simple list is necessary because the target API expects individual values rather than dataframe structures as input.

```{r}
add_list <- sort(unique(pivoted_cc_data$postal_code)) #parse a list as API cannot read df
#unique reduces records to pass to portal
#sort is used to easier to find geo codes
```

The below codechunk defines a function called get_coords that takes a list of Singapore postal codes as input and retrieves their geographic coordinates from the OneMap API (a Singapore government mapping service). The function first initializes an empty data frame to store the results. For each postal code in the input list, it makes an HTTP GET request to the OneMap API, which returns the location data in JSON format. The function then processes the response differently depending on how many matches are found: if there's exactly one match, it extracts those coordinates; if there are multiple matches, it looks for an exact postal code match; and if no matches are found, it records NA values. The valid coordinates (in WGS84 latitude/longitude format) are converted into an sf (simple features) spatial object, which is then transformed to Singapore's SVY21 projected coordinate system (EPSG:3414). The function extracts these SVY21 coordinates and merges them back into the original data frame, preserving any rows that had invalid or missing coordinates. The final output is a data frame containing the original postal codes, any matching postal codes found, WGS84 coordinates, SVY21 coordinates, and the geometric data as an sf geometry column. This function is particularly useful for geocoding Singapore addresses and preparing spatial data for analysis with Singapore-specific geographic information systems.

```{r}
get_coords <- function(postal_list){
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (postal in postal_list){
    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=postal,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each postal code
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal_code <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(postal_code = postal, 
                           postal_found = postal_code, 
                           latitude_wgs84 = lat,
                           longitude_wgs84 = lng)
    }
    
    # If multiple results, use the exact postal code match
    else if (found > 1){
      # Find exact match for postal code
      res_match <- res[res$POSTAL == postal, ]
      
      # If exact match found, use it
      if (nrow(res_match) > 0) {
        postal_code <- res_match$POSTAL[1]
        lat <- res_match$LATITUDE[1]
        lng <- res_match$LONGITUDE[1]
        new_row <- data.frame(postal_code = postal,
                             postal_found = postal_code,
                             latitude_wgs84 = lat,
                             longitude_wgs84 = lng)
      }
      # If no exact match, set as NA
      else {
        new_row <- data.frame(postal_code = postal,
                             postal_found = NA,
                             latitude_wgs84 = NA,
                             longitude_wgs84 = NA)
      }
    }
    # If no results found
    else {
      new_row <- data.frame(postal_code = postal,
                           postal_found = NA,
                           latitude_wgs84 = NA,
                           longitude_wgs84 = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  
  # Convert to sf object with WGS84 coordinates (EPSG:4326)
  # Filter out rows with NA coordinates first
  valid_coords <- postal_coords[!is.na(postal_coords$latitude_wgs84) & 
                              !is.na(postal_coords$longitude_wgs84), ]
  
  if(nrow(valid_coords) > 0) {
    coords_sf <- st_as_sf(valid_coords, 
                         coords = c("longitude_wgs84", "latitude_wgs84"),
                         crs = 4326)
    
    # Transform to SVY21 (EPSG:3414)
    coords_svy21 <- st_transform(coords_sf, 3414)
    
    # Extract coordinates
    coords_matrix <- st_coordinates(coords_svy21)
    
    # Add SVY21 coordinates back to the original dataframe
    valid_coords$longitude <- coords_matrix[, 1]  # SVY21 X coordinate
    valid_coords$latitude <- coords_matrix[, 2]   # SVY21 Y coordinate

    
    # Merge back with rows that had NA coordinates
    result <- merge(postal_coords, 
                   valid_coords[c("postal_code", "longitude", "latitude")], 
                   by = "postal_code", all.x = TRUE)
  } else {
    # If no valid coordinates, add empty SVY21 columns
    result <- postal_coords
    result$longitude <- NA
    result$latitude <- NA
  }
  
  return(result)
}


```

The code `coords <- get_coords(add_list)` calls the previously defined `get_coords()` function to geocode (convert to geographic coordinates) a list of Singapore postal codes stored in `add_list`.

```{r}
coords <- get_coords(add_list)
```

### Final Dataset of CC

The below code chunk merges the original dataset (`pivoted_cc_data`) with the geocoded coordinates (`coords`) using a left join operation, which preserves all records from the primary dataset while matching and appending geographic data where available using the properties of `postal_code` in both dataframes.

```{r}
cc_data_final <- pivoted_cc_data %>%
  left_join(coords, 
            join_by(postal_code == postal_code)
)
```

```{r}
cc_sf <- st_as_sf(cc_data_final,
                  coords = c("longitude", "latitude"), #c is use column
                         crs = 3414)
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_data_final, "data/rds/carecentre/refined/cc_data_final.rds")
```

```{r}
#| echo: false
#| eval: false
write_rds(cc_sf, "data/rds/carecentre/refined/cc_sf.rds")
```

```{r}
#| echo: false
#| eval: false
cc_data_final = read_rds("data/rds/carecentre/refined/cc_data_final.rds")
```

```{r}
#| echo: false
#| eval: false
cc_sf = read_rds("data/rds/carecentre/refined/cc_sf.rds")
```

```{r}
mpsz_land_fixed <- st_make_valid(mpsz_land)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz_land_fixed) +
  tm_polygons()
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(cc_sf) +
  tm_dots() #change to name
```

# Exploratory Data Analysis

## Population Pyramid of Historical Data Year 2020 to 2024

In generating the population pyramid, we will load a function called `get_age_distribution`.

```{r}
#load data
get_age_distribution <- function(data, min_age = 60, max_age = 90) {
  data %>%
    filter(age >= min_age & age <= max_age) %>%
    mutate(
      # Standardize sex labels to match your data
      sex = factor(sex, levels = c("Males", "Females"))
    ) %>%
    group_by(time, age, sex) %>%
    summarise(total_pop = sum(pop), .groups = "drop")
}
```

From the population pyramid below, there is a steady decline of population from age 60 to 75 for both male and female and there is a large number of population for age 90. This is due to the aggregation of the population of age 90 and above. Hence the data presented is not a true representation of the population of age 90.

```{r}
# year 2020
age_dist <- get_age_distribution(popdata20_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata21_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata22_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata23_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

```{r}
#| echo: false #not show code

age_dist <- get_age_distribution(popdata24_c)
# 3. Create population pyramid plot function 
plot_population_pyramid <- function(age_data, plot_year = max(age_data$time)) {
  # Filter and transform data
  plot_data <- age_data %>% 
    filter(time == plot_year) %>%
    mutate(total_pop = ifelse(sex == "Males", -total_pop, total_pop))
  
  # Calculate axis limits
  max_pop <- max(abs(plot_data$total_pop))
  
  ggplot(plot_data, aes(x = age, y = total_pop, fill = sex)) +
    geom_bar(stat = "identity", width = 0.8) +
    geom_hline(yintercept = 0, color = "black") +
    coord_flip() +
    scale_y_continuous(
      labels = function(x) comma(abs(x)),
      breaks = pretty_breaks(n = 6),
      limits = c(-max_pop * 1.1, max_pop * 1.1)
    ) +
    scale_fill_manual(
      values = c("Males" = "#3498db", "Females" = "#e74c3c"),
      labels = c("Males" = "Male", "Females" = "Female") # Optional: display cleaner labels
    ) +
    labs(
      title = paste("Population Pyramid of Age 60 & Above -", plot_year),
      x = "Age",
      y = "Population Count",
      fill = "Gender"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "bottom"
    )
}

# 4. Generate and save the plot ------------------------------------------------
final_plot <- plot_population_pyramid(age_dist)
print(final_plot)
```

## Population Forecast from Year 2025 to 2029 with Active Ageing Centre (AAC)

In this population forecast for Year 2025, we are able to see concentrated (areas in red) older adults population against the backdrop of Active Ageing Centre (AAC)

```{r}
tmap_mode("view")
tm_shape(geri_forecast) +
  tm_fill("aged_2025",
          title = "Population 2025",
          palette = c("white", "#ffcccc", "red"),
          style = "quantile",
          n = 7) +
  tm_borders(col = "white", lwd = 0.5) +
tm_shape(cc_sf[cc_sf$aac == 1, ]) +
  tm_dots(col = "black",
          size = 0.1,
          alpha = 0.8,
          title = "Active Ageing Centres") +
  tm_layout(
    title = "Population Forecast 2025 with Active Ageing Centres",
    title.position = c("left", "top"),
    legend.position = c("left", "bottom"),
    frame = FALSE
  ) +
  tm_compass(position = c("right", "top")) +
  tm_scale_bar(position = c("left", "bottom"))
```

## Population Growth from Year 2025 to 2029 with AAC

In the interactive map below, the population growth of Age 60 and above from Year 2025 to Year 2029 has been plotted. The darker colour indicates increase of population. Active Ageing Centres has been overlaid onto the population growth of the older adults.

Observation 1: One subzone that will be facing an increase of older adult population is Tampines East. However, there is only 6 Active Ageing Centres available in the subzone.

Observation 2: In the subzone of Holland Road, there is an increase of population of 287 older adults from 2025 to 2029. However, there is no Active Ageing Centre in this subzone. Local context is important here as Holland Road is an affluent residential area, predominantly filled with landed properties and condominiums. Accessibility is likely to be poor in this subzone Hence, Active Ageing Centre may not be established in this region as most residents drive and they can commute using their own transport to the nearest AAC.

```{r}
tmap_mode("view")
# Reshape data to long format for faceting
geri_long <- geri_forecast %>%
  pivot_longer(cols = c(aged_2025, aged_2026, aged_2027, aged_2028, aged_2029),
               names_to = "year",
               values_to = "population") %>%
  mutate(year = gsub("aged_", "", year))

# Create faceted heatmap with AAC overlay
tm_shape(geri_long) +
  tm_fill("population",
          title = "Population",
          palette = c("white", "orange", "red"),
          style = "cont") +
  tm_borders(col = "white", lwd = 0.3) +
  tm_facets(by = "year", ncol = 3) +
tm_shape(cc_sf[cc_sf$aac == 1, ]) +
  tm_dots(col = "black",
          size = 0.1,
          alpha = 0.8,
          title = "Active Ageing Centres") +
  tm_layout(
    title = "Population Forecast by Year with Active Ageing Centres",
    legend.position = c("right", "bottom"),
    panel.labels = c("2025", "2026", "2027", "2028", "2029"),
    frame = FALSE
  )

# Method 2: Calculate and show year-over-year changes
geri_forecast <- geri_forecast %>%
  mutate(
    change_2025_2026 = aged_2026 - aged_2025,
    change_2026_2027 = aged_2027 - aged_2026,
    change_2027_2028 = aged_2028 - aged_2027,
    change_2028_2029 = aged_2029 - aged_2028,
    total_change = aged_2029 - aged_2025,
    percent_change = ((aged_2029 - aged_2025) / aged_2025) * 100
  )

# Heatmap of total change (2025 to 2029) with AAC overlay
tm_shape(geri_forecast) +
  tm_fill("total_change",
          title = "Population Change\n(2025-2029)",
          palette = "RdBu",
          style = "cont",
          midpoint = 0) +
  tm_borders(col = "white", lwd = 0.3) +
tm_shape(cc_sf[cc_sf$aac == 1, ]) +
  tm_dots(col = "black",
          size = 0.1,
          alpha = 0.8,
          title = "Active Ageing Centres") +
  tm_layout(
    title = "Total Population Change 2025-2029 with Active Ageing Centres",
    legend.position = c("left", "bottom"),
    frame = FALSE
  )
```

# References

Burdziej, J. (2019). Using hexagonal grids and network analysis for spatial accessibility assessment in urban environments-A case study of public amenities in Toruń. Miscellanea Geographica. Regional Studies on Development, 23(2), 99-110.

ESRI. (2025). Why hexagons?. Retrieved on May, 2025 from https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-whyhexagons.htm

Kam, T. S. (2024). R for Geospatial Data Science and Analytics. Retrieved on April 2, 2, 2025 from https://r4gdsa.netlify.app/

Kam, T. S. (2022). GIS for Urban PlanningL QGIS methods. Retrieved on May 2, 2025 from https://gis4urbplan.netlify.app/

Tan, K. (2023). Take-home Exercise 1: Geospatial Analytics for Public Good. Retrieved from <https://isss624-kytjy.netlify.app/take-home_ex/take-home_ex1/the1#background>

Ministry of Health Singapore. (2022). Healthier SG White Paper. Retrieved from https://file.go.gov.sg/healthiersg-whitepaper-pdf.pdf

Urban Redevelopment Authority. (2023). Master Plan 2019 Planning Area Boundary (No Sea) (2024) \[Dataset\]. data.gov.sg. Retrieved February 23, 2025 from https://data.gov.sg/datasets/d_6c6d7361dd826d97b91bac914ca6b2ac/view
